{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#For data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For reproducibility\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, preprocess_params):\n",
    "        self.num_lag = preprocess_params[\"num_lag\"]\n",
    "        self.train_valid_ratio = preprocess_params[\"train_valid_ratio\"]\n",
    "        self.p_value_threshold = preprocess_params[\"p_value_threshold\"]\n",
    "        self.scaler = None\n",
    "\n",
    "    def preproces_train_data(self, data_path):\n",
    "        \"\"\"\n",
    "        get_lag_variable -> correlation coefficient -> split_train_valid -> normalize\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(data_path)\n",
    "        X = data.loc[:, data.columns != \"PM2.5\"]\n",
    "        y = data.loc[:, data.columns == \"PM2.5\"]\n",
    "        X = self.get_lag_variable(X, self.num_lag, isTesting = False)\n",
    "        X = X.dropna()\n",
    "        X = self.do_correlation_coefficient_test(X, y, self.p_value_threshold)\n",
    "        X_train, X_valid, y_train, y_valid = self.split_train_valid(X, y, self.train_valid_ratio)\n",
    "        X_train = self.normalize(X_train, isTraining=True)\n",
    "        X_valid = self.normalize(X_valid, isTraining=False)\n",
    "        return X_train, X_valid, np.array(y_train), np.array(y_valid)\n",
    "\n",
    "    def preprocess_test_data(self, data_path):\n",
    "        \"\"\"\n",
    "        get_lag_variable -> correlation coefficient -> normalize\n",
    "        \"\"\"\n",
    "        X_test = pd.read_csv(data_path)\n",
    "        X_test = self.get_lag_variable(X_test, self.num_lag, isTesting = True)\n",
    "        X_test = X_test[self.kept_columns]\n",
    "        X_test = self.normalize(X_test, isTraining=False)\n",
    "        return X_test\n",
    "\n",
    "    def normalize(self, data, isTraining = False):\n",
    "        if isTraining:\n",
    "            self.scaler = StandardScaler().fit(data)\n",
    "        data_scaled = self.scaler.transform(data)\n",
    "        return data_scaled\n",
    "\n",
    "    def do_correlation_coefficient_test(self, X, y, p_value_threshold):\n",
    "        corr_features = abs(X.corrwith(y['PM2.5'])).sort_values(ascending=False)\n",
    "        print(corr_features)\n",
    "        self.kept_columns = (corr_features[corr_features > p_value_threshold]).index\n",
    "        X = X[self.kept_columns]\n",
    "        return X\n",
    "\n",
    "    def split_train_valid(self, X, y, train_valid_ratio = 0.75):\n",
    "        data_len = len(X)\n",
    "        train_len = round(train_valid_ratio * data_len)\n",
    "        train_index = random.sample(range(data_len), train_len)\n",
    "        valid_index = [i for i in range(data_len) if i not in train_index]\n",
    "        X_train = X.iloc[train_index, :]\n",
    "        y_train = y.iloc[train_index, :]\n",
    "        X_valid = X.iloc[valid_index, :]\n",
    "        y_valid = y.iloc[valid_index, :]\n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    def get_lag_variable(self, data: pd.DataFrame, num_lag: int, isTesting = False):\n",
    "        #start from 0\n",
    "        data_ = data.copy()\n",
    "        for idx in range(num_lag):\n",
    "            #start from 1\n",
    "            time = idx + 1\n",
    "            lag_data = data.shift(time)\n",
    "            lag_data.columns = [col + \"_\" + str(-time) for col in data.columns]\n",
    "            data_ = pd.concat([data_, lag_data], axis = 1)\n",
    "        #fill testing set nan with \"mean\" \n",
    "        if isTesting:\n",
    "            data_.fillna(data_.mean(), inplace = True)\n",
    "        return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM10       0.854647\n",
      "PM10_-1    0.838010\n",
      "PM10_-2    0.797987\n",
      "PM10_-3    0.751594\n",
      "PM10_-4    0.707345\n",
      "             ...   \n",
      "NO_-6      0.025897\n",
      "RH_-3      0.024222\n",
      "RH_-5      0.022119\n",
      "RH_-4      0.019992\n",
      "NO_-7      0.004001\n",
      "Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/\"\n",
    "train_path = os.path.join(data_path, \"training_data.csv\")\n",
    "test_path = os.path.join(data_path, \"testing_data.csv\")\n",
    "preprocess_params = {\n",
    "    \"num_lag\": 7,\n",
    "    \"train_valid_ratio\": 0.75,\n",
    "    \"p_value_threshold\": 0.3\n",
    "}\n",
    "\n",
    "DP = DataPreprocessor(preprocess_params)\n",
    "X_train, X_valid, y_train, y_valid = DP.preproces_train_data(train_path)\n",
    "X_test = DP.preprocess_test_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6588, 48)\n",
      "(2196, 48)\n",
      "(8760, 48)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPADataSet(Dataset):\n",
    "    \"\"\"Dataset for loading and preprocessing the EPA Weather data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y = None, mode = \"train\"):\n",
    "        self.mode = mode\n",
    "        if mode in [\"train\", \"valid\"]:\n",
    "            self.data = torch.from_numpy(X).float()\n",
    "            self.target = torch.from_numpy(y.reshape(-1, 1)).float()\n",
    "            self.dim = X.shape[1]\n",
    "            print(f\"Check nan...\")\n",
    "            if not np.any(np.isnan(X)) and not np.any(np.isnan(y)):\n",
    "                print(\"No nan in dataset.\")\n",
    "        else:\n",
    "            self.data = torch.from_numpy(X).float()\n",
    "            self.dim = X.shape[1]\n",
    "            print(f\"Check nan...\")\n",
    "            if not np.any(np.isnan(X)):\n",
    "                print(\"No nan in dataset.\")\n",
    "\n",
    "        print(f\"Finish reading {self.mode} dataset ({len(self.data)} samples, {self.dim} features)\")\n",
    "        if mode in [\"train\", \"valid\"]:\n",
    "            print(f\"Check target size... The target size is {len(self.target)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in [\"train\", \"valid\"]:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataloader(mode, batch_size, X, y = None):\n",
    "    \"\"\"Generates a dataset, then is put into a dataloader. \n",
    "    \"\"\"\n",
    "    dataset = EPADataSet(X, y, mode)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle = (mode == \"train\"), drop_last=False, pin_memory = True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(train_loss, valid_loss):\n",
    "    num_epoch = len(train_loss)\n",
    "    x = [i+1 for i in range(num_epoch)]\n",
    "    plt.plot(x, train_loss)\n",
    "    plt.plot(x, valid_loss)\n",
    "    plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #input layer\n",
    "            nn.Linear(input_dim, 32), \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #hidden layer\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            #output layer\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "\n",
    "        self.net.apply(self.init_weights)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, valid_set, params_set, device, verbose = False):\n",
    "    #params settings\n",
    "    num_epoch = params_set[\"num_epoch\"]\n",
    "    optimizer = getattr(torch.optim, params_set['optimizer'])(\n",
    "    model.parameters(), **params_set['optim_hparams'])\n",
    "    patience = params_set[\"patience\"]\n",
    "\n",
    "    #recording\n",
    "    best_train_MSE = 0\n",
    "    best_valid_MSE = 99999\n",
    "    best_epoch = 0\n",
    "    loss_record = {\"train\": [], \"valid\": []}\n",
    "\n",
    "    #start training\n",
    "    for idx_epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_SSE = 0\n",
    "        for idx, train_data in enumerate(train_set):\n",
    "            X, y = train_data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(X)\n",
    "            train_loss = model.cal_loss(train_pred.flatten(), y.flatten())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_SSE += (train_loss.detach().cpu().item()) * len(X)\n",
    "        train_MSE = train_SSE / len(train_set.dataset)\n",
    "        loss_record[\"train\"].append(train_MSE)\n",
    "        #validation\n",
    "        valid_MSE = valid(model, valid_set, device)\n",
    "        loss_record[\"valid\"].append(valid_MSE)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {idx_epoch}, Train loss: {round(train_MSE, 4)}, Valid loss: {round(valid_MSE, 4)}\")\n",
    "\n",
    "        #save best result\n",
    "        if valid_MSE < best_valid_MSE:\n",
    "            best_valid_MSE = valid_MSE\n",
    "            best_train_MSE = train_MSE\n",
    "            best_epoch = idx_epoch\n",
    "        \n",
    "        #early stopping\n",
    "        if valid_MSE > best_valid_MSE and idx_epoch >= best_epoch + patience:\n",
    "            print(\"=\"*50)\n",
    "            print(\"Early Stopping!\")\n",
    "            print(f\"Best epoch is {best_epoch}, training loss = {round(best_train_MSE, 4)}, minimum valid loss = {round(best_valid_MSE, 4)}\")\n",
    "            return loss_record, model\n",
    "\n",
    "    #Training until the final epoch\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Model result:\")\n",
    "    print(f\"Best epoch is {best_epoch}, training loss = {round(best_train_MSE, 4)}, minimum valid loss = {round(best_valid_MSE, 4)}\")\n",
    "    return loss_record, model\n",
    "\n",
    "def valid(model, valid_set, device):\n",
    "    valid_SSE = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()     \n",
    "        for idx, valid_data in enumerate(valid_set):\n",
    "            X, y = valid_data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            valid_pred = model(X)\n",
    "            valid_loss = model.cal_loss(valid_pred, y)\n",
    "            valid_SSE += (valid_loss.detach().cpu().item()) * len(X)\n",
    "    valid_MSE = valid_SSE / len(valid_set.dataset)\n",
    "    return valid_MSE\n",
    "\n",
    "def test(model, test_set, device):\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X in test_set:\n",
    "            X = X.to(device)\n",
    "            test_pred = model(X)\n",
    "            preds.append(test_pred.flatten().detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set = {\n",
    "    \"num_batch\": [512], \n",
    "    \"num_epoch\": [20, 50],\n",
    "    #optimizer\n",
    "    \"optimizer\": [\"Adam\"],\n",
    "    \"optim_hparams\": [{\n",
    "        \"lr\": [0.5, 0.1, 0.05, 0.01, 0.005],\n",
    "        \"weight_decay\": [0.05, 0.1, 1, 5, 10]\n",
    "    }],\n",
    "    #for early stopping\n",
    "    \"patience\": [5, 15]\n",
    "}\n",
    "\n",
    "param_grid = []\n",
    "count = 0\n",
    "for values in itertools.product(*params_set.values()):\n",
    "    param = dict(zip(params_set.keys(), values))\n",
    "    for optim_values in itertools.product(*param[\"optim_hparams\"].values()):\n",
    "        param_copy = param.copy()\n",
    "        param_copy[\"optim_hparams\"] = dict(zip(param[\"optim_hparams\"].keys(), optim_values))\n",
    "        param_grid.append(param_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check nan...\n",
      "No nan in dataset.\n",
      "Finish reading train dataset (6583 samples, 48 features)\n",
      "Check target size... The target size is 6583\n",
      "Check nan...\n",
      "No nan in dataset.\n",
      "Finish reading valid dataset (2194 samples, 48 features)\n",
      "Check target size... The target size is 2194\n",
      "Check nan...\n",
      "No nan in dataset.\n",
      "Finish reading test dataset (8760 samples, 48 features)\n"
     ]
    }
   ],
   "source": [
    "train_set = prep_dataloader(\"train\", params_set[\"num_batch\"][0], X_train, y_train)\n",
    "valid_set = prep_dataloader(\"valid\", params_set[\"num_batch\"][0], X_valid, y_valid)\n",
    "test_set = prep_dataloader(\"test\", params_set[\"num_batch\"][0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.6847e-01, -9.0799e-01, -7.0171e-01, -7.6957e-01, -8.3434e-01,\n",
      "         -8.3490e-01, -7.6547e-01, -8.6728e-01, -5.8054e-01, -7.9727e-01,\n",
      "         -9.3215e-01, -6.3285e-01, -3.4048e-02, -6.1508e-01, -6.2223e-01,\n",
      "         -6.1485e-01, -9.7914e-01, -1.2156e+00, -6.0691e-01, -6.0625e-01,\n",
      "         -1.0949e+00, -5.6026e-01, -1.0747e+00, -1.1945e+00, -5.9349e-01,\n",
      "         -1.4182e+00, -1.0339e+00, -6.6062e-01, -1.0910e+00, -5.4310e-01,\n",
      "         -9.0507e-01, -9.4106e-01, -6.3727e-01, -1.1962e+00, -9.5285e-01,\n",
      "         -7.0783e-01, -1.4754e+00,  6.8760e-01,  6.8868e-01,  6.8599e-01,\n",
      "          6.8950e-01,  6.9227e-01, -3.0619e-01, -8.7873e-01, -5.2184e-01,\n",
      "          6.9399e-01,  8.7956e-01,  4.4466e-02],\n",
      "        [ 2.8281e+00,  2.5747e+00,  2.5832e+00,  2.1157e+00,  2.3476e+00,\n",
      "          2.3111e+00,  2.4106e+00,  1.0707e+00,  3.0731e+00,  2.6359e+00,\n",
      "          5.1054e-01,  2.6292e+00,  1.1340e+00,  1.1347e+00,  1.1336e+00,\n",
      "          1.1352e+00,  1.5434e+00,  5.4830e-01,  1.7147e+00,  1.7298e+00,\n",
      "          6.9088e-01,  2.3963e+00,  9.0224e-01,  1.1546e+00,  1.5820e+00,\n",
      "         -4.8823e-02,  4.5865e-01,  1.1783e+00,  5.7849e-01,  2.5083e+00,\n",
      "          6.9460e-01,  1.0409e+00,  1.8138e+00, -1.1029e-01,  2.3238e-01,\n",
      "          1.3114e+00,  3.7200e-01, -4.1427e-01, -4.1384e-01, -4.1581e-01,\n",
      "         -4.1351e-01, -2.2865e-01,  9.6688e-01,  7.0844e-01,  1.7989e-01,\n",
      "         -4.3403e-02,  3.2608e-01,  1.7364e+00],\n",
      "        [ 1.6855e+00,  1.3236e+00,  1.2286e+00,  1.5047e+00,  2.2460e+00,\n",
      "          2.8524e+00,  3.1878e+00, -1.2312e-02, -9.6633e-03,  3.3428e+00,\n",
      "          3.9512e-01,  2.8282e-01, -3.4048e-02, -3.1823e-02, -3.6957e-02,\n",
      "          5.5184e-01,  7.4075e-01, -3.9662e-02,  5.5390e-01,  1.1458e+00,\n",
      "          9.7891e-01, -1.7645e-01, -1.3365e-01,  1.1546e+00, -3.0973e-01,\n",
      "         -4.0799e-03,  1.7790e+00, -1.7385e-01,  2.3056e+00, -2.9638e-01,\n",
      "         -2.5011e-01, -1.7255e-01, -5.0614e-01, -1.7728e-01,  1.5792e+00,\n",
      "         -4.0495e-01,  1.8553e+00, -4.1427e-01, -2.3009e-01, -7.8308e-01,\n",
      "         -4.5840e-02, -4.4468e-02,  4.7440e-02, -4.0359e-01,  3.9541e-02,\n",
      "         -2.2775e-01, -4.1189e-01,  1.8546e-01],\n",
      "        [-9.5557e-02, -9.6497e-02, -1.9372e-01, -2.6041e-01, -1.9119e-01,\n",
      "         -2.2599e-01, -1.5728e-01,  1.5868e-01,  2.1869e-01, -4.6068e-01,\n",
      "         -8.8296e-03, -2.3225e-01,  5.5000e-01,  5.5143e-01,  1.1336e+00,\n",
      "         -3.1506e-02, -2.3385e-01,  1.1363e+00, -2.6509e-02, -2.2252e-02,\n",
      "         -1.1563e-01,  2.2635e-01,  9.0224e-01, -1.9924e-02, -4.4486e-01,\n",
      "          2.2235e-01, -6.0152e-04, -6.0653e-01, -1.6992e-01,  9.1340e-02,\n",
      "          5.9399e-01, -5.7703e-01, -4.0527e-01, -8.8073e-03, -4.0064e-01,\n",
      "         -5.9677e-01, -6.5282e-01,  6.8760e-01,  6.8868e-01,  8.6962e-01,\n",
      "          5.0567e-01,  5.0809e-01,  9.6688e-01, -5.7545e-01,  6.0092e-01,\n",
      "          3.2529e-01,  3.2608e-01,  7.4945e-01],\n",
      "        [ 8.4539e-01,  1.0869e+00,  8.8997e-01,  1.1652e+00,  6.5508e-01,\n",
      "          5.5206e-01,  2.4818e-01,  2.1568e-01,  3.8995e-01,  3.8079e-01,\n",
      "          5.1054e-01,  5.1174e-01,  2.8862e+00,  2.3012e+00,  3.4747e+00,\n",
      "          2.3019e+00,  7.4075e-01,  4.0761e+00,  1.1343e+00,  5.6175e-01,\n",
      "          4.6045e-01,  1.3113e+00,  1.1731e+00, -1.9924e-02,  1.4469e+00,\n",
      "          9.0027e-01,  6.3087e-01,  1.7191e+00,  8.6635e-01,  8.9698e-01,\n",
      "          1.0970e+00,  1.1757e+00,  1.0069e+00,  1.0061e+00,  1.0405e+00,\n",
      "          1.3114e+00,  1.1811e+00,  3.2031e-01,  3.2117e-01,  3.1872e-01,\n",
      "          3.2183e-01,  1.3972e-01, -3.0619e-01,  7.0844e-01, -3.8149e-01,\n",
      "          1.4094e-01,  1.4159e-01, -4.4902e-01],\n",
      "        [-2.8347e-02,  1.0638e-01,  9.4698e-03, -1.2463e-01, -4.2814e-01,\n",
      "         -5.3044e-01, -3.9380e-01,  5.5767e-01,  2.7577e-01, -3.2604e-01,\n",
      "          3.3742e-01, -6.0557e-02,  1.1340e+00,  1.1347e+00,  5.4831e-01,\n",
      "          5.5184e-01, -2.3385e-01, -3.9662e-02,  5.5390e-01,  1.1458e+00,\n",
      "         -1.1563e-01,  9.0724e-02,  2.2519e-01,  5.6736e-01,  3.6589e-01,\n",
      "         -4.8823e-02,  5.6805e-02,  5.0221e-01,  3.4821e-01,  5.9487e-01,\n",
      "          8.9581e-01,  3.6676e-01,  4.0167e-01,  4.9863e-01,  5.0176e-01,\n",
      "          4.0272e-01,  9.1139e-01,  1.3667e-01,  1.3742e-01,  1.3509e-01,\n",
      "          1.3800e-01,  1.3972e-01,  1.1083e+00,  2.0297e-01,  2.2149e+00,\n",
      "          1.4094e-01,  3.2608e-01,  2.5596e-01],\n",
      "        [-4.3161e-01, -1.6412e-01, -4.3078e-01, -4.3013e-01, -5.2969e-01,\n",
      "         -3.6130e-01, -6.3031e-01, -9.2428e-01, -7.5180e-01, -6.2897e-01,\n",
      "         -5.2820e-01, -8.0454e-01, -1.2021e+00, -1.1983e+00, -1.2075e+00,\n",
      "         -1.1982e+00, -8.6448e-01, -6.2763e-01, -1.1873e+00, -1.1903e+00,\n",
      "         -9.7973e-01, -6.4163e-01, -1.1289e+00, -1.1945e+00, -8.7725e-01,\n",
      "         -7.5386e-01, -9.7650e-01, -1.0122e+00, -1.0910e+00, -7.4451e-01,\n",
      "         -1.1365e+00, -1.3725e+00, -8.9953e-01, -8.4101e-01, -1.3838e+00,\n",
      "         -9.7032e-01, -1.5158e+00,  1.0549e+00,  8.7243e-01,  1.0533e+00,\n",
      "          6.8950e-01,  5.0809e-01, -2.3287e-02, -1.2427e+00, -3.0632e-02,\n",
      "          3.2529e-01,  3.2608e-01,  1.3134e+00],\n",
      "        [ 1.3495e+00,  1.0531e+00,  1.0254e+00,  8.5974e-01,  6.2123e-01,\n",
      "          1.7995e-01,  1.4682e-01,  7.8566e-01,  1.1321e+00,  4.1445e-01,\n",
      "          7.4137e-01,  1.3130e+00,  1.1340e+00, -3.1823e-02,  5.4831e-01,\n",
      "         -3.1506e-02,  9.1274e-01,  5.4830e-01, -6.0691e-01, -6.0625e-01,\n",
      "          6.3327e-01,  1.1757e+00,  7.6683e-01, -6.0721e-01,  1.3118e+00,\n",
      "          4.9352e-01,  1.7162e-01,  1.3135e+00, -6.3049e-01,  7.9628e-01,\n",
      "          3.9277e-01,  2.3193e-01,  9.0601e-01,  1.9417e-01, -3.4676e-01,\n",
      "          9.0752e-01, -5.7192e-01,  5.0396e-01,  5.0493e-01,  5.0235e-01,\n",
      "          5.0567e-01,  3.2390e-01,  1.4620e+00,  7.8223e-04,  4.6058e-01,\n",
      "          3.2529e-01,  3.2608e-01,  1.6659e+00],\n",
      "        [-7.3405e-01, -6.0368e-01, -7.0171e-01, -7.3563e-01, -7.6664e-01,\n",
      "         -9.7021e-01, -9.3441e-01, -8.6728e-01, -1.2656e+00, -1.1675e+00,\n",
      "         -4.7049e-01, -1.1479e+00, -1.2021e+00, -1.1983e+00, -1.2075e+00,\n",
      "         -1.1982e+00, -1.1511e+00, -6.2763e-01, -1.1873e+00, -1.1903e+00,\n",
      "         -1.3254e+00, -1.2112e+00, -9.3933e-01, -1.1945e+00, -1.1745e+00,\n",
      "         -1.8441e-01, -1.4358e+00, -1.1879e+00, -1.5516e+00, -1.1071e+00,\n",
      "         -8.7488e-01, -1.3051e+00, -1.0912e+00, -3.1327e-01, -1.3434e+00,\n",
      "         -1.1016e+00, -1.4080e+00,  1.6058e+00,  1.4237e+00,  1.6042e+00,\n",
      "          1.2410e+00,  1.2448e+00, -1.1549e+00, -1.1517e+00,  1.0971e-01,\n",
      "          1.2470e+00,  1.0640e+00, -1.1540e+00],\n",
      "        [ 1.6183e+00,  1.2898e+00,  1.0254e+00,  5.2030e-01,  2.1502e-01,\n",
      "         -9.0676e-02, -2.2124e-02,  6.1467e-01,  6.7539e-01, -5.6772e-02,\n",
      "          6.8366e-01,  1.1113e-01, -3.4048e-02, -6.1508e-01, -3.6957e-02,\n",
      "         -6.1485e-01,  5.2795e-02, -3.9662e-02, -6.0691e-01, -6.0625e-01,\n",
      "         -5.8018e-02,  9.0445e-01,  9.0224e-01, -6.0721e-01, -3.0973e-01,\n",
      "          6.2910e-01, -5.8008e-02, -4.9836e-01, -1.1235e-01,  6.9557e-01,\n",
      "          5.9399e-01, -6.3096e-01, -4.0527e-01,  2.9566e-01, -6.1613e-01,\n",
      "         -5.6649e-01, -4.9101e-01, -1.1489e+00, -9.6510e-01, -1.3340e+00,\n",
      "         -9.6502e-01, -9.6539e-01,  4.0107e-01, -5.9567e-01, -3.0632e-02,\n",
      "         -1.1495e+00, -1.3344e+00,  1.3134e+00]]) tensor([[ 23.5000],\n",
      "        [116.3200],\n",
      "        [ 59.9000],\n",
      "        [ 39.1000],\n",
      "        [ 84.6000],\n",
      "        [ 28.7000],\n",
      "        [ 33.9000],\n",
      "        [ 58.6000],\n",
      "        [ 18.3000],\n",
      "        [ 56.0000]])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(train_set))\n",
    "print(X[:10], y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device() \n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = []\n",
    "count = 0\n",
    "for values in itertools.product(*params_set.values()):\n",
    "    param = dict(zip(params_set.keys(), values))\n",
    "    for optim_values in itertools.product(*param[\"optim_hparams\"].values()):\n",
    "        param_copy = param.copy()\n",
    "        param_copy[\"optim_hparams\"] = dict(zip(param[\"optim_hparams\"].keys(), optim_values))\n",
    "        param_grid.append(param_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 142304.2059, Valid loss: 2252.9457\n",
      "Epoch 1, Train loss: 2283.4174, Valid loss: 2199.8864\n",
      "Epoch 2, Train loss: 2188.3554, Valid loss: 2066.5113\n",
      "Epoch 3, Train loss: 2037.3818, Valid loss: 1908.5646\n",
      "Epoch 4, Train loss: 1876.3, Valid loss: 1747.4911\n",
      "Epoch 5, Train loss: 10122.8127, Valid loss: 1601.0471\n",
      "Epoch 6, Train loss: 1583.4212, Valid loss: 1473.3235\n",
      "Epoch 7, Train loss: 1459.9083, Valid loss: 1346.4664\n",
      "Epoch 8, Train loss: 1331.9941, Valid loss: 1231.5929\n",
      "Epoch 9, Train loss: 1222.3407, Valid loss: 1132.6547\n",
      "Epoch 10, Train loss: 1129.3283, Valid loss: 1047.005\n",
      "Epoch 11, Train loss: 1049.3113, Valid loss: 975.2698\n",
      "Epoch 12, Train loss: 982.7141, Valid loss: 915.6312\n",
      "Epoch 13, Train loss: 927.7229, Valid loss: 866.7219\n",
      "Epoch 14, Train loss: 882.3314, Valid loss: 827.6286\n",
      "Epoch 15, Train loss: 846.376, Valid loss: 795.9885\n",
      "Epoch 16, Train loss: 817.3182, Valid loss: 771.0157\n",
      "Epoch 17, Train loss: 794.7607, Valid loss: 751.0889\n",
      "Epoch 18, Train loss: 776.5052, Valid loss: 736.0229\n",
      "Epoch 19, Train loss: 762.8106, Valid loss: 724.2608\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 762.8106, minimum valid loss = 724.2608\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 77463.7727, Valid loss: 2016.5031\n",
      "Epoch 1, Train loss: 21135.612, Valid loss: 1084.8739\n",
      "Epoch 2, Train loss: 1467.359, Valid loss: 1289.9396\n",
      "Epoch 3, Train loss: 1410.1739, Valid loss: 1074.6684\n",
      "Epoch 4, Train loss: 1323.9303, Valid loss: 930.0232\n",
      "Epoch 5, Train loss: 1243.5741, Valid loss: 890.6151\n",
      "Epoch 6, Train loss: 1208.8581, Valid loss: 892.8094\n",
      "Epoch 7, Train loss: 1154.1519, Valid loss: 884.8358\n",
      "Epoch 8, Train loss: 1120.2253, Valid loss: 861.9937\n",
      "Epoch 9, Train loss: 1071.1037, Valid loss: 834.629\n",
      "Epoch 10, Train loss: 333161.8255, Valid loss: 1268.3973\n",
      "Epoch 11, Train loss: 1190.7172, Valid loss: 1274.4185\n",
      "Epoch 12, Train loss: 1203.2017, Valid loss: 1125.425\n",
      "Epoch 13, Train loss: 1110.5691, Valid loss: 850.4656\n",
      "Epoch 14, Train loss: 1097.8317, Valid loss: 1151.6821\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 9, training loss = 1071.1037, minimum valid loss = 834.629\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 751397.4121, Valid loss: 2271.9846\n",
      "Epoch 1, Train loss: 3243.3293, Valid loss: 2265.7036\n",
      "Epoch 2, Train loss: 2879.0718, Valid loss: 2213.3895\n",
      "Epoch 3, Train loss: 2562.8625, Valid loss: 2140.6262\n",
      "Epoch 4, Train loss: 2324.4153, Valid loss: 2062.4521\n",
      "Epoch 5, Train loss: 2068.8714, Valid loss: 1982.7928\n",
      "Epoch 6, Train loss: 1988.8507, Valid loss: 1902.7043\n",
      "Epoch 7, Train loss: 3025.2133, Valid loss: 1826.9194\n",
      "Epoch 8, Train loss: 1834.359, Valid loss: 1751.2054\n",
      "Epoch 9, Train loss: 1758.2988, Valid loss: 1677.1649\n",
      "Epoch 10, Train loss: 1685.0763, Valid loss: 1605.4492\n",
      "Epoch 11, Train loss: 1614.567, Valid loss: 1537.582\n",
      "Epoch 12, Train loss: 1548.0255, Valid loss: 1473.822\n",
      "Epoch 13, Train loss: 1485.7945, Valid loss: 1414.0792\n",
      "Epoch 14, Train loss: 1427.5837, Valid loss: 1358.7516\n",
      "Epoch 15, Train loss: 1373.8251, Valid loss: 1307.4967\n",
      "Epoch 16, Train loss: 1324.1633, Valid loss: 1260.4234\n",
      "Epoch 17, Train loss: 1278.6856, Valid loss: 1217.2047\n",
      "Epoch 18, Train loss: 1236.9401, Valid loss: 1178.0983\n",
      "Epoch 19, Train loss: 1199.1839, Valid loss: 1142.4667\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1199.1839, minimum valid loss = 1142.4667\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 28756.5302, Valid loss: 2180.3647\n",
      "Epoch 1, Train loss: 2236.4136, Valid loss: 2011.544\n",
      "Epoch 2, Train loss: 1979.2678, Valid loss: 1813.5787\n",
      "Epoch 3, Train loss: 1779.1638, Valid loss: 1641.0699\n",
      "Epoch 4, Train loss: 1422.6903, Valid loss: 830.0341\n",
      "Epoch 5, Train loss: 1210.8126, Valid loss: 1105.9298\n",
      "Epoch 6, Train loss: 1508.6121, Valid loss: 1346.5014\n",
      "Epoch 7, Train loss: 1314.3887, Valid loss: 974.5669\n",
      "Epoch 8, Train loss: 1213.47, Valid loss: 989.2545\n",
      "Epoch 9, Train loss: 1225.1481, Valid loss: 931.1707\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 4, training loss = 1422.6903, minimum valid loss = 830.0341\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 3640.7997, Valid loss: 1915.8034\n",
      "Epoch 1, Train loss: 1871.4071, Valid loss: 1665.4111\n",
      "Epoch 2, Train loss: 1688.1304, Valid loss: 1655.5216\n",
      "Epoch 3, Train loss: 1731.4479, Valid loss: 1726.9675\n",
      "Epoch 4, Train loss: 1779.1625, Valid loss: 1735.483\n",
      "Epoch 5, Train loss: 1763.1595, Valid loss: 1708.994\n",
      "Epoch 6, Train loss: 1749.0855, Valid loss: 1701.5999\n",
      "Epoch 7, Train loss: 2484.0132, Valid loss: 1834.8146\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 2, training loss = 1688.1304, minimum valid loss = 1655.5216\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1614.8561, Valid loss: 1009.7793\n",
      "Epoch 1, Train loss: 1205.527, Valid loss: 644.0246\n",
      "Epoch 2, Train loss: 1027.2117, Valid loss: 417.485\n",
      "Epoch 3, Train loss: 918.2262, Valid loss: 525.9888\n",
      "Epoch 4, Train loss: 832.8389, Valid loss: 755.4932\n",
      "Epoch 5, Train loss: 767.6025, Valid loss: 540.4174\n",
      "Epoch 6, Train loss: 705.7966, Valid loss: 321.9176\n",
      "Epoch 7, Train loss: 654.1822, Valid loss: 334.9455\n",
      "Epoch 8, Train loss: 609.7152, Valid loss: 300.7808\n",
      "Epoch 9, Train loss: 593.7743, Valid loss: 361.1447\n",
      "Epoch 10, Train loss: 578.4201, Valid loss: 291.1565\n",
      "Epoch 11, Train loss: 593.4434, Valid loss: 269.6112\n",
      "Epoch 12, Train loss: 587.0867, Valid loss: 563.4134\n",
      "Epoch 13, Train loss: 633.471, Valid loss: 410.3912\n",
      "Epoch 14, Train loss: 557.1946, Valid loss: 268.6076\n",
      "Epoch 15, Train loss: 523.4313, Valid loss: 202.5753\n",
      "Epoch 16, Train loss: 498.1625, Valid loss: 317.8223\n",
      "Epoch 17, Train loss: 496.3071, Valid loss: 185.7554\n",
      "Epoch 18, Train loss: 464.7392, Valid loss: 196.2845\n",
      "Epoch 19, Train loss: 462.0577, Valid loss: 173.9566\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 462.0577, minimum valid loss = 173.9566\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1217.6318, Valid loss: 523.291\n",
      "Epoch 1, Train loss: 802.6669, Valid loss: 543.2342\n",
      "Epoch 2, Train loss: 742.3919, Valid loss: 516.2133\n",
      "Epoch 3, Train loss: 700.4816, Valid loss: 425.4649\n",
      "Epoch 4, Train loss: 650.993, Valid loss: 225.3611\n",
      "Epoch 5, Train loss: 655.514, Valid loss: 352.9185\n",
      "Epoch 6, Train loss: 595.5529, Valid loss: 239.1127\n",
      "Epoch 7, Train loss: 545.2206, Valid loss: 199.9361\n",
      "Epoch 8, Train loss: 536.1393, Valid loss: 225.5865\n",
      "Epoch 9, Train loss: 521.78, Valid loss: 229.3968\n",
      "Epoch 10, Train loss: 519.8749, Valid loss: 189.4179\n",
      "Epoch 11, Train loss: 485.1915, Valid loss: 191.2135\n",
      "Epoch 12, Train loss: 476.2512, Valid loss: 215.8516\n",
      "Epoch 13, Train loss: 485.0979, Valid loss: 312.2787\n",
      "Epoch 14, Train loss: 468.2265, Valid loss: 236.9396\n",
      "Epoch 15, Train loss: 485.3448, Valid loss: 430.052\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 10, training loss = 519.8749, minimum valid loss = 189.4179\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1861.0833, Valid loss: 1025.3542\n",
      "Epoch 1, Train loss: 1231.5747, Valid loss: 444.3711\n",
      "Epoch 2, Train loss: 922.4547, Valid loss: 474.062\n",
      "Epoch 3, Train loss: 816.3209, Valid loss: 320.5726\n",
      "Epoch 4, Train loss: 782.4301, Valid loss: 303.2479\n",
      "Epoch 5, Train loss: 709.9709, Valid loss: 265.4733\n",
      "Epoch 6, Train loss: 671.0521, Valid loss: 421.6614\n",
      "Epoch 7, Train loss: 679.1833, Valid loss: 484.5354\n",
      "Epoch 8, Train loss: 664.6637, Valid loss: 439.0179\n",
      "Epoch 9, Train loss: 608.0268, Valid loss: 329.306\n",
      "Epoch 10, Train loss: 600.0793, Valid loss: 250.2712\n",
      "Epoch 11, Train loss: 626.7699, Valid loss: 365.0875\n",
      "Epoch 12, Train loss: 537.697, Valid loss: 180.8178\n",
      "Epoch 13, Train loss: 601.3213, Valid loss: 378.0316\n",
      "Epoch 14, Train loss: 578.7986, Valid loss: 294.3355\n",
      "Epoch 15, Train loss: 522.8992, Valid loss: 270.8829\n",
      "Epoch 16, Train loss: 485.2029, Valid loss: 208.4236\n",
      "Epoch 17, Train loss: 514.8379, Valid loss: 331.2176\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 12, training loss = 537.697, minimum valid loss = 180.8178\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1194.1344, Valid loss: 476.3695\n",
      "Epoch 1, Train loss: 907.6359, Valid loss: 742.6009\n",
      "Epoch 2, Train loss: 832.0724, Valid loss: 776.7483\n",
      "Epoch 3, Train loss: 695.2878, Valid loss: 326.9856\n",
      "Epoch 4, Train loss: 640.1935, Valid loss: 252.0095\n",
      "Epoch 5, Train loss: 627.8624, Valid loss: 237.3439\n",
      "Epoch 6, Train loss: 626.5267, Valid loss: 425.2535\n",
      "Epoch 7, Train loss: 659.356, Valid loss: 232.7627\n",
      "Epoch 8, Train loss: 619.4033, Valid loss: 230.163\n",
      "Epoch 9, Train loss: 614.1031, Valid loss: 235.253\n",
      "Epoch 10, Train loss: 620.0661, Valid loss: 361.6054\n",
      "Epoch 11, Train loss: 661.8186, Valid loss: 269.4758\n",
      "Epoch 12, Train loss: 606.3442, Valid loss: 236.2233\n",
      "Epoch 13, Train loss: 632.802, Valid loss: 307.9945\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 619.4033, minimum valid loss = 230.163\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1517.3727, Valid loss: 534.0691\n",
      "Epoch 1, Train loss: 1007.5463, Valid loss: 385.0213\n",
      "Epoch 2, Train loss: 868.3167, Valid loss: 624.1705\n",
      "Epoch 3, Train loss: 745.4227, Valid loss: 301.5101\n",
      "Epoch 4, Train loss: 677.3493, Valid loss: 360.1786\n",
      "Epoch 5, Train loss: 653.6326, Valid loss: 301.9105\n",
      "Epoch 6, Train loss: 700.0732, Valid loss: 402.1281\n",
      "Epoch 7, Train loss: 720.1473, Valid loss: 384.5356\n",
      "Epoch 8, Train loss: 706.9771, Valid loss: 291.6389\n",
      "Epoch 9, Train loss: 675.9618, Valid loss: 305.7895\n",
      "Epoch 10, Train loss: 661.4261, Valid loss: 570.3102\n",
      "Epoch 11, Train loss: 716.1748, Valid loss: 488.39\n",
      "Epoch 12, Train loss: 664.9092, Valid loss: 282.0723\n",
      "Epoch 13, Train loss: 709.641, Valid loss: 422.8854\n",
      "Epoch 14, Train loss: 688.8374, Valid loss: 220.7475\n",
      "Epoch 15, Train loss: 701.7009, Valid loss: 464.3819\n",
      "Epoch 16, Train loss: 661.5215, Valid loss: 201.5913\n",
      "Epoch 17, Train loss: 657.9208, Valid loss: 327.9116\n",
      "Epoch 18, Train loss: 698.1007, Valid loss: 292.0815\n",
      "Epoch 19, Train loss: 703.6742, Valid loss: 289.5678\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 661.5215, minimum valid loss = 201.5913\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1389.432, Valid loss: 767.9963\n",
      "Epoch 1, Train loss: 1122.3951, Valid loss: 704.9713\n",
      "Epoch 2, Train loss: 1016.0076, Valid loss: 473.9322\n",
      "Epoch 3, Train loss: 961.8779, Valid loss: 451.0524\n",
      "Epoch 4, Train loss: 889.5555, Valid loss: 442.036\n",
      "Epoch 5, Train loss: 877.5531, Valid loss: 462.039\n",
      "Epoch 6, Train loss: 881.288, Valid loss: 443.3829\n",
      "Epoch 7, Train loss: 842.8483, Valid loss: 448.4235\n",
      "Epoch 8, Train loss: 869.2387, Valid loss: 304.3852\n",
      "Epoch 9, Train loss: 807.1392, Valid loss: 436.9634\n",
      "Epoch 10, Train loss: 788.9942, Valid loss: 350.9008\n",
      "Epoch 11, Train loss: 761.4192, Valid loss: 344.8059\n",
      "Epoch 12, Train loss: 756.3697, Valid loss: 424.4465\n",
      "Epoch 13, Train loss: 758.3219, Valid loss: 350.3219\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 869.2387, minimum valid loss = 304.3852\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1823.9821, Valid loss: 952.6081\n",
      "Epoch 1, Train loss: 1411.5053, Valid loss: 860.8709\n",
      "Epoch 2, Train loss: 1317.8003, Valid loss: 705.5629\n",
      "Epoch 3, Train loss: 1116.33, Valid loss: 529.3771\n",
      "Epoch 4, Train loss: 1018.0557, Valid loss: 723.1244\n",
      "Epoch 5, Train loss: 974.3617, Valid loss: 447.4591\n",
      "Epoch 6, Train loss: 946.2111, Valid loss: 579.4295\n",
      "Epoch 7, Train loss: 941.4275, Valid loss: 677.8424\n",
      "Epoch 8, Train loss: 904.3313, Valid loss: 560.671\n",
      "Epoch 9, Train loss: 871.3896, Valid loss: 407.8515\n",
      "Epoch 10, Train loss: 811.0217, Valid loss: 353.2215\n",
      "Epoch 11, Train loss: 802.4276, Valid loss: 353.3808\n",
      "Epoch 12, Train loss: 794.274, Valid loss: 398.8357\n",
      "Epoch 13, Train loss: 779.8106, Valid loss: 401.4908\n",
      "Epoch 14, Train loss: 776.0612, Valid loss: 428.1028\n",
      "Epoch 15, Train loss: 748.3177, Valid loss: 374.2923\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 10, training loss = 811.0217, minimum valid loss = 353.2215\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1121.3335, Valid loss: 413.7032\n",
      "Epoch 1, Train loss: 784.9351, Valid loss: 414.6268\n",
      "Epoch 2, Train loss: 701.3397, Valid loss: 248.9646\n",
      "Epoch 3, Train loss: 674.4793, Valid loss: 231.3655\n",
      "Epoch 4, Train loss: 638.2787, Valid loss: 358.2485\n",
      "Epoch 5, Train loss: 635.0117, Valid loss: 247.0254\n",
      "Epoch 6, Train loss: 655.828, Valid loss: 230.39\n",
      "Epoch 7, Train loss: 629.8768, Valid loss: 295.6546\n",
      "Epoch 8, Train loss: 610.4125, Valid loss: 206.7705\n",
      "Epoch 9, Train loss: 577.5367, Valid loss: 229.508\n",
      "Epoch 10, Train loss: 598.2715, Valid loss: 239.9029\n",
      "Epoch 11, Train loss: 560.7557, Valid loss: 413.6657\n",
      "Epoch 12, Train loss: 529.8106, Valid loss: 206.9676\n",
      "Epoch 13, Train loss: 529.8077, Valid loss: 221.3744\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 610.4125, minimum valid loss = 206.7705\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1509.6256, Valid loss: 484.8281\n",
      "Epoch 1, Train loss: 959.9362, Valid loss: 348.3124\n",
      "Epoch 2, Train loss: 847.1289, Valid loss: 550.5508\n",
      "Epoch 3, Train loss: 798.2018, Valid loss: 405.9185\n",
      "Epoch 4, Train loss: 753.7167, Valid loss: 376.2868\n",
      "Epoch 5, Train loss: 707.9639, Valid loss: 232.328\n",
      "Epoch 6, Train loss: 654.4637, Valid loss: 226.0017\n",
      "Epoch 7, Train loss: 714.4851, Valid loss: 283.875\n",
      "Epoch 8, Train loss: 650.7318, Valid loss: 347.3959\n",
      "Epoch 9, Train loss: 614.3498, Valid loss: 323.8151\n",
      "Epoch 10, Train loss: 593.3825, Valid loss: 331.7742\n",
      "Epoch 11, Train loss: 636.3157, Valid loss: 350.7526\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 654.4637, minimum valid loss = 226.0017\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1566.371, Valid loss: 1033.9951\n",
      "Epoch 1, Train loss: 1172.637, Valid loss: 567.7059\n",
      "Epoch 2, Train loss: 749.5687, Valid loss: 442.7589\n",
      "Epoch 3, Train loss: 717.1166, Valid loss: 475.5553\n",
      "Epoch 4, Train loss: 702.8685, Valid loss: 338.7291\n",
      "Epoch 5, Train loss: 662.0811, Valid loss: 377.5866\n",
      "Epoch 6, Train loss: 685.1867, Valid loss: 293.807\n",
      "Epoch 7, Train loss: 659.0624, Valid loss: 362.8708\n",
      "Epoch 8, Train loss: 630.0517, Valid loss: 260.6211\n",
      "Epoch 9, Train loss: 665.2647, Valid loss: 332.7679\n",
      "Epoch 10, Train loss: 634.3162, Valid loss: 290.2332\n",
      "Epoch 11, Train loss: 680.6347, Valid loss: 321.7152\n",
      "Epoch 12, Train loss: 662.1389, Valid loss: 377.5422\n",
      "Epoch 13, Train loss: 657.8874, Valid loss: 265.411\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 630.0517, minimum valid loss = 260.6211\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1548.5451, Valid loss: 602.4437\n",
      "Epoch 1, Train loss: 1006.4931, Valid loss: 404.296\n",
      "Epoch 2, Train loss: 883.7857, Valid loss: 361.7212\n",
      "Epoch 3, Train loss: 796.7844, Valid loss: 361.7581\n",
      "Epoch 4, Train loss: 732.7138, Valid loss: 307.3392\n",
      "Epoch 5, Train loss: 727.8988, Valid loss: 278.8016\n",
      "Epoch 6, Train loss: 715.8112, Valid loss: 300.8969\n",
      "Epoch 7, Train loss: 699.6758, Valid loss: 275.0222\n",
      "Epoch 8, Train loss: 687.535, Valid loss: 329.6337\n",
      "Epoch 9, Train loss: 664.24, Valid loss: 275.6611\n",
      "Epoch 10, Train loss: 697.488, Valid loss: 293.2747\n",
      "Epoch 11, Train loss: 669.6951, Valid loss: 276.9969\n",
      "Epoch 12, Train loss: 684.7307, Valid loss: 252.4365\n",
      "Epoch 13, Train loss: 646.256, Valid loss: 270.7828\n",
      "Epoch 14, Train loss: 655.0371, Valid loss: 284.3346\n",
      "Epoch 15, Train loss: 645.7613, Valid loss: 255.3915\n",
      "Epoch 16, Train loss: 630.0132, Valid loss: 229.7777\n",
      "Epoch 17, Train loss: 660.9636, Valid loss: 313.2681\n",
      "Epoch 18, Train loss: 697.392, Valid loss: 288.4636\n",
      "Epoch 19, Train loss: 632.6471, Valid loss: 231.7112\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 630.0132, minimum valid loss = 229.7777\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1965.5578, Valid loss: 1320.7076\n",
      "Epoch 1, Train loss: 1433.4087, Valid loss: 871.5634\n",
      "Epoch 2, Train loss: 1085.5449, Valid loss: 476.1235\n",
      "Epoch 3, Train loss: 945.9538, Valid loss: 473.4549\n",
      "Epoch 4, Train loss: 938.7022, Valid loss: 461.1784\n",
      "Epoch 5, Train loss: 871.5084, Valid loss: 391.5457\n",
      "Epoch 6, Train loss: 874.7859, Valid loss: 326.3465\n",
      "Epoch 7, Train loss: 836.2393, Valid loss: 347.9708\n",
      "Epoch 8, Train loss: 779.5806, Valid loss: 400.0692\n",
      "Epoch 9, Train loss: 802.542, Valid loss: 308.1647\n",
      "Epoch 10, Train loss: 791.9978, Valid loss: 317.349\n",
      "Epoch 11, Train loss: 779.5159, Valid loss: 383.4124\n",
      "Epoch 12, Train loss: 764.9018, Valid loss: 313.1367\n",
      "Epoch 13, Train loss: 725.3892, Valid loss: 327.7003\n",
      "Epoch 14, Train loss: 722.0988, Valid loss: 301.2749\n",
      "Epoch 15, Train loss: 742.7608, Valid loss: 311.2829\n",
      "Epoch 16, Train loss: 747.7474, Valid loss: 368.8405\n",
      "Epoch 17, Train loss: 756.6459, Valid loss: 333.0687\n",
      "Epoch 18, Train loss: 758.5726, Valid loss: 332.9455\n",
      "Epoch 19, Train loss: 739.4462, Valid loss: 403.7937\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 14, training loss = 722.0988, minimum valid loss = 301.2749\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1724.1149, Valid loss: 726.1841\n",
      "Epoch 1, Train loss: 1178.0787, Valid loss: 567.5314\n",
      "Epoch 2, Train loss: 939.265, Valid loss: 571.0145\n",
      "Epoch 3, Train loss: 911.3514, Valid loss: 415.8807\n",
      "Epoch 4, Train loss: 865.2366, Valid loss: 348.0475\n",
      "Epoch 5, Train loss: 809.7619, Valid loss: 394.3969\n",
      "Epoch 6, Train loss: 789.2396, Valid loss: 404.0884\n",
      "Epoch 7, Train loss: 780.4132, Valid loss: 354.5033\n",
      "Epoch 8, Train loss: 768.352, Valid loss: 391.2967\n",
      "Epoch 9, Train loss: 776.3868, Valid loss: 372.481\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 4, training loss = 865.2366, minimum valid loss = 348.0475\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1888.1423, Valid loss: 1020.316\n",
      "Epoch 1, Train loss: 1457.9198, Valid loss: 865.6104\n",
      "Epoch 2, Train loss: 1187.4368, Valid loss: 678.899\n",
      "Epoch 3, Train loss: 1071.771, Valid loss: 578.0748\n",
      "Epoch 4, Train loss: 1036.7608, Valid loss: 518.3855\n",
      "Epoch 5, Train loss: 980.651, Valid loss: 555.3665\n",
      "Epoch 6, Train loss: 954.008, Valid loss: 460.5013\n",
      "Epoch 7, Train loss: 941.6625, Valid loss: 477.6769\n",
      "Epoch 8, Train loss: 910.6682, Valid loss: 506.882\n",
      "Epoch 9, Train loss: 930.4071, Valid loss: 447.9263\n",
      "Epoch 10, Train loss: 922.6436, Valid loss: 542.3371\n",
      "Epoch 11, Train loss: 925.6505, Valid loss: 461.7518\n",
      "Epoch 12, Train loss: 976.4445, Valid loss: 420.3872\n",
      "Epoch 13, Train loss: 885.6514, Valid loss: 516.918\n",
      "Epoch 14, Train loss: 915.2986, Valid loss: 431.0107\n",
      "Epoch 15, Train loss: 906.7154, Valid loss: 526.2618\n",
      "Epoch 16, Train loss: 876.3883, Valid loss: 469.9277\n",
      "Epoch 17, Train loss: 865.3718, Valid loss: 461.284\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 12, training loss = 976.4445, minimum valid loss = 420.3872\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1605.4823, Valid loss: 664.1679\n",
      "Epoch 1, Train loss: 1111.4865, Valid loss: 535.7428\n",
      "Epoch 2, Train loss: 953.8373, Valid loss: 560.062\n",
      "Epoch 3, Train loss: 822.4121, Valid loss: 497.1529\n",
      "Epoch 4, Train loss: 759.6091, Valid loss: 431.1011\n",
      "Epoch 5, Train loss: 768.2882, Valid loss: 347.8959\n",
      "Epoch 6, Train loss: 674.1174, Valid loss: 276.2197\n",
      "Epoch 7, Train loss: 709.8264, Valid loss: 299.5424\n",
      "Epoch 8, Train loss: 663.121, Valid loss: 272.2062\n",
      "Epoch 9, Train loss: 656.2304, Valid loss: 257.0127\n",
      "Epoch 10, Train loss: 681.4886, Valid loss: 295.9741\n",
      "Epoch 11, Train loss: 670.2221, Valid loss: 458.3266\n",
      "Epoch 12, Train loss: 664.5023, Valid loss: 300.4159\n",
      "Epoch 13, Train loss: 654.5043, Valid loss: 279.7448\n",
      "Epoch 14, Train loss: 656.5172, Valid loss: 286.9962\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 9, training loss = 656.2304, minimum valid loss = 257.0127\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2010.6094, Valid loss: 1641.4178\n",
      "Epoch 1, Train loss: 1615.5391, Valid loss: 953.2335\n",
      "Epoch 2, Train loss: 1438.9114, Valid loss: 926.4636\n",
      "Epoch 3, Train loss: 1212.8015, Valid loss: 592.1332\n",
      "Epoch 4, Train loss: 1083.2466, Valid loss: 541.082\n",
      "Epoch 5, Train loss: 1073.1096, Valid loss: 552.9906\n",
      "Epoch 6, Train loss: 1004.6264, Valid loss: 498.1214\n",
      "Epoch 7, Train loss: 1019.7745, Valid loss: 454.9641\n",
      "Epoch 8, Train loss: 1004.4575, Valid loss: 503.9976\n",
      "Epoch 9, Train loss: 953.8396, Valid loss: 474.7763\n",
      "Epoch 10, Train loss: 950.387, Valid loss: 540.4513\n",
      "Epoch 11, Train loss: 936.4963, Valid loss: 511.9803\n",
      "Epoch 12, Train loss: 950.6558, Valid loss: 463.9881\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 7, training loss = 1019.7745, minimum valid loss = 454.9641\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2038.9398, Valid loss: 1689.8189\n",
      "Epoch 1, Train loss: 1636.7913, Valid loss: 956.827\n",
      "Epoch 2, Train loss: 1429.3864, Valid loss: 859.4693\n",
      "Epoch 3, Train loss: 1244.6132, Valid loss: 660.0207\n",
      "Epoch 4, Train loss: 1115.3934, Valid loss: 609.5806\n",
      "Epoch 5, Train loss: 1044.8999, Valid loss: 587.1763\n",
      "Epoch 6, Train loss: 981.7009, Valid loss: 501.4089\n",
      "Epoch 7, Train loss: 1012.7788, Valid loss: 538.8012\n",
      "Epoch 8, Train loss: 1003.7066, Valid loss: 513.9302\n",
      "Epoch 9, Train loss: 908.813, Valid loss: 407.5322\n",
      "Epoch 10, Train loss: 824.3746, Valid loss: 333.6724\n",
      "Epoch 11, Train loss: 864.3322, Valid loss: 346.9328\n",
      "Epoch 12, Train loss: 814.3274, Valid loss: 385.8535\n",
      "Epoch 13, Train loss: 786.2504, Valid loss: 340.6683\n",
      "Epoch 14, Train loss: 809.4795, Valid loss: 324.2064\n",
      "Epoch 15, Train loss: 762.122, Valid loss: 369.6712\n",
      "Epoch 16, Train loss: 770.0896, Valid loss: 349.1907\n",
      "Epoch 17, Train loss: 745.0059, Valid loss: 340.0385\n",
      "Epoch 18, Train loss: 783.5066, Valid loss: 314.515\n",
      "Epoch 19, Train loss: 760.8768, Valid loss: 339.3904\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 783.5066, minimum valid loss = 314.515\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2228.0535, Valid loss: 2165.4121\n",
      "Epoch 1, Train loss: 2206.7722, Valid loss: 2160.3546\n",
      "Epoch 2, Train loss: 2201.4248, Valid loss: 2155.3959\n",
      "Epoch 3, Train loss: 2196.4457, Valid loss: 2150.4386\n",
      "Epoch 4, Train loss: 2191.4457, Valid loss: 2145.5184\n",
      "Epoch 5, Train loss: 2186.5077, Valid loss: 2140.5823\n",
      "Epoch 6, Train loss: 2181.5562, Valid loss: 2135.6694\n",
      "Epoch 7, Train loss: 2176.6358, Valid loss: 2130.7633\n",
      "Epoch 8, Train loss: 2171.7261, Valid loss: 2125.8717\n",
      "Epoch 9, Train loss: 2166.808, Valid loss: 2121.0182\n",
      "Epoch 10, Train loss: 2161.9315, Valid loss: 2116.1624\n",
      "Epoch 11, Train loss: 2157.0544, Valid loss: 2111.3226\n",
      "Epoch 12, Train loss: 2152.1986, Valid loss: 2106.4958\n",
      "Epoch 13, Train loss: 2147.3524, Valid loss: 2101.6825\n",
      "Epoch 14, Train loss: 2142.5277, Valid loss: 2096.8773\n",
      "Epoch 15, Train loss: 2137.7067, Valid loss: 2092.0884\n",
      "Epoch 16, Train loss: 2132.907, Valid loss: 2087.3075\n",
      "Epoch 17, Train loss: 2128.1096, Valid loss: 2082.5508\n",
      "Epoch 18, Train loss: 2123.3425, Valid loss: 2077.7992\n",
      "Epoch 19, Train loss: 2118.5734, Valid loss: 2073.0737\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2118.5734, minimum valid loss = 2073.0737\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1983.3914, Valid loss: 1520.4287\n",
      "Epoch 1, Train loss: 1545.2745, Valid loss: 839.341\n",
      "Epoch 2, Train loss: 1276.4832, Valid loss: 743.963\n",
      "Epoch 3, Train loss: 1094.0975, Valid loss: 614.2194\n",
      "Epoch 4, Train loss: 1057.0271, Valid loss: 586.1607\n",
      "Epoch 5, Train loss: 1023.7529, Valid loss: 588.7677\n",
      "Epoch 6, Train loss: 1041.4365, Valid loss: 578.1417\n",
      "Epoch 7, Train loss: 958.5813, Valid loss: 508.5077\n",
      "Epoch 8, Train loss: 1011.2269, Valid loss: 518.7279\n",
      "Epoch 9, Train loss: 937.3825, Valid loss: 395.3528\n",
      "Epoch 10, Train loss: 878.9979, Valid loss: 372.6119\n",
      "Epoch 11, Train loss: 838.9006, Valid loss: 366.1654\n",
      "Epoch 12, Train loss: 811.4533, Valid loss: 347.8399\n",
      "Epoch 13, Train loss: 809.1019, Valid loss: 327.9794\n",
      "Epoch 14, Train loss: 796.7461, Valid loss: 371.2504\n",
      "Epoch 15, Train loss: 768.9537, Valid loss: 323.7108\n",
      "Epoch 16, Train loss: 779.0386, Valid loss: 342.5845\n",
      "Epoch 17, Train loss: 766.3143, Valid loss: 344.7704\n",
      "Epoch 18, Train loss: 774.3065, Valid loss: 300.9398\n",
      "Epoch 19, Train loss: 787.7142, Valid loss: 348.5983\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 774.3065, minimum valid loss = 300.9398\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2099.5161, Valid loss: 1879.5356\n",
      "Epoch 1, Train loss: 1760.0902, Valid loss: 1209.4057\n",
      "Epoch 2, Train loss: 1448.7813, Valid loss: 844.7896\n",
      "Epoch 3, Train loss: 1176.8218, Valid loss: 620.221\n",
      "Epoch 4, Train loss: 1016.659, Valid loss: 555.1518\n",
      "Epoch 5, Train loss: 923.1781, Valid loss: 492.5322\n",
      "Epoch 6, Train loss: 885.4769, Valid loss: 437.8942\n",
      "Epoch 7, Train loss: 858.9442, Valid loss: 390.1278\n",
      "Epoch 8, Train loss: 819.1445, Valid loss: 418.8971\n",
      "Epoch 9, Train loss: 807.4439, Valid loss: 366.8172\n",
      "Epoch 10, Train loss: 813.9137, Valid loss: 424.1773\n",
      "Epoch 11, Train loss: 814.9872, Valid loss: 392.7837\n",
      "Epoch 12, Train loss: 829.4849, Valid loss: 403.9861\n",
      "Epoch 13, Train loss: 835.1883, Valid loss: 358.433\n",
      "Epoch 14, Train loss: 769.401, Valid loss: 404.517\n",
      "Epoch 15, Train loss: 788.6331, Valid loss: 418.5875\n",
      "Epoch 16, Train loss: 780.5581, Valid loss: 350.7484\n",
      "Epoch 17, Train loss: 788.4221, Valid loss: 335.6765\n",
      "Epoch 18, Train loss: 777.0107, Valid loss: 395.197\n",
      "Epoch 19, Train loss: 788.7381, Valid loss: 320.0814\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 788.7381, minimum valid loss = 320.0814\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 66734.0353, Valid loss: 2214.2036\n",
      "Epoch 1, Train loss: 2215.4633, Valid loss: 2105.3504\n",
      "Epoch 2, Train loss: 2076.0354, Valid loss: 1939.0886\n",
      "Epoch 3, Train loss: 1899.6485, Valid loss: 1762.6909\n",
      "Epoch 4, Train loss: 1723.9335, Valid loss: 1590.2414\n",
      "Epoch 5, Train loss: 1556.2161, Valid loss: 1432.2891\n",
      "Epoch 6, Train loss: 1406.1127, Valid loss: 1292.142\n",
      "Epoch 7, Train loss: 1273.6015, Valid loss: 1171.826\n",
      "Epoch 8, Train loss: 1161.3408, Valid loss: 1069.9177\n",
      "Epoch 9, Train loss: 1066.3246, Valid loss: 986.1713\n",
      "Epoch 10, Train loss: 989.0656, Valid loss: 917.5576\n",
      "Epoch 11, Train loss: 926.4528, Valid loss: 862.4781\n",
      "Epoch 12, Train loss: 876.4844, Valid loss: 819.2688\n",
      "Epoch 13, Train loss: 837.179, Valid loss: 785.963\n",
      "Epoch 14, Train loss: 807.151, Valid loss: 760.399\n",
      "Epoch 15, Train loss: 783.7999, Valid loss: 741.4094\n",
      "Epoch 16, Train loss: 766.8265, Valid loss: 726.8076\n",
      "Epoch 17, Train loss: 753.9098, Valid loss: 715.9548\n",
      "Epoch 18, Train loss: 375387.1906, Valid loss: 747.9599\n",
      "Epoch 19, Train loss: 961.4556, Valid loss: 763.9902\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 753.9098, minimum valid loss = 715.9548\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 16435.1983, Valid loss: 2139.1873\n",
      "Epoch 1, Train loss: 2117.2684, Valid loss: 1923.2531\n",
      "Epoch 2, Train loss: 1857.2495, Valid loss: 1672.3378\n",
      "Epoch 3, Train loss: 1610.241, Valid loss: 1443.6532\n",
      "Epoch 4, Train loss: 1391.6933, Valid loss: 1249.9785\n",
      "Epoch 5, Train loss: 1214.444, Valid loss: 1128.5728\n",
      "Epoch 6, Train loss: 1107.7404, Valid loss: 975.1398\n",
      "Epoch 7, Train loss: 967.931, Valid loss: 350802.2895\n",
      "Epoch 8, Train loss: 628713.6431, Valid loss: 868.0385\n",
      "Epoch 9, Train loss: 800485.8692, Valid loss: 879.9322\n",
      "Epoch 10, Train loss: 941.3355, Valid loss: 920.6984\n",
      "Epoch 11, Train loss: 953.4619, Valid loss: 909.7407\n",
      "Epoch 12, Train loss: 934.3427, Valid loss: 886.0287\n",
      "Epoch 13, Train loss: 909.0479, Valid loss: 860.5049\n",
      "Epoch 14, Train loss: 884.1523, Valid loss: 836.5142\n",
      "Epoch 15, Train loss: 861.0272, Valid loss: 815.4764\n",
      "Epoch 16, Train loss: 840.789, Valid loss: 796.9894\n",
      "Epoch 17, Train loss: 823.0778, Valid loss: 780.8002\n",
      "Epoch 18, Train loss: 807.4678, Valid loss: 767.055\n",
      "Epoch 19, Train loss: 794.4049, Valid loss: 754.8606\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 794.4049, minimum valid loss = 754.8606\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 140825.581, Valid loss: 2250.7453\n",
      "Epoch 1, Train loss: 2268.8316, Valid loss: 2178.3766\n",
      "Epoch 2, Train loss: 9962.5815, Valid loss: 2074.1844\n",
      "Epoch 3, Train loss: 2057.9879, Valid loss: 1941.5151\n",
      "Epoch 4, Train loss: 1920.3644, Valid loss: 1802.295\n",
      "Epoch 5, Train loss: 1782.3798, Valid loss: 1669.1265\n",
      "Epoch 6, Train loss: 1653.982, Valid loss: 1546.1126\n",
      "Epoch 7, Train loss: 1534.6831, Valid loss: 1435.9427\n",
      "Epoch 8, Train loss: 1430.1223, Valid loss: 1339.0349\n",
      "Epoch 9, Train loss: 1338.5449, Valid loss: 1255.6495\n",
      "Epoch 10, Train loss: 1259.9434, Valid loss: 1184.0517\n",
      "Epoch 11, Train loss: 1192.992, Valid loss: 1123.4682\n",
      "Epoch 12, Train loss: 1136.6785, Valid loss: 1072.8095\n",
      "Epoch 13, Train loss: 1089.3822, Valid loss: 1030.3935\n",
      "Epoch 14, Train loss: 1050.1689, Valid loss: 995.0612\n",
      "Epoch 15, Train loss: 1021.1345, Valid loss: 966.5692\n",
      "Epoch 16, Train loss: 991.2247, Valid loss: 943.3266\n",
      "Epoch 17, Train loss: 969.7004, Valid loss: 923.9644\n",
      "Epoch 18, Train loss: 951.9244, Valid loss: 908.5654\n",
      "Epoch 19, Train loss: 937.9022, Valid loss: 896.3677\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 937.9022, minimum valid loss = 896.3677\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 773932.8192, Valid loss: 4558.2752\n",
      "Epoch 1, Train loss: 2694.552, Valid loss: 2275.304\n",
      "Epoch 2, Train loss: 2299.6498, Valid loss: 2227.758\n",
      "Epoch 3, Train loss: 2242.7183, Valid loss: 2163.781\n",
      "Epoch 4, Train loss: 2193.1966, Valid loss: 2094.2354\n",
      "Epoch 5, Train loss: 2107.695, Valid loss: 2024.1415\n",
      "Epoch 6, Train loss: 2035.716, Valid loss: 1956.3425\n",
      "Epoch 7, Train loss: 1969.1925, Valid loss: 1895.1093\n",
      "Epoch 8, Train loss: 1907.2769, Valid loss: 1833.4231\n",
      "Epoch 9, Train loss: 1849.8849, Valid loss: 1779.5829\n",
      "Epoch 10, Train loss: 1798.1206, Valid loss: 1730.74\n",
      "Epoch 11, Train loss: 1751.5233, Valid loss: 1687.1447\n",
      "Epoch 12, Train loss: 1710.0923, Valid loss: 1649.1243\n",
      "Epoch 13, Train loss: 1674.0704, Valid loss: 1615.3656\n",
      "Epoch 14, Train loss: 1642.329, Valid loss: 1586.0999\n",
      "Epoch 15, Train loss: 1614.7428, Valid loss: 1561.249\n",
      "Epoch 16, Train loss: 1591.4294, Valid loss: 1540.0606\n",
      "Epoch 17, Train loss: 1571.6751, Valid loss: 1521.7759\n",
      "Epoch 18, Train loss: 1554.5156, Valid loss: 1506.3477\n",
      "Epoch 19, Train loss: 1540.3591, Valid loss: 1493.9533\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1540.3591, minimum valid loss = 1493.9533\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 88448.5321, Valid loss: 2222.9461\n",
      "Epoch 1, Train loss: 2438.8835, Valid loss: 2130.253\n",
      "Epoch 2, Train loss: 2113.8228, Valid loss: 1989.6548\n",
      "Epoch 3, Train loss: 2039.4369, Valid loss: 1867.1645\n",
      "Epoch 4, Train loss: 1868.8539, Valid loss: 1783.3079\n",
      "Epoch 5, Train loss: 1801.113, Valid loss: 1737.0825\n",
      "Epoch 6, Train loss: 1767.8962, Valid loss: 1716.4326\n",
      "Epoch 7, Train loss: 1754.395, Valid loss: 1710.7712\n",
      "Epoch 8, Train loss: 1751.7381, Valid loss: 1711.091\n",
      "Epoch 9, Train loss: 1754.4214, Valid loss: 1715.0302\n",
      "Epoch 10, Train loss: 1756.7012, Valid loss: 1716.0119\n",
      "Epoch 11, Train loss: 1756.7154, Valid loss: 1715.5084\n",
      "Epoch 12, Train loss: 1757.9094, Valid loss: 1717.0459\n",
      "Epoch 13, Train loss: 1757.6254, Valid loss: 1716.0794\n",
      "Epoch 14, Train loss: 1757.5433, Valid loss: 1716.4875\n",
      "Epoch 15, Train loss: 1757.0069, Valid loss: 1715.3839\n",
      "Epoch 16, Train loss: 1757.3357, Valid loss: 1716.8927\n",
      "Epoch 17, Train loss: 1758.2042, Valid loss: 1716.6687\n",
      "Epoch 18, Train loss: 1757.6631, Valid loss: 1715.9642\n",
      "Epoch 19, Train loss: 1757.5073, Valid loss: 1715.8301\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 7, training loss = 1754.395, minimum valid loss = 1710.7712\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1239.3512, Valid loss: 525.9965\n",
      "Epoch 1, Train loss: 898.0488, Valid loss: 468.0798\n",
      "Epoch 2, Train loss: 742.6716, Valid loss: 374.25\n",
      "Epoch 3, Train loss: 713.4783, Valid loss: 204.375\n",
      "Epoch 4, Train loss: 693.3794, Valid loss: 240.2691\n",
      "Epoch 5, Train loss: 593.0899, Valid loss: 216.2493\n",
      "Epoch 6, Train loss: 567.1728, Valid loss: 243.4969\n",
      "Epoch 7, Train loss: 553.2736, Valid loss: 300.194\n",
      "Epoch 8, Train loss: 554.5767, Valid loss: 291.318\n",
      "Epoch 9, Train loss: 548.3455, Valid loss: 188.7411\n",
      "Epoch 10, Train loss: 512.3659, Valid loss: 280.7546\n",
      "Epoch 11, Train loss: 525.1939, Valid loss: 176.2938\n",
      "Epoch 12, Train loss: 567.3567, Valid loss: 327.3812\n",
      "Epoch 13, Train loss: 497.1835, Valid loss: 228.2136\n",
      "Epoch 14, Train loss: 512.119, Valid loss: 258.786\n",
      "Epoch 15, Train loss: 478.9447, Valid loss: 313.0627\n",
      "Epoch 16, Train loss: 490.2924, Valid loss: 287.9267\n",
      "Epoch 17, Train loss: 468.0625, Valid loss: 220.386\n",
      "Epoch 18, Train loss: 460.6029, Valid loss: 361.3075\n",
      "Epoch 19, Train loss: 465.7887, Valid loss: 181.84\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 11, training loss = 525.1939, minimum valid loss = 176.2938\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1492.8068, Valid loss: 665.2247\n",
      "Epoch 1, Train loss: 1006.8402, Valid loss: 444.6009\n",
      "Epoch 2, Train loss: 810.968, Valid loss: 561.3456\n",
      "Epoch 3, Train loss: 757.4735, Valid loss: 440.7785\n",
      "Epoch 4, Train loss: 703.6438, Valid loss: 353.1413\n",
      "Epoch 5, Train loss: 646.7557, Valid loss: 258.6055\n",
      "Epoch 6, Train loss: 598.8087, Valid loss: 245.8227\n",
      "Epoch 7, Train loss: 598.8513, Valid loss: 245.0419\n",
      "Epoch 8, Train loss: 576.2896, Valid loss: 230.6193\n",
      "Epoch 9, Train loss: 534.2072, Valid loss: 262.252\n",
      "Epoch 10, Train loss: 525.6156, Valid loss: 244.6121\n",
      "Epoch 11, Train loss: 509.5913, Valid loss: 201.5122\n",
      "Epoch 12, Train loss: 507.8996, Valid loss: 192.2885\n",
      "Epoch 13, Train loss: 506.1723, Valid loss: 183.5856\n",
      "Epoch 14, Train loss: 486.1999, Valid loss: 288.4696\n",
      "Epoch 15, Train loss: 477.6787, Valid loss: 201.238\n",
      "Epoch 16, Train loss: 479.9646, Valid loss: 258.2959\n",
      "Epoch 17, Train loss: 466.0837, Valid loss: 335.8631\n",
      "Epoch 18, Train loss: 476.6256, Valid loss: 219.8372\n",
      "Epoch 19, Train loss: 471.3152, Valid loss: 176.8063\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 471.3152, minimum valid loss = 176.8063\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2115.4213, Valid loss: 856.5757\n",
      "Epoch 1, Train loss: 1096.4516, Valid loss: 595.605\n",
      "Epoch 2, Train loss: 938.1634, Valid loss: 651.8908\n",
      "Epoch 3, Train loss: 767.8874, Valid loss: 349.8641\n",
      "Epoch 4, Train loss: 715.6095, Valid loss: 436.4529\n",
      "Epoch 5, Train loss: 663.629, Valid loss: 275.9987\n",
      "Epoch 6, Train loss: 609.9397, Valid loss: 238.9599\n",
      "Epoch 7, Train loss: 598.3088, Valid loss: 310.0751\n",
      "Epoch 8, Train loss: 627.944, Valid loss: 226.62\n",
      "Epoch 9, Train loss: 573.2576, Valid loss: 314.8498\n",
      "Epoch 10, Train loss: 600.6954, Valid loss: 280.7935\n",
      "Epoch 11, Train loss: 582.1065, Valid loss: 277.8148\n",
      "Epoch 12, Train loss: 565.1999, Valid loss: 242.5267\n",
      "Epoch 13, Train loss: 529.1339, Valid loss: 317.3486\n",
      "Epoch 14, Train loss: 531.2128, Valid loss: 263.5533\n",
      "Epoch 15, Train loss: 501.4652, Valid loss: 228.0732\n",
      "Epoch 16, Train loss: 501.4847, Valid loss: 283.1498\n",
      "Epoch 17, Train loss: 492.2689, Valid loss: 312.5822\n",
      "Epoch 18, Train loss: 487.7539, Valid loss: 210.143\n",
      "Epoch 19, Train loss: 485.4253, Valid loss: 258.382\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 487.7539, minimum valid loss = 210.143\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1658.5322, Valid loss: 1017.2345\n",
      "Epoch 1, Train loss: 1026.4237, Valid loss: 454.7421\n",
      "Epoch 2, Train loss: 838.5094, Valid loss: 320.4829\n",
      "Epoch 3, Train loss: 768.7304, Valid loss: 513.8514\n",
      "Epoch 4, Train loss: 667.6888, Valid loss: 261.5454\n",
      "Epoch 5, Train loss: 651.7556, Valid loss: 374.1905\n",
      "Epoch 6, Train loss: 636.3255, Valid loss: 294.2484\n",
      "Epoch 7, Train loss: 643.5666, Valid loss: 280.9229\n",
      "Epoch 8, Train loss: 660.2212, Valid loss: 512.7784\n",
      "Epoch 9, Train loss: 631.1019, Valid loss: 259.9576\n",
      "Epoch 10, Train loss: 641.5314, Valid loss: 340.1014\n",
      "Epoch 11, Train loss: 619.1132, Valid loss: 264.9555\n",
      "Epoch 12, Train loss: 608.2095, Valid loss: 302.0635\n",
      "Epoch 13, Train loss: 650.4071, Valid loss: 233.9784\n",
      "Epoch 14, Train loss: 632.4355, Valid loss: 298.5569\n",
      "Epoch 15, Train loss: 625.309, Valid loss: 659.2635\n",
      "Epoch 16, Train loss: 748.3609, Valid loss: 455.0019\n",
      "Epoch 17, Train loss: 818.0542, Valid loss: 563.5159\n",
      "Epoch 18, Train loss: 719.8879, Valid loss: 339.4905\n",
      "Epoch 19, Train loss: 616.0859, Valid loss: 272.4115\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 13, training loss = 650.4071, minimum valid loss = 233.9784\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1398.7751, Valid loss: 647.5127\n",
      "Epoch 1, Train loss: 958.546, Valid loss: 587.8085\n",
      "Epoch 2, Train loss: 795.7658, Valid loss: 586.0164\n",
      "Epoch 3, Train loss: 753.0086, Valid loss: 354.9758\n",
      "Epoch 4, Train loss: 663.228, Valid loss: 272.385\n",
      "Epoch 5, Train loss: 677.9606, Valid loss: 336.5034\n",
      "Epoch 6, Train loss: 639.3197, Valid loss: 286.5645\n",
      "Epoch 7, Train loss: 636.5641, Valid loss: 266.7783\n",
      "Epoch 8, Train loss: 651.7092, Valid loss: 394.3976\n",
      "Epoch 9, Train loss: 667.3645, Valid loss: 325.3306\n",
      "Epoch 10, Train loss: 645.838, Valid loss: 271.7218\n",
      "Epoch 11, Train loss: 683.2519, Valid loss: 424.5479\n",
      "Epoch 12, Train loss: 723.6332, Valid loss: 609.1316\n",
      "Epoch 13, Train loss: 726.6062, Valid loss: 394.2279\n",
      "Epoch 14, Train loss: 717.904, Valid loss: 314.0599\n",
      "Epoch 15, Train loss: 672.8402, Valid loss: 393.3934\n",
      "Epoch 16, Train loss: 646.42, Valid loss: 283.0882\n",
      "Epoch 17, Train loss: 687.9699, Valid loss: 446.4466\n",
      "Epoch 18, Train loss: 678.6679, Valid loss: 427.6923\n",
      "Epoch 19, Train loss: 710.9593, Valid loss: 484.3665\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 7, training loss = 636.5641, minimum valid loss = 266.7783\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1569.5417, Valid loss: 539.6046\n",
      "Epoch 1, Train loss: 1087.4099, Valid loss: 646.3574\n",
      "Epoch 2, Train loss: 965.0213, Valid loss: 452.8201\n",
      "Epoch 3, Train loss: 941.6647, Valid loss: 511.5727\n",
      "Epoch 4, Train loss: 895.8629, Valid loss: 278.4377\n",
      "Epoch 5, Train loss: 842.8875, Valid loss: 566.1977\n",
      "Epoch 6, Train loss: 781.3929, Valid loss: 511.2492\n",
      "Epoch 7, Train loss: 713.1134, Valid loss: 331.4603\n",
      "Epoch 8, Train loss: 695.0904, Valid loss: 268.9381\n",
      "Epoch 9, Train loss: 670.1168, Valid loss: 297.6071\n",
      "Epoch 10, Train loss: 656.1691, Valid loss: 300.5077\n",
      "Epoch 11, Train loss: 685.2504, Valid loss: 233.2721\n",
      "Epoch 12, Train loss: 674.6643, Valid loss: 324.7755\n",
      "Epoch 13, Train loss: 655.9683, Valid loss: 269.8553\n",
      "Epoch 14, Train loss: 616.2325, Valid loss: 279.5572\n",
      "Epoch 15, Train loss: 609.4203, Valid loss: 325.7989\n",
      "Epoch 16, Train loss: 624.0462, Valid loss: 335.5991\n",
      "Epoch 17, Train loss: 592.9787, Valid loss: 238.5041\n",
      "Epoch 18, Train loss: 596.8485, Valid loss: 263.3161\n",
      "Epoch 19, Train loss: 561.4273, Valid loss: 307.0186\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 11, training loss = 685.2504, minimum valid loss = 233.2721\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1388.6842, Valid loss: 397.0348\n",
      "Epoch 1, Train loss: 919.1422, Valid loss: 282.5268\n",
      "Epoch 2, Train loss: 872.9744, Valid loss: 513.6473\n",
      "Epoch 3, Train loss: 781.0579, Valid loss: 303.659\n",
      "Epoch 4, Train loss: 719.0161, Valid loss: 391.6916\n",
      "Epoch 5, Train loss: 734.2507, Valid loss: 303.3211\n",
      "Epoch 6, Train loss: 716.1504, Valid loss: 355.6631\n",
      "Epoch 7, Train loss: 696.9239, Valid loss: 238.8594\n",
      "Epoch 8, Train loss: 686.1168, Valid loss: 364.2981\n",
      "Epoch 9, Train loss: 677.1584, Valid loss: 285.665\n",
      "Epoch 10, Train loss: 616.6226, Valid loss: 222.9111\n",
      "Epoch 11, Train loss: 614.4276, Valid loss: 403.5827\n",
      "Epoch 12, Train loss: 605.6901, Valid loss: 268.583\n",
      "Epoch 13, Train loss: 606.7828, Valid loss: 239.1357\n",
      "Epoch 14, Train loss: 613.1359, Valid loss: 247.5951\n",
      "Epoch 15, Train loss: 583.474, Valid loss: 260.4425\n",
      "Epoch 16, Train loss: 572.2903, Valid loss: 391.6472\n",
      "Epoch 17, Train loss: 573.1061, Valid loss: 339.6521\n",
      "Epoch 18, Train loss: 563.8049, Valid loss: 317.2938\n",
      "Epoch 19, Train loss: 548.9206, Valid loss: 206.3078\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 548.9206, minimum valid loss = 206.3078\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1232.0213, Valid loss: 423.3715\n",
      "Epoch 1, Train loss: 830.2002, Valid loss: 375.6464\n",
      "Epoch 2, Train loss: 718.0402, Valid loss: 361.3115\n",
      "Epoch 3, Train loss: 678.4099, Valid loss: 304.3856\n",
      "Epoch 4, Train loss: 666.0516, Valid loss: 264.3157\n",
      "Epoch 5, Train loss: 639.7415, Valid loss: 219.1341\n",
      "Epoch 6, Train loss: 597.2806, Valid loss: 329.3223\n",
      "Epoch 7, Train loss: 613.276, Valid loss: 310.6804\n",
      "Epoch 8, Train loss: 629.3268, Valid loss: 207.5963\n",
      "Epoch 9, Train loss: 609.6313, Valid loss: 211.8615\n",
      "Epoch 10, Train loss: 563.4451, Valid loss: 219.6612\n",
      "Epoch 11, Train loss: 555.3366, Valid loss: 206.334\n",
      "Epoch 12, Train loss: 535.3155, Valid loss: 246.2265\n",
      "Epoch 13, Train loss: 565.6432, Valid loss: 300.7875\n",
      "Epoch 14, Train loss: 546.2785, Valid loss: 268.2844\n",
      "Epoch 15, Train loss: 555.9051, Valid loss: 367.3636\n",
      "Epoch 16, Train loss: 545.116, Valid loss: 208.1306\n",
      "Epoch 17, Train loss: 551.8529, Valid loss: 318.9114\n",
      "Epoch 18, Train loss: 526.0399, Valid loss: 222.5909\n",
      "Epoch 19, Train loss: 514.5373, Valid loss: 227.6921\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 11, training loss = 555.3366, minimum valid loss = 206.334\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1626.095, Valid loss: 552.1139\n",
      "Epoch 1, Train loss: 991.6463, Valid loss: 625.654\n",
      "Epoch 2, Train loss: 830.8115, Valid loss: 456.2646\n",
      "Epoch 3, Train loss: 773.0824, Valid loss: 412.3887\n",
      "Epoch 4, Train loss: 752.2328, Valid loss: 429.3009\n",
      "Epoch 5, Train loss: 758.9697, Valid loss: 289.2919\n",
      "Epoch 6, Train loss: 728.2136, Valid loss: 303.3207\n",
      "Epoch 7, Train loss: 715.3453, Valid loss: 444.6758\n",
      "Epoch 8, Train loss: 706.2435, Valid loss: 336.6615\n",
      "Epoch 9, Train loss: 711.1822, Valid loss: 291.8374\n",
      "Epoch 10, Train loss: 688.7853, Valid loss: 379.6845\n",
      "Epoch 11, Train loss: 705.5467, Valid loss: 448.6034\n",
      "Epoch 12, Train loss: 663.5244, Valid loss: 321.3045\n",
      "Epoch 13, Train loss: 688.2893, Valid loss: 368.9254\n",
      "Epoch 14, Train loss: 663.5341, Valid loss: 351.5268\n",
      "Epoch 15, Train loss: 683.1979, Valid loss: 307.611\n",
      "Epoch 16, Train loss: 615.414, Valid loss: 244.339\n",
      "Epoch 17, Train loss: 586.6382, Valid loss: 315.9636\n",
      "Epoch 18, Train loss: 602.3395, Valid loss: 348.2484\n",
      "Epoch 19, Train loss: 590.0871, Valid loss: 242.2383\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 590.0871, minimum valid loss = 242.2383\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1272.3891, Valid loss: 751.9499\n",
      "Epoch 1, Train loss: 854.7527, Valid loss: 329.7089\n",
      "Epoch 2, Train loss: 681.7542, Valid loss: 261.6826\n",
      "Epoch 3, Train loss: 725.0163, Valid loss: 620.8361\n",
      "Epoch 4, Train loss: 688.1629, Valid loss: 268.3881\n",
      "Epoch 5, Train loss: 669.5402, Valid loss: 256.753\n",
      "Epoch 6, Train loss: 677.2088, Valid loss: 293.6871\n",
      "Epoch 7, Train loss: 644.8177, Valid loss: 307.6555\n",
      "Epoch 8, Train loss: 726.1152, Valid loss: 277.5372\n",
      "Epoch 9, Train loss: 688.2383, Valid loss: 220.6569\n",
      "Epoch 10, Train loss: 657.9128, Valid loss: 286.1662\n",
      "Epoch 11, Train loss: 680.5656, Valid loss: 324.1054\n",
      "Epoch 12, Train loss: 644.4044, Valid loss: 285.0646\n",
      "Epoch 13, Train loss: 643.8199, Valid loss: 287.0647\n",
      "Epoch 14, Train loss: 707.8983, Valid loss: 513.2904\n",
      "Epoch 15, Train loss: 628.59, Valid loss: 278.2617\n",
      "Epoch 16, Train loss: 605.5813, Valid loss: 339.0615\n",
      "Epoch 17, Train loss: 652.0457, Valid loss: 354.9128\n",
      "Epoch 18, Train loss: 659.4026, Valid loss: 247.9976\n",
      "Epoch 19, Train loss: 660.6733, Valid loss: 251.3382\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 9, training loss = 688.2383, minimum valid loss = 220.6569\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2224.3298, Valid loss: 2160.3499\n",
      "Epoch 1, Train loss: 2199.2665, Valid loss: 2150.4105\n",
      "Epoch 2, Train loss: 2189.1448, Valid loss: 2140.5255\n",
      "Epoch 3, Train loss: 2179.2408, Valid loss: 2130.6749\n",
      "Epoch 4, Train loss: 2169.3827, Valid loss: 2120.8709\n",
      "Epoch 5, Train loss: 2159.5267, Valid loss: 2111.1554\n",
      "Epoch 6, Train loss: 2149.7904, Valid loss: 2101.4454\n",
      "Epoch 7, Train loss: 2140.0619, Valid loss: 2091.7939\n",
      "Epoch 8, Train loss: 2130.4065, Valid loss: 2082.1805\n",
      "Epoch 9, Train loss: 2120.764, Valid loss: 2072.6397\n",
      "Epoch 10, Train loss: 2111.195, Valid loss: 2063.1424\n",
      "Epoch 11, Train loss: 2101.6957, Valid loss: 2053.6669\n",
      "Epoch 12, Train loss: 2092.1893, Valid loss: 2044.2758\n",
      "Epoch 13, Train loss: 2082.7707, Valid loss: 2034.9288\n",
      "Epoch 14, Train loss: 2073.401, Valid loss: 2025.6133\n",
      "Epoch 15, Train loss: 2064.076, Valid loss: 2016.3447\n",
      "Epoch 16, Train loss: 2054.7855, Valid loss: 2007.1406\n",
      "Epoch 17, Train loss: 2045.5735, Valid loss: 1997.9645\n",
      "Epoch 18, Train loss: 2036.3397, Valid loss: 1988.8897\n",
      "Epoch 19, Train loss: 2027.2568, Valid loss: 1979.7924\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2027.2568, minimum valid loss = 1979.7924\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2193.8509, Valid loss: 2044.334\n",
      "Epoch 1, Train loss: 1922.015, Valid loss: 1166.2248\n",
      "Epoch 2, Train loss: 1579.0508, Valid loss: 1106.5042\n",
      "Epoch 3, Train loss: 1331.0098, Valid loss: 622.7836\n",
      "Epoch 4, Train loss: 1070.7252, Valid loss: 566.971\n",
      "Epoch 5, Train loss: 1056.1633, Valid loss: 451.8536\n",
      "Epoch 6, Train loss: 993.7256, Valid loss: 501.1721\n",
      "Epoch 7, Train loss: 980.1827, Valid loss: 419.499\n",
      "Epoch 8, Train loss: 976.6744, Valid loss: 437.9062\n",
      "Epoch 9, Train loss: 980.4523, Valid loss: 603.7042\n",
      "Epoch 10, Train loss: 986.9424, Valid loss: 593.7986\n",
      "Epoch 11, Train loss: 1007.6277, Valid loss: 447.1346\n",
      "Epoch 12, Train loss: 984.6174, Valid loss: 347.1924\n",
      "Epoch 13, Train loss: 940.0902, Valid loss: 479.919\n",
      "Epoch 14, Train loss: 925.532, Valid loss: 546.6765\n",
      "Epoch 15, Train loss: 967.7684, Valid loss: 484.7015\n",
      "Epoch 16, Train loss: 907.5918, Valid loss: 409.3481\n",
      "Epoch 17, Train loss: 903.3026, Valid loss: 472.3084\n",
      "Epoch 18, Train loss: 923.745, Valid loss: 424.2902\n",
      "Epoch 19, Train loss: 875.2796, Valid loss: 345.65\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 875.2796, minimum valid loss = 345.65\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1790.6457, Valid loss: 823.9147\n",
      "Epoch 1, Train loss: 1303.2517, Valid loss: 640.6821\n",
      "Epoch 2, Train loss: 1034.8568, Valid loss: 621.1526\n",
      "Epoch 3, Train loss: 1027.0495, Valid loss: 520.4609\n",
      "Epoch 4, Train loss: 1021.5698, Valid loss: 531.3768\n",
      "Epoch 5, Train loss: 942.9178, Valid loss: 557.6551\n",
      "Epoch 6, Train loss: 949.1741, Valid loss: 538.0639\n",
      "Epoch 7, Train loss: 936.4008, Valid loss: 600.6754\n",
      "Epoch 8, Train loss: 913.9265, Valid loss: 475.7323\n",
      "Epoch 9, Train loss: 929.0656, Valid loss: 425.5946\n",
      "Epoch 10, Train loss: 928.1984, Valid loss: 370.0034\n",
      "Epoch 11, Train loss: 938.1934, Valid loss: 472.6803\n",
      "Epoch 12, Train loss: 921.4442, Valid loss: 474.3843\n",
      "Epoch 13, Train loss: 878.6009, Valid loss: 408.4819\n",
      "Epoch 14, Train loss: 891.6655, Valid loss: 405.2473\n",
      "Epoch 15, Train loss: 866.5924, Valid loss: 350.7408\n",
      "Epoch 16, Train loss: 904.3882, Valid loss: 439.2101\n",
      "Epoch 17, Train loss: 858.5351, Valid loss: 432.5386\n",
      "Epoch 18, Train loss: 881.3888, Valid loss: 373.7884\n",
      "Epoch 19, Train loss: 872.6555, Valid loss: 508.8187\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 866.5924, minimum valid loss = 350.7408\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2210.6252, Valid loss: 2078.996\n",
      "Epoch 1, Train loss: 1982.075, Valid loss: 1437.1557\n",
      "Epoch 2, Train loss: 1607.9548, Valid loss: 1095.8574\n",
      "Epoch 3, Train loss: 1348.882, Valid loss: 761.3974\n",
      "Epoch 4, Train loss: 1142.9645, Valid loss: 537.9909\n",
      "Epoch 5, Train loss: 1033.0034, Valid loss: 582.3267\n",
      "Epoch 6, Train loss: 982.7047, Valid loss: 547.1431\n",
      "Epoch 7, Train loss: 954.6596, Valid loss: 465.8747\n",
      "Epoch 8, Train loss: 919.5096, Valid loss: 444.1975\n",
      "Epoch 9, Train loss: 922.5432, Valid loss: 434.0166\n",
      "Epoch 10, Train loss: 916.1453, Valid loss: 455.9204\n",
      "Epoch 11, Train loss: 891.2604, Valid loss: 437.0685\n",
      "Epoch 12, Train loss: 908.9806, Valid loss: 428.6817\n",
      "Epoch 13, Train loss: 903.343, Valid loss: 467.9571\n",
      "Epoch 14, Train loss: 892.2399, Valid loss: 392.7313\n",
      "Epoch 15, Train loss: 925.7563, Valid loss: 483.8719\n",
      "Epoch 16, Train loss: 919.7854, Valid loss: 482.586\n",
      "Epoch 17, Train loss: 875.1233, Valid loss: 375.0307\n",
      "Epoch 18, Train loss: 892.3238, Valid loss: 376.1422\n",
      "Epoch 19, Train loss: 884.1537, Valid loss: 464.7786\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 875.1233, minimum valid loss = 375.0307\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1979.8193, Valid loss: 1305.4613\n",
      "Epoch 1, Train loss: 1481.7877, Valid loss: 940.6121\n",
      "Epoch 2, Train loss: 1178.3008, Valid loss: 626.3471\n",
      "Epoch 3, Train loss: 1039.4487, Valid loss: 552.5362\n",
      "Epoch 4, Train loss: 992.3589, Valid loss: 503.7691\n",
      "Epoch 5, Train loss: 973.005, Valid loss: 585.6312\n",
      "Epoch 6, Train loss: 941.5436, Valid loss: 458.8623\n",
      "Epoch 7, Train loss: 949.3798, Valid loss: 429.2677\n",
      "Epoch 8, Train loss: 943.8922, Valid loss: 483.7776\n",
      "Epoch 9, Train loss: 914.0476, Valid loss: 455.2124\n",
      "Epoch 10, Train loss: 899.8407, Valid loss: 417.3129\n",
      "Epoch 11, Train loss: 935.9806, Valid loss: 503.9897\n",
      "Epoch 12, Train loss: 892.5418, Valid loss: 497.1406\n",
      "Epoch 13, Train loss: 868.8305, Valid loss: 554.6607\n",
      "Epoch 14, Train loss: 885.4638, Valid loss: 439.8845\n",
      "Epoch 15, Train loss: 920.9719, Valid loss: 407.4895\n",
      "Epoch 16, Train loss: 899.8145, Valid loss: 458.2858\n",
      "Epoch 17, Train loss: 852.0608, Valid loss: 469.5415\n",
      "Epoch 18, Train loss: 882.3871, Valid loss: 459.5751\n",
      "Epoch 19, Train loss: 889.3366, Valid loss: 504.6316\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 920.9719, minimum valid loss = 407.4895\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2014.2949, Valid loss: 1628.4891\n",
      "Epoch 1, Train loss: 1511.8604, Valid loss: 818.0349\n",
      "Epoch 2, Train loss: 1261.1575, Valid loss: 736.2803\n",
      "Epoch 3, Train loss: 1074.7778, Valid loss: 540.7464\n",
      "Epoch 4, Train loss: 915.4033, Valid loss: 441.3115\n",
      "Epoch 5, Train loss: 859.3445, Valid loss: 449.8922\n",
      "Epoch 6, Train loss: 853.3279, Valid loss: 369.1657\n",
      "Epoch 7, Train loss: 803.7214, Valid loss: 383.9745\n",
      "Epoch 8, Train loss: 804.2421, Valid loss: 365.6583\n",
      "Epoch 9, Train loss: 769.744, Valid loss: 329.7708\n",
      "Epoch 10, Train loss: 781.229, Valid loss: 349.502\n",
      "Epoch 11, Train loss: 701.082, Valid loss: 324.3035\n",
      "Epoch 12, Train loss: 721.7937, Valid loss: 355.348\n",
      "Epoch 13, Train loss: 701.9877, Valid loss: 370.5646\n",
      "Epoch 14, Train loss: 709.8329, Valid loss: 312.2377\n",
      "Epoch 15, Train loss: 721.8822, Valid loss: 293.7869\n",
      "Epoch 16, Train loss: 707.4331, Valid loss: 310.5052\n",
      "Epoch 17, Train loss: 739.4159, Valid loss: 364.2536\n",
      "Epoch 18, Train loss: 674.2297, Valid loss: 280.6608\n",
      "Epoch 19, Train loss: 697.9728, Valid loss: 324.9167\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 674.2297, minimum valid loss = 280.6608\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1857.7765, Valid loss: 1285.1432\n",
      "Epoch 1, Train loss: 1440.1161, Valid loss: 818.4508\n",
      "Epoch 2, Train loss: 1154.7387, Valid loss: 620.2621\n",
      "Epoch 3, Train loss: 1036.0768, Valid loss: 444.4283\n",
      "Epoch 4, Train loss: 874.8834, Valid loss: 388.3475\n",
      "Epoch 5, Train loss: 783.0222, Valid loss: 414.0823\n",
      "Epoch 6, Train loss: 771.6354, Valid loss: 325.447\n",
      "Epoch 7, Train loss: 737.5727, Valid loss: 420.0895\n",
      "Epoch 8, Train loss: 750.1146, Valid loss: 329.961\n",
      "Epoch 9, Train loss: 748.4604, Valid loss: 290.1147\n",
      "Epoch 10, Train loss: 723.0063, Valid loss: 310.1336\n",
      "Epoch 11, Train loss: 728.3169, Valid loss: 356.7669\n",
      "Epoch 12, Train loss: 698.4543, Valid loss: 275.1907\n",
      "Epoch 13, Train loss: 700.7555, Valid loss: 310.8607\n",
      "Epoch 14, Train loss: 699.0454, Valid loss: 272.5003\n",
      "Epoch 15, Train loss: 676.0589, Valid loss: 316.6228\n",
      "Epoch 16, Train loss: 690.1054, Valid loss: 300.7665\n",
      "Epoch 17, Train loss: 661.9441, Valid loss: 252.9847\n",
      "Epoch 18, Train loss: 652.4616, Valid loss: 245.7133\n",
      "Epoch 19, Train loss: 682.115, Valid loss: 245.903\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 652.4616, minimum valid loss = 245.7133\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2216.1363, Valid loss: 2111.4082\n",
      "Epoch 1, Train loss: 2054.8578, Valid loss: 1801.3632\n",
      "Epoch 2, Train loss: 1634.3782, Valid loss: 945.8217\n",
      "Epoch 3, Train loss: 1354.4983, Valid loss: 755.8257\n",
      "Epoch 4, Train loss: 1216.3852, Valid loss: 691.8206\n",
      "Epoch 5, Train loss: 1109.723, Valid loss: 619.3753\n",
      "Epoch 6, Train loss: 1099.6654, Valid loss: 589.0688\n",
      "Epoch 7, Train loss: 1092.9727, Valid loss: 594.7078\n",
      "Epoch 8, Train loss: 1038.3411, Valid loss: 582.2844\n",
      "Epoch 9, Train loss: 1037.905, Valid loss: 502.1792\n",
      "Epoch 10, Train loss: 1045.6731, Valid loss: 560.5342\n",
      "Epoch 11, Train loss: 989.8256, Valid loss: 507.9432\n",
      "Epoch 12, Train loss: 1035.9494, Valid loss: 576.8479\n",
      "Epoch 13, Train loss: 977.542, Valid loss: 487.4696\n",
      "Epoch 14, Train loss: 974.3441, Valid loss: 511.6787\n",
      "Epoch 15, Train loss: 946.0819, Valid loss: 439.794\n",
      "Epoch 16, Train loss: 970.6379, Valid loss: 479.926\n",
      "Epoch 17, Train loss: 958.1676, Valid loss: 496.6389\n",
      "Epoch 18, Train loss: 946.7991, Valid loss: 463.9011\n",
      "Epoch 19, Train loss: 973.4459, Valid loss: 484.9984\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 946.0819, minimum valid loss = 439.794\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2037.9591, Valid loss: 1696.7193\n",
      "Epoch 1, Train loss: 1558.2355, Valid loss: 895.1684\n",
      "Epoch 2, Train loss: 1352.6756, Valid loss: 888.3651\n",
      "Epoch 3, Train loss: 1231.2487, Valid loss: 691.1921\n",
      "Epoch 4, Train loss: 1176.0135, Valid loss: 630.8285\n",
      "Epoch 5, Train loss: 1065.7875, Valid loss: 619.7782\n",
      "Epoch 6, Train loss: 1044.193, Valid loss: 566.2797\n",
      "Epoch 7, Train loss: 1024.0444, Valid loss: 485.8589\n",
      "Epoch 8, Train loss: 1027.3116, Valid loss: 600.1724\n",
      "Epoch 9, Train loss: 992.808, Valid loss: 509.088\n",
      "Epoch 10, Train loss: 985.3061, Valid loss: 553.4774\n",
      "Epoch 11, Train loss: 974.246, Valid loss: 538.3745\n",
      "Epoch 12, Train loss: 962.1324, Valid loss: 489.8121\n",
      "Epoch 13, Train loss: 949.9642, Valid loss: 470.6287\n",
      "Epoch 14, Train loss: 964.6732, Valid loss: 459.2163\n",
      "Epoch 15, Train loss: 937.7686, Valid loss: 432.4789\n",
      "Epoch 16, Train loss: 957.1204, Valid loss: 405.459\n",
      "Epoch 17, Train loss: 958.4762, Valid loss: 469.8943\n",
      "Epoch 18, Train loss: 911.5231, Valid loss: 437.6923\n",
      "Epoch 19, Train loss: 831.5914, Valid loss: 340.8857\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 831.5914, minimum valid loss = 340.8857\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2229.1619, Valid loss: 2165.9119\n",
      "Epoch 1, Train loss: 2206.9065, Valid loss: 2160.3728\n",
      "Epoch 2, Train loss: 2201.449, Valid loss: 2155.4423\n",
      "Epoch 3, Train loss: 2196.5067, Valid loss: 2150.5205\n",
      "Epoch 4, Train loss: 2191.5549, Valid loss: 2145.6372\n",
      "Epoch 5, Train loss: 2186.6572, Valid loss: 2140.7937\n",
      "Epoch 6, Train loss: 2181.8168, Valid loss: 2135.9649\n",
      "Epoch 7, Train loss: 2176.9764, Valid loss: 2131.1902\n",
      "Epoch 8, Train loss: 2172.1982, Valid loss: 2126.4356\n",
      "Epoch 9, Train loss: 2167.4339, Valid loss: 2121.7228\n",
      "Epoch 10, Train loss: 2162.711, Valid loss: 2117.0421\n",
      "Epoch 11, Train loss: 2158.0164, Valid loss: 2112.3922\n",
      "Epoch 12, Train loss: 2153.3701, Valid loss: 2107.7569\n",
      "Epoch 13, Train loss: 2148.7334, Valid loss: 2103.1713\n",
      "Epoch 14, Train loss: 2144.1452, Valid loss: 2098.6118\n",
      "Epoch 15, Train loss: 2139.5792, Valid loss: 2094.0934\n",
      "Epoch 16, Train loss: 2135.0448, Valid loss: 2089.6148\n",
      "Epoch 17, Train loss: 2130.5689, Valid loss: 2085.1376\n",
      "Epoch 18, Train loss: 2126.0868, Valid loss: 2080.7207\n",
      "Epoch 19, Train loss: 2121.6608, Valid loss: 2076.33\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2121.6608, minimum valid loss = 2076.33\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 314116.0975, Valid loss: 2262.9367\n",
      "Epoch 1, Train loss: 2306.471, Valid loss: 2234.8531\n",
      "Epoch 2, Train loss: 2261.6318, Valid loss: 2163.2878\n",
      "Epoch 3, Train loss: 2160.5627, Valid loss: 2064.4112\n",
      "Epoch 4, Train loss: 2060.5215, Valid loss: 1961.3973\n",
      "Epoch 5, Train loss: 1955.7742, Valid loss: 1855.2608\n",
      "Epoch 6, Train loss: 1849.1004, Valid loss: 1750.1906\n",
      "Epoch 7, Train loss: 1744.0756, Valid loss: 1647.9886\n",
      "Epoch 8, Train loss: 1643.1493, Valid loss: 1549.3161\n",
      "Epoch 9, Train loss: 1546.1493, Valid loss: 1456.2269\n",
      "Epoch 10, Train loss: 1455.1049, Valid loss: 1368.9737\n",
      "Epoch 11, Train loss: 1370.1629, Valid loss: 1288.2282\n",
      "Epoch 12, Train loss: 1292.0543, Valid loss: 1214.0031\n",
      "Epoch 13, Train loss: 1220.7329, Valid loss: 1146.284\n",
      "Epoch 14, Train loss: 1155.1282, Valid loss: 1086.0771\n",
      "Epoch 15, Train loss: 1097.505, Valid loss: 1031.2572\n",
      "Epoch 16, Train loss: 1044.7215, Valid loss: 983.3429\n",
      "Epoch 17, Train loss: 998.9312, Valid loss: 940.4906\n",
      "Epoch 18, Train loss: 958.2894, Valid loss: 902.7319\n",
      "Epoch 19, Train loss: 922.2436, Valid loss: 870.2075\n",
      "Epoch 20, Train loss: 891.4835, Valid loss: 841.6354\n",
      "Epoch 21, Train loss: 864.4363, Valid loss: 817.1998\n",
      "Epoch 22, Train loss: 841.3321, Valid loss: 796.2282\n",
      "Epoch 23, Train loss: 821.422, Valid loss: 778.4906\n",
      "Epoch 24, Train loss: 804.6828, Valid loss: 763.2351\n",
      "Epoch 25, Train loss: 790.322, Valid loss: 750.4023\n",
      "Epoch 26, Train loss: 778.0429, Valid loss: 739.8628\n",
      "Epoch 27, Train loss: 768.0552, Valid loss: 730.8202\n",
      "Epoch 28, Train loss: 759.5431, Valid loss: 723.2671\n",
      "Epoch 29, Train loss: 752.3791, Valid loss: 717.0626\n",
      "Epoch 30, Train loss: 746.465, Valid loss: 711.9556\n",
      "Epoch 31, Train loss: 741.6837, Valid loss: 707.589\n",
      "Epoch 32, Train loss: 737.5641, Valid loss: 704.1097\n",
      "Epoch 33, Train loss: 734.193, Valid loss: 701.3615\n",
      "Epoch 34, Train loss: 731.5346, Valid loss: 699.0279\n",
      "Epoch 35, Train loss: 729.3001, Valid loss: 697.1276\n",
      "Epoch 36, Train loss: 727.4769, Valid loss: 695.5755\n",
      "Epoch 37, Train loss: 725.9719, Valid loss: 694.3303\n",
      "Epoch 38, Train loss: 724.7751, Valid loss: 693.2686\n",
      "Epoch 39, Train loss: 723.7381, Valid loss: 692.4924\n",
      "Epoch 40, Train loss: 722.9712, Valid loss: 691.7777\n",
      "Epoch 41, Train loss: 722.2614, Valid loss: 691.2477\n",
      "Epoch 42, Train loss: 721.722, Valid loss: 690.8421\n",
      "Epoch 43, Train loss: 721.2936, Valid loss: 690.4916\n",
      "Epoch 44, Train loss: 720.9301, Valid loss: 690.183\n",
      "Epoch 45, Train loss: 720.6117, Valid loss: 689.8942\n",
      "Epoch 46, Train loss: 720.3345, Valid loss: 689.7126\n",
      "Epoch 47, Train loss: 720.1399, Valid loss: 689.5141\n",
      "Epoch 48, Train loss: 719.9469, Valid loss: 689.4035\n",
      "Epoch 49, Train loss: 719.8267, Valid loss: 689.25\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 719.8267, minimum valid loss = 689.25\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 483263.7762, Valid loss: 2269.7102\n",
      "Epoch 1, Train loss: 2317.6039, Valid loss: 2261.8852\n",
      "Epoch 2, Train loss: 2284.5255, Valid loss: 2205.6941\n",
      "Epoch 3, Train loss: 3029.9514, Valid loss: 2134.0367\n",
      "Epoch 4, Train loss: 2140.4945, Valid loss: 2052.9531\n",
      "Epoch 5, Train loss: 2056.1647, Valid loss: 1966.0358\n",
      "Epoch 6, Train loss: 1967.4867, Valid loss: 1877.6138\n",
      "Epoch 7, Train loss: 1880.2317, Valid loss: 1788.5347\n",
      "Epoch 8, Train loss: 1789.5766, Valid loss: 1701.642\n",
      "Epoch 9, Train loss: 1703.7114, Valid loss: 1616.4962\n",
      "Epoch 10, Train loss: 1619.5722, Valid loss: 1534.9823\n",
      "Epoch 11, Train loss: 1539.0137, Valid loss: 1457.4854\n",
      "Epoch 12, Train loss: 1462.9602, Valid loss: 1384.034\n",
      "Epoch 13, Train loss: 1391.3641, Valid loss: 1314.9425\n",
      "Epoch 14, Train loss: 1323.6498, Valid loss: 1251.2219\n",
      "Epoch 15, Train loss: 1261.791, Valid loss: 1191.4843\n",
      "Epoch 16, Train loss: 1203.8127, Valid loss: 1136.8966\n",
      "Epoch 17, Train loss: 1150.9649, Valid loss: 1086.8237\n",
      "Epoch 18, Train loss: 1102.6621, Valid loss: 1041.0503\n",
      "Epoch 19, Train loss: 1058.3527, Valid loss: 999.9648\n",
      "Epoch 20, Train loss: 1018.5424, Valid loss: 962.9024\n",
      "Epoch 21, Train loss: 982.9133, Valid loss: 929.2887\n",
      "Epoch 22, Train loss: 950.5648, Valid loss: 899.4764\n",
      "Epoch 23, Train loss: 921.9925, Valid loss: 872.7224\n",
      "Epoch 24, Train loss: 896.4039, Valid loss: 849.0396\n",
      "Epoch 25, Train loss: 873.6411, Valid loss: 828.3024\n",
      "Epoch 26, Train loss: 853.7965, Valid loss: 809.8887\n",
      "Epoch 27, Train loss: 836.1169, Valid loss: 793.8468\n",
      "Epoch 28, Train loss: 820.7325, Valid loss: 779.7453\n",
      "Epoch 29, Train loss: 807.2037, Valid loss: 767.4917\n",
      "Epoch 30, Train loss: 795.522, Valid loss: 756.6996\n",
      "Epoch 31, Train loss: 785.2313, Valid loss: 747.4136\n",
      "Epoch 32, Train loss: 776.3443, Valid loss: 739.3654\n",
      "Epoch 33, Train loss: 768.5834, Valid loss: 732.5321\n",
      "Epoch 34, Train loss: 762.0499, Valid loss: 726.4975\n",
      "Epoch 35, Train loss: 756.2611, Valid loss: 721.4298\n",
      "Epoch 36, Train loss: 751.3201, Valid loss: 717.1823\n",
      "Epoch 37, Train loss: 747.2078, Valid loss: 713.3453\n",
      "Epoch 38, Train loss: 743.5751, Valid loss: 709.9938\n",
      "Epoch 39, Train loss: 740.4076, Valid loss: 707.1666\n",
      "Epoch 40, Train loss: 737.6604, Valid loss: 704.9257\n",
      "Epoch 41, Train loss: 735.4546, Valid loss: 702.9159\n",
      "Epoch 42, Train loss: 733.488, Valid loss: 701.2257\n",
      "Epoch 43, Train loss: 731.8497, Valid loss: 699.6769\n",
      "Epoch 44, Train loss: 730.3537, Valid loss: 698.4842\n",
      "Epoch 45, Train loss: 729.1539, Valid loss: 697.4326\n",
      "Epoch 46, Train loss: 728.1118, Valid loss: 696.5114\n",
      "Epoch 47, Train loss: 727.2086, Valid loss: 695.6847\n",
      "Epoch 48, Train loss: 726.3997, Valid loss: 694.9948\n",
      "Epoch 49, Train loss: 725.7271, Valid loss: 694.4218\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 725.7271, minimum valid loss = 694.4218\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 167950.2705, Valid loss: 2251.2962\n",
      "Epoch 1, Train loss: 2280.1464, Valid loss: 2196.8456\n",
      "Epoch 2, Train loss: 2194.1032, Valid loss: 2085.769\n",
      "Epoch 3, Train loss: 2072.9894, Valid loss: 1956.6026\n",
      "Epoch 4, Train loss: 1938.2399, Valid loss: 1823.8325\n",
      "Epoch 5, Train loss: 1806.778, Valid loss: 1697.1161\n",
      "Epoch 6, Train loss: 1683.7784, Valid loss: 1578.3992\n",
      "Epoch 7, Train loss: 1568.8814, Valid loss: 1471.255\n",
      "Epoch 8, Train loss: 1468.7454, Valid loss: 1375.483\n",
      "Epoch 9, Train loss: 1375.2529, Valid loss: 1291.987\n",
      "Epoch 10, Train loss: 1296.0782, Valid loss: 1219.4111\n",
      "Epoch 11, Train loss: 1227.6335, Valid loss: 1156.9427\n",
      "Epoch 12, Train loss: 1169.151, Valid loss: 1103.3224\n",
      "Epoch 13, Train loss: 1119.0879, Valid loss: 1058.5324\n",
      "Epoch 14, Train loss: 1077.2866, Valid loss: 1020.5974\n",
      "Epoch 15, Train loss: 1042.0676, Valid loss: 989.6015\n",
      "Epoch 16, Train loss: 1013.0716, Valid loss: 963.067\n",
      "Epoch 17, Train loss: 988.5398, Valid loss: 941.7383\n",
      "Epoch 18, Train loss: 968.6529, Valid loss: 923.3667\n",
      "Epoch 19, Train loss: 951.9582, Valid loss: 908.9389\n",
      "Epoch 20, Train loss: 938.4848, Valid loss: 896.9043\n",
      "Epoch 21, Train loss: 927.4591, Valid loss: 887.5424\n",
      "Epoch 22, Train loss: 918.8342, Valid loss: 879.9433\n",
      "Epoch 23, Train loss: 911.763, Valid loss: 873.4953\n",
      "Epoch 24, Train loss: 905.7067, Valid loss: 868.1906\n",
      "Epoch 25, Train loss: 901.2339, Valid loss: 864.4615\n",
      "Epoch 26, Train loss: 897.5296, Valid loss: 860.9872\n",
      "Epoch 27, Train loss: 894.4877, Valid loss: 858.4993\n",
      "Epoch 28, Train loss: 891.6681, Valid loss: 855.8981\n",
      "Epoch 29, Train loss: 889.9873, Valid loss: 854.9816\n",
      "Epoch 30, Train loss: 888.797, Valid loss: 853.5742\n",
      "Epoch 31, Train loss: 887.3831, Valid loss: 852.113\n",
      "Epoch 32, Train loss: 886.7238, Valid loss: 852.161\n",
      "Epoch 33, Train loss: 886.629, Valid loss: 851.8036\n",
      "Epoch 34, Train loss: 886.313, Valid loss: 851.5584\n",
      "Epoch 35, Train loss: 885.5608, Valid loss: 850.6386\n",
      "Epoch 36, Train loss: 885.3725, Valid loss: 851.042\n",
      "Epoch 37, Train loss: 885.3513, Valid loss: 850.616\n",
      "Epoch 38, Train loss: 885.0732, Valid loss: 850.5814\n",
      "Epoch 39, Train loss: 884.8213, Valid loss: 850.2424\n",
      "Epoch 40, Train loss: 884.8611, Valid loss: 850.4739\n",
      "Epoch 41, Train loss: 884.8836, Valid loss: 850.4014\n",
      "Epoch 42, Train loss: 884.9035, Valid loss: 850.2989\n",
      "Epoch 43, Train loss: 884.6886, Valid loss: 850.16\n",
      "Epoch 44, Train loss: 884.4669, Valid loss: 849.7904\n",
      "Epoch 45, Train loss: 884.7247, Valid loss: 850.4148\n",
      "Epoch 46, Train loss: 884.6285, Valid loss: 850.0149\n",
      "Epoch 47, Train loss: 883.9494, Valid loss: 849.4189\n",
      "Epoch 48, Train loss: 884.0215, Valid loss: 849.8291\n",
      "Epoch 49, Train loss: 884.1958, Valid loss: 849.7963\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 883.9494, minimum valid loss = 849.4189\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 20291.2029, Valid loss: 2134.5613\n",
      "Epoch 1, Train loss: 2693.9787, Valid loss: 1916.2256\n",
      "Epoch 2, Train loss: 1953.5645, Valid loss: 1705.5596\n",
      "Epoch 3, Train loss: 1682.6016, Valid loss: 1556.3101\n",
      "Epoch 4, Train loss: 1555.6133, Valid loss: 1472.3976\n",
      "Epoch 5, Train loss: 1493.9553, Valid loss: 1438.627\n",
      "Epoch 6, Train loss: 1475.2573, Valid loss: 1429.7261\n",
      "Epoch 7, Train loss: 1472.277, Valid loss: 1432.2454\n",
      "Epoch 8, Train loss: 1475.6192, Valid loss: 1438.9494\n",
      "Epoch 9, Train loss: 1480.4626, Valid loss: 1442.6299\n",
      "Epoch 10, Train loss: 1482.679, Valid loss: 1443.3874\n",
      "Epoch 11, Train loss: 1482.9289, Valid loss: 1442.8392\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 1475.2573, minimum valid loss = 1429.7261\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 51301.1384, Valid loss: 2194.5822\n",
      "Epoch 1, Train loss: 2246.0786, Valid loss: 2044.847\n",
      "Epoch 2, Train loss: 1998.458, Valid loss: 1881.5832\n",
      "Epoch 3, Train loss: 1864.0451, Valid loss: 1768.6708\n",
      "Epoch 4, Train loss: 1771.6053, Valid loss: 1713.818\n",
      "Epoch 5, Train loss: 1734.0619, Valid loss: 1703.1001\n",
      "Epoch 6, Train loss: 1706.2203, Valid loss: 1707.5669\n",
      "Epoch 7, Train loss: 1688.4667, Valid loss: 1718.8918\n",
      "Epoch 8, Train loss: 1661.2618, Valid loss: 1718.6672\n",
      "Epoch 9, Train loss: 1615.6054, Valid loss: 1691.927\n",
      "Epoch 10, Train loss: 1482.9122, Valid loss: 871.8462\n",
      "Epoch 11, Train loss: 1291.4898, Valid loss: 941.6023\n",
      "Epoch 12, Train loss: 1246.2955, Valid loss: 950.3081\n",
      "Epoch 13, Train loss: 1260.7966, Valid loss: 1006.2046\n",
      "Epoch 14, Train loss: 1255.2569, Valid loss: 886.7168\n",
      "Epoch 15, Train loss: 1201.7091, Valid loss: 845.4688\n",
      "Epoch 16, Train loss: 1247.1163, Valid loss: 890.1028\n",
      "Epoch 17, Train loss: 1260.4073, Valid loss: 857.6532\n",
      "Epoch 18, Train loss: 1205.1786, Valid loss: 794.2322\n",
      "Epoch 19, Train loss: 1154.2259, Valid loss: 779.8633\n",
      "Epoch 20, Train loss: 1164.8653, Valid loss: 760.4109\n",
      "Epoch 21, Train loss: 1174.3986, Valid loss: 816.1445\n",
      "Epoch 22, Train loss: 1169.5395, Valid loss: 622.0496\n",
      "Epoch 23, Train loss: 1124.89, Valid loss: 728.7173\n",
      "Epoch 24, Train loss: 1133.3129, Valid loss: 688.7289\n",
      "Epoch 25, Train loss: 1153.62, Valid loss: 536.1336\n",
      "Epoch 26, Train loss: 1178.0313, Valid loss: 677.177\n",
      "Epoch 27, Train loss: 1142.4519, Valid loss: 837.9027\n",
      "Epoch 28, Train loss: 1151.586, Valid loss: 542.271\n",
      "Epoch 29, Train loss: 1138.6698, Valid loss: 632.3742\n",
      "Epoch 30, Train loss: 1181.5459, Valid loss: 826.3888\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 25, training loss = 1153.62, minimum valid loss = 536.1336\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1879.7759, Valid loss: 970.505\n",
      "Epoch 1, Train loss: 1483.4274, Valid loss: 861.2787\n",
      "Epoch 2, Train loss: 1363.5777, Valid loss: 1009.3113\n",
      "Epoch 3, Train loss: 1210.6241, Valid loss: 650.3552\n",
      "Epoch 4, Train loss: 1059.2649, Valid loss: 455.8999\n",
      "Epoch 5, Train loss: 914.7672, Valid loss: 374.909\n",
      "Epoch 6, Train loss: 837.1753, Valid loss: 329.8724\n",
      "Epoch 7, Train loss: 805.345, Valid loss: 476.2511\n",
      "Epoch 8, Train loss: 780.6435, Valid loss: 377.0111\n",
      "Epoch 9, Train loss: 731.6881, Valid loss: 326.0012\n",
      "Epoch 10, Train loss: 688.8708, Valid loss: 453.5443\n",
      "Epoch 11, Train loss: 686.4033, Valid loss: 452.246\n",
      "Epoch 12, Train loss: 667.3135, Valid loss: 374.4922\n",
      "Epoch 13, Train loss: 657.1093, Valid loss: 255.803\n",
      "Epoch 14, Train loss: 645.4331, Valid loss: 374.1362\n",
      "Epoch 15, Train loss: 620.85, Valid loss: 308.5781\n",
      "Epoch 16, Train loss: 629.5909, Valid loss: 305.4274\n",
      "Epoch 17, Train loss: 608.0704, Valid loss: 363.5109\n",
      "Epoch 18, Train loss: 612.2873, Valid loss: 349.41\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 13, training loss = 657.1093, minimum valid loss = 255.803\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1291.1356, Valid loss: 753.735\n",
      "Epoch 1, Train loss: 1014.4011, Valid loss: 833.4808\n",
      "Epoch 2, Train loss: 909.0473, Valid loss: 471.0639\n",
      "Epoch 3, Train loss: 765.9248, Valid loss: 253.398\n",
      "Epoch 4, Train loss: 717.1286, Valid loss: 343.3294\n",
      "Epoch 5, Train loss: 673.8941, Valid loss: 280.1173\n",
      "Epoch 6, Train loss: 660.5479, Valid loss: 336.1875\n",
      "Epoch 7, Train loss: 648.7881, Valid loss: 237.7139\n",
      "Epoch 8, Train loss: 633.6197, Valid loss: 408.5133\n",
      "Epoch 9, Train loss: 594.6173, Valid loss: 275.7465\n",
      "Epoch 10, Train loss: 577.0569, Valid loss: 274.7257\n",
      "Epoch 11, Train loss: 592.4628, Valid loss: 171.9934\n",
      "Epoch 12, Train loss: 563.8327, Valid loss: 258.8127\n",
      "Epoch 13, Train loss: 546.5631, Valid loss: 332.5116\n",
      "Epoch 14, Train loss: 546.7488, Valid loss: 268.4927\n",
      "Epoch 15, Train loss: 525.467, Valid loss: 256.4842\n",
      "Epoch 16, Train loss: 485.4313, Valid loss: 265.1658\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 11, training loss = 592.4628, minimum valid loss = 171.9934\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2169.584, Valid loss: 1963.5822\n",
      "Epoch 1, Train loss: 1597.1418, Valid loss: 988.3179\n",
      "Epoch 2, Train loss: 1156.1677, Valid loss: 458.7009\n",
      "Epoch 3, Train loss: 1044.8113, Valid loss: 881.788\n",
      "Epoch 4, Train loss: 824.2837, Valid loss: 338.6358\n",
      "Epoch 5, Train loss: 750.2049, Valid loss: 361.8057\n",
      "Epoch 6, Train loss: 718.0893, Valid loss: 523.8332\n",
      "Epoch 7, Train loss: 662.1242, Valid loss: 338.8306\n",
      "Epoch 8, Train loss: 673.0722, Valid loss: 342.8802\n",
      "Epoch 9, Train loss: 644.0328, Valid loss: 310.7983\n",
      "Epoch 10, Train loss: 670.2429, Valid loss: 603.2043\n",
      "Epoch 11, Train loss: 697.8247, Valid loss: 430.6634\n",
      "Epoch 12, Train loss: 596.3191, Valid loss: 259.3174\n",
      "Epoch 13, Train loss: 525.5819, Valid loss: 477.0883\n",
      "Epoch 14, Train loss: 607.1905, Valid loss: 208.3486\n",
      "Epoch 15, Train loss: 531.7965, Valid loss: 200.1498\n",
      "Epoch 16, Train loss: 516.3442, Valid loss: 262.3116\n",
      "Epoch 17, Train loss: 516.8674, Valid loss: 295.276\n",
      "Epoch 18, Train loss: 524.0163, Valid loss: 271.4917\n",
      "Epoch 19, Train loss: 520.2762, Valid loss: 222.9189\n",
      "Epoch 20, Train loss: 497.1548, Valid loss: 191.9128\n",
      "Epoch 21, Train loss: 492.9625, Valid loss: 281.6614\n",
      "Epoch 22, Train loss: 495.1414, Valid loss: 263.984\n",
      "Epoch 23, Train loss: 496.849, Valid loss: 298.6992\n",
      "Epoch 24, Train loss: 490.9423, Valid loss: 201.4958\n",
      "Epoch 25, Train loss: 473.4651, Valid loss: 282.0275\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 20, training loss = 497.1548, minimum valid loss = 191.9128\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1570.4557, Valid loss: 787.5471\n",
      "Epoch 1, Train loss: 1123.0287, Valid loss: 867.9053\n",
      "Epoch 2, Train loss: 885.9824, Valid loss: 597.5066\n",
      "Epoch 3, Train loss: 691.8084, Valid loss: 280.2788\n",
      "Epoch 4, Train loss: 656.3236, Valid loss: 338.3539\n",
      "Epoch 5, Train loss: 617.3812, Valid loss: 412.8226\n",
      "Epoch 6, Train loss: 669.4112, Valid loss: 336.2376\n",
      "Epoch 7, Train loss: 642.4782, Valid loss: 328.4921\n",
      "Epoch 8, Train loss: 642.9436, Valid loss: 263.215\n",
      "Epoch 9, Train loss: 632.2365, Valid loss: 208.5522\n",
      "Epoch 10, Train loss: 611.9499, Valid loss: 528.0154\n",
      "Epoch 11, Train loss: 699.995, Valid loss: 340.6537\n",
      "Epoch 12, Train loss: 621.0432, Valid loss: 257.9936\n",
      "Epoch 13, Train loss: 605.8927, Valid loss: 309.5854\n",
      "Epoch 14, Train loss: 618.2144, Valid loss: 284.7895\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 9, training loss = 632.2365, minimum valid loss = 208.5522\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1680.634, Valid loss: 702.9239\n",
      "Epoch 1, Train loss: 945.6091, Valid loss: 439.9526\n",
      "Epoch 2, Train loss: 752.3021, Valid loss: 240.1672\n",
      "Epoch 3, Train loss: 680.7131, Valid loss: 628.5379\n",
      "Epoch 4, Train loss: 692.3974, Valid loss: 376.6325\n",
      "Epoch 5, Train loss: 695.499, Valid loss: 267.9446\n",
      "Epoch 6, Train loss: 660.2184, Valid loss: 335.1141\n",
      "Epoch 7, Train loss: 688.2288, Valid loss: 308.1368\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 2, training loss = 752.3021, minimum valid loss = 240.1672\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1353.6206, Valid loss: 696.6726\n",
      "Epoch 1, Train loss: 970.3482, Valid loss: 275.4081\n",
      "Epoch 2, Train loss: 897.5368, Valid loss: 325.4566\n",
      "Epoch 3, Train loss: 807.3608, Valid loss: 441.4554\n",
      "Epoch 4, Train loss: 786.9175, Valid loss: 359.8024\n",
      "Epoch 5, Train loss: 759.7836, Valid loss: 292.5459\n",
      "Epoch 6, Train loss: 717.2209, Valid loss: 248.1171\n",
      "Epoch 7, Train loss: 683.953, Valid loss: 324.8612\n",
      "Epoch 8, Train loss: 716.221, Valid loss: 355.8264\n",
      "Epoch 9, Train loss: 664.7412, Valid loss: 285.3882\n",
      "Epoch 10, Train loss: 651.8356, Valid loss: 249.0685\n",
      "Epoch 11, Train loss: 646.4882, Valid loss: 333.0776\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 717.2209, minimum valid loss = 248.1171\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1373.5493, Valid loss: 536.3916\n",
      "Epoch 1, Train loss: 919.803, Valid loss: 474.4375\n",
      "Epoch 2, Train loss: 810.0248, Valid loss: 504.9966\n",
      "Epoch 3, Train loss: 857.4183, Valid loss: 268.7144\n",
      "Epoch 4, Train loss: 745.5199, Valid loss: 299.0537\n",
      "Epoch 5, Train loss: 736.3103, Valid loss: 315.5335\n",
      "Epoch 6, Train loss: 747.0503, Valid loss: 403.9662\n",
      "Epoch 7, Train loss: 706.9749, Valid loss: 318.5543\n",
      "Epoch 8, Train loss: 710.8257, Valid loss: 432.1933\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 3, training loss = 857.4183, minimum valid loss = 268.7144\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1621.9853, Valid loss: 945.677\n",
      "Epoch 1, Train loss: 1356.373, Valid loss: 813.714\n",
      "Epoch 2, Train loss: 1255.659, Valid loss: 552.9849\n",
      "Epoch 3, Train loss: 1086.1598, Valid loss: 507.2898\n",
      "Epoch 4, Train loss: 930.4973, Valid loss: 560.0214\n",
      "Epoch 5, Train loss: 897.8122, Valid loss: 476.4622\n",
      "Epoch 6, Train loss: 831.6784, Valid loss: 404.0221\n",
      "Epoch 7, Train loss: 855.1526, Valid loss: 450.6513\n",
      "Epoch 8, Train loss: 847.1656, Valid loss: 333.0814\n",
      "Epoch 9, Train loss: 820.5017, Valid loss: 323.8996\n",
      "Epoch 10, Train loss: 792.6321, Valid loss: 378.4561\n",
      "Epoch 11, Train loss: 772.7466, Valid loss: 435.1118\n",
      "Epoch 12, Train loss: 757.5982, Valid loss: 465.5942\n",
      "Epoch 13, Train loss: 761.3439, Valid loss: 440.0365\n",
      "Epoch 14, Train loss: 753.4864, Valid loss: 322.2377\n",
      "Epoch 15, Train loss: 752.9377, Valid loss: 475.9731\n",
      "Epoch 16, Train loss: 714.5252, Valid loss: 311.8013\n",
      "Epoch 17, Train loss: 689.5794, Valid loss: 375.9583\n",
      "Epoch 18, Train loss: 670.7453, Valid loss: 366.6697\n",
      "Epoch 19, Train loss: 595.3165, Valid loss: 257.878\n",
      "Epoch 20, Train loss: 585.7833, Valid loss: 275.635\n",
      "Epoch 21, Train loss: 580.2949, Valid loss: 258.9735\n",
      "Epoch 22, Train loss: 555.3014, Valid loss: 356.7495\n",
      "Epoch 23, Train loss: 571.5962, Valid loss: 263.3548\n",
      "Epoch 24, Train loss: 584.1682, Valid loss: 193.91\n",
      "Epoch 25, Train loss: 588.1048, Valid loss: 434.8755\n",
      "Epoch 26, Train loss: 557.8776, Valid loss: 237.0398\n",
      "Epoch 27, Train loss: 569.1706, Valid loss: 254.4411\n",
      "Epoch 28, Train loss: 526.4702, Valid loss: 246.1807\n",
      "Epoch 29, Train loss: 563.4613, Valid loss: 297.5444\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 24, training loss = 584.1682, minimum valid loss = 193.91\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1437.6962, Valid loss: 664.8356\n",
      "Epoch 1, Train loss: 1012.123, Valid loss: 517.1663\n",
      "Epoch 2, Train loss: 828.7941, Valid loss: 502.9352\n",
      "Epoch 3, Train loss: 793.3612, Valid loss: 277.1708\n",
      "Epoch 4, Train loss: 802.7473, Valid loss: 433.3061\n",
      "Epoch 5, Train loss: 717.9613, Valid loss: 436.1902\n",
      "Epoch 6, Train loss: 651.0577, Valid loss: 283.5101\n",
      "Epoch 7, Train loss: 648.5839, Valid loss: 293.5635\n",
      "Epoch 8, Train loss: 626.9462, Valid loss: 265.779\n",
      "Epoch 9, Train loss: 620.9316, Valid loss: 290.775\n",
      "Epoch 10, Train loss: 630.5457, Valid loss: 291.7627\n",
      "Epoch 11, Train loss: 637.0638, Valid loss: 326.1488\n",
      "Epoch 12, Train loss: 625.237, Valid loss: 307.8108\n",
      "Epoch 13, Train loss: 608.3605, Valid loss: 283.7334\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 626.9462, minimum valid loss = 265.779\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1722.3233, Valid loss: 771.0864\n",
      "Epoch 1, Train loss: 1053.8465, Valid loss: 419.9004\n",
      "Epoch 2, Train loss: 843.8671, Valid loss: 491.3982\n",
      "Epoch 3, Train loss: 807.5403, Valid loss: 543.5242\n",
      "Epoch 4, Train loss: 787.5059, Valid loss: 389.9236\n",
      "Epoch 5, Train loss: 668.4695, Valid loss: 351.3873\n",
      "Epoch 6, Train loss: 658.9734, Valid loss: 244.7762\n",
      "Epoch 7, Train loss: 672.8505, Valid loss: 294.2609\n",
      "Epoch 8, Train loss: 622.8614, Valid loss: 219.2689\n",
      "Epoch 9, Train loss: 641.4752, Valid loss: 254.5909\n",
      "Epoch 10, Train loss: 642.0327, Valid loss: 283.2815\n",
      "Epoch 11, Train loss: 649.4061, Valid loss: 331.5823\n",
      "Epoch 12, Train loss: 655.6546, Valid loss: 297.2848\n",
      "Epoch 13, Train loss: 645.1744, Valid loss: 219.3547\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 622.8614, minimum valid loss = 219.2689\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1741.9271, Valid loss: 743.4317\n",
      "Epoch 1, Train loss: 1195.7186, Valid loss: 556.8229\n",
      "Epoch 2, Train loss: 971.1518, Valid loss: 446.1138\n",
      "Epoch 3, Train loss: 822.3819, Valid loss: 338.2934\n",
      "Epoch 4, Train loss: 754.4937, Valid loss: 351.3948\n",
      "Epoch 5, Train loss: 726.1606, Valid loss: 377.3639\n",
      "Epoch 6, Train loss: 737.4503, Valid loss: 433.3172\n",
      "Epoch 7, Train loss: 694.4557, Valid loss: 318.8688\n",
      "Epoch 8, Train loss: 664.9487, Valid loss: 259.4299\n",
      "Epoch 9, Train loss: 678.4998, Valid loss: 262.3513\n",
      "Epoch 10, Train loss: 663.262, Valid loss: 304.5917\n",
      "Epoch 11, Train loss: 699.2833, Valid loss: 247.8652\n",
      "Epoch 12, Train loss: 679.8414, Valid loss: 320.7366\n",
      "Epoch 13, Train loss: 667.2732, Valid loss: 296.2843\n",
      "Epoch 14, Train loss: 676.1669, Valid loss: 359.8783\n",
      "Epoch 15, Train loss: 663.405, Valid loss: 314.4417\n",
      "Epoch 16, Train loss: 663.0053, Valid loss: 301.1638\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 11, training loss = 699.2833, minimum valid loss = 247.8652\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2099.5116, Valid loss: 1645.387\n",
      "Epoch 1, Train loss: 1671.8761, Valid loss: 1065.9778\n",
      "Epoch 2, Train loss: 1436.9789, Valid loss: 863.8702\n",
      "Epoch 3, Train loss: 1337.5391, Valid loss: 848.634\n",
      "Epoch 4, Train loss: 1360.2857, Valid loss: 889.7799\n",
      "Epoch 5, Train loss: 1341.0332, Valid loss: 846.105\n",
      "Epoch 6, Train loss: 1279.5453, Valid loss: 736.8626\n",
      "Epoch 7, Train loss: 1240.5083, Valid loss: 802.894\n",
      "Epoch 8, Train loss: 1280.3644, Valid loss: 754.1318\n",
      "Epoch 9, Train loss: 1223.2044, Valid loss: 743.4281\n",
      "Epoch 10, Train loss: 1242.2193, Valid loss: 713.3632\n",
      "Epoch 11, Train loss: 1191.846, Valid loss: 680.4353\n",
      "Epoch 12, Train loss: 1225.5657, Valid loss: 774.6337\n",
      "Epoch 13, Train loss: 1215.7712, Valid loss: 726.0964\n",
      "Epoch 14, Train loss: 1213.3909, Valid loss: 692.9409\n",
      "Epoch 15, Train loss: 1166.9491, Valid loss: 644.3189\n",
      "Epoch 16, Train loss: 1163.1599, Valid loss: 643.5062\n",
      "Epoch 17, Train loss: 1206.3241, Valid loss: 711.773\n",
      "Epoch 18, Train loss: 1144.4564, Valid loss: 665.001\n",
      "Epoch 19, Train loss: 1137.4194, Valid loss: 656.5635\n",
      "Epoch 20, Train loss: 1153.4882, Valid loss: 689.2197\n",
      "Epoch 21, Train loss: 1187.4771, Valid loss: 717.6757\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 16, training loss = 1163.1599, minimum valid loss = 643.5062\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1867.4815, Valid loss: 985.5837\n",
      "Epoch 1, Train loss: 1420.5596, Valid loss: 678.1481\n",
      "Epoch 2, Train loss: 1128.9823, Valid loss: 583.5954\n",
      "Epoch 3, Train loss: 1036.5005, Valid loss: 496.6504\n",
      "Epoch 4, Train loss: 963.1058, Valid loss: 450.4609\n",
      "Epoch 5, Train loss: 974.1311, Valid loss: 499.3011\n",
      "Epoch 6, Train loss: 942.6265, Valid loss: 514.4469\n",
      "Epoch 7, Train loss: 917.0499, Valid loss: 454.9054\n",
      "Epoch 8, Train loss: 926.5784, Valid loss: 436.9904\n",
      "Epoch 9, Train loss: 921.4258, Valid loss: 384.026\n",
      "Epoch 10, Train loss: 934.418, Valid loss: 540.0979\n",
      "Epoch 11, Train loss: 928.8338, Valid loss: 503.3506\n",
      "Epoch 12, Train loss: 924.1771, Valid loss: 526.0403\n",
      "Epoch 13, Train loss: 947.6933, Valid loss: 456.9095\n",
      "Epoch 14, Train loss: 907.9206, Valid loss: 437.4309\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 9, training loss = 921.4258, minimum valid loss = 384.026\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2018.7553, Valid loss: 1419.1123\n",
      "Epoch 1, Train loss: 1441.2692, Valid loss: 789.2886\n",
      "Epoch 2, Train loss: 1133.6526, Valid loss: 609.1465\n",
      "Epoch 3, Train loss: 1045.6775, Valid loss: 536.167\n",
      "Epoch 4, Train loss: 1015.1446, Valid loss: 509.2999\n",
      "Epoch 5, Train loss: 958.2457, Valid loss: 451.7233\n",
      "Epoch 6, Train loss: 937.2337, Valid loss: 491.0516\n",
      "Epoch 7, Train loss: 944.6192, Valid loss: 464.5616\n",
      "Epoch 8, Train loss: 906.4436, Valid loss: 501.8455\n",
      "Epoch 9, Train loss: 954.528, Valid loss: 445.0769\n",
      "Epoch 10, Train loss: 926.0026, Valid loss: 425.208\n",
      "Epoch 11, Train loss: 919.6645, Valid loss: 423.4939\n",
      "Epoch 12, Train loss: 888.6772, Valid loss: 419.1302\n",
      "Epoch 13, Train loss: 955.8573, Valid loss: 466.8446\n",
      "Epoch 14, Train loss: 939.8367, Valid loss: 547.5762\n",
      "Epoch 15, Train loss: 892.6913, Valid loss: 419.9926\n",
      "Epoch 16, Train loss: 917.1662, Valid loss: 443.1893\n",
      "Epoch 17, Train loss: 892.4199, Valid loss: 490.7772\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 12, training loss = 888.6772, minimum valid loss = 419.1302\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1795.5134, Valid loss: 891.0714\n",
      "Epoch 1, Train loss: 1320.0403, Valid loss: 720.912\n",
      "Epoch 2, Train loss: 1001.4352, Valid loss: 454.3266\n",
      "Epoch 3, Train loss: 864.0224, Valid loss: 478.0314\n",
      "Epoch 4, Train loss: 808.7967, Valid loss: 337.398\n",
      "Epoch 5, Train loss: 760.1544, Valid loss: 391.1793\n",
      "Epoch 6, Train loss: 701.1595, Valid loss: 336.7127\n",
      "Epoch 7, Train loss: 732.3091, Valid loss: 293.4882\n",
      "Epoch 8, Train loss: 667.8984, Valid loss: 289.9059\n",
      "Epoch 9, Train loss: 674.1705, Valid loss: 383.7289\n",
      "Epoch 10, Train loss: 691.1549, Valid loss: 277.6174\n",
      "Epoch 11, Train loss: 677.7766, Valid loss: 312.0398\n",
      "Epoch 12, Train loss: 654.1616, Valid loss: 356.768\n",
      "Epoch 13, Train loss: 637.4354, Valid loss: 253.4458\n",
      "Epoch 14, Train loss: 693.9154, Valid loss: 313.8701\n",
      "Epoch 15, Train loss: 687.5822, Valid loss: 268.9911\n",
      "Epoch 16, Train loss: 650.1383, Valid loss: 304.7249\n",
      "Epoch 17, Train loss: 690.5376, Valid loss: 304.2845\n",
      "Epoch 18, Train loss: 649.7537, Valid loss: 368.8793\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 13, training loss = 637.4354, minimum valid loss = 253.4458\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2248.6624, Valid loss: 2166.187\n",
      "Epoch 1, Train loss: 2207.8835, Valid loss: 2160.4049\n",
      "Epoch 2, Train loss: 2201.559, Valid loss: 2155.4439\n",
      "Epoch 3, Train loss: 2196.5197, Valid loss: 2150.4967\n",
      "Epoch 4, Train loss: 2191.5261, Valid loss: 2145.5628\n",
      "Epoch 5, Train loss: 2186.5718, Valid loss: 2140.634\n",
      "Epoch 6, Train loss: 2181.628, Valid loss: 2135.7103\n",
      "Epoch 7, Train loss: 2176.669, Valid loss: 2130.8215\n",
      "Epoch 8, Train loss: 2171.7767, Valid loss: 2125.9148\n",
      "Epoch 9, Train loss: 2166.8436, Valid loss: 2121.0389\n",
      "Epoch 10, Train loss: 2161.9598, Valid loss: 2116.1582\n",
      "Epoch 11, Train loss: 2157.058, Valid loss: 2111.3016\n",
      "Epoch 12, Train loss: 2152.1892, Valid loss: 2106.4482\n",
      "Epoch 13, Train loss: 2147.3017, Valid loss: 2101.6327\n",
      "Epoch 14, Train loss: 2142.4821, Valid loss: 2096.789\n",
      "Epoch 15, Train loss: 2137.6253, Valid loss: 2091.9771\n",
      "Epoch 16, Train loss: 2132.782, Valid loss: 2087.1956\n",
      "Epoch 17, Train loss: 2127.994, Valid loss: 2082.3881\n",
      "Epoch 18, Train loss: 2123.1661, Valid loss: 2077.6254\n",
      "Epoch 19, Train loss: 2118.3893, Valid loss: 2072.8526\n",
      "Epoch 20, Train loss: 2113.6059, Valid loss: 2068.0976\n",
      "Epoch 21, Train loss: 2108.8169, Valid loss: 2063.3817\n",
      "Epoch 22, Train loss: 2104.0822, Valid loss: 2058.6476\n",
      "Epoch 23, Train loss: 2099.3355, Valid loss: 2053.9267\n",
      "Epoch 24, Train loss: 2094.5926, Valid loss: 2049.2282\n",
      "Epoch 25, Train loss: 2089.8863, Valid loss: 2044.5207\n",
      "Epoch 26, Train loss: 2085.1571, Valid loss: 2039.8484\n",
      "Epoch 27, Train loss: 2080.4594, Valid loss: 2035.1872\n",
      "Epoch 28, Train loss: 2075.7839, Valid loss: 2030.5237\n",
      "Epoch 29, Train loss: 2071.1134, Valid loss: 2025.8642\n",
      "Epoch 30, Train loss: 2066.4353, Valid loss: 2021.2356\n",
      "Epoch 31, Train loss: 2061.7951, Valid loss: 2016.6067\n",
      "Epoch 32, Train loss: 2057.1539, Valid loss: 2011.9939\n",
      "Epoch 33, Train loss: 2052.5193, Valid loss: 2007.4001\n",
      "Epoch 34, Train loss: 2047.9153, Valid loss: 2002.8027\n",
      "Epoch 35, Train loss: 2043.2936, Valid loss: 1998.2374\n",
      "Epoch 36, Train loss: 2038.7215, Valid loss: 1993.6603\n",
      "Epoch 37, Train loss: 2034.12, Valid loss: 1989.1186\n",
      "Epoch 38, Train loss: 2029.5608, Valid loss: 1984.5754\n",
      "Epoch 39, Train loss: 2024.9963, Valid loss: 1980.0466\n",
      "Epoch 40, Train loss: 2020.4601, Valid loss: 1975.5157\n",
      "Epoch 41, Train loss: 2015.9214, Valid loss: 1971.0052\n",
      "Epoch 42, Train loss: 2011.3974, Valid loss: 1966.5081\n",
      "Epoch 43, Train loss: 2006.8782, Valid loss: 1962.0338\n",
      "Epoch 44, Train loss: 2002.3798, Valid loss: 1957.5661\n",
      "Epoch 45, Train loss: 1997.889, Valid loss: 1953.1092\n",
      "Epoch 46, Train loss: 1993.4244, Valid loss: 1948.645\n",
      "Epoch 47, Train loss: 1988.9432, Valid loss: 1944.2117\n",
      "Epoch 48, Train loss: 1984.4971, Valid loss: 1939.7771\n",
      "Epoch 49, Train loss: 1980.0421, Valid loss: 1935.3662\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 1980.0421, minimum valid loss = 1935.3662\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1917.0033, Valid loss: 1352.9208\n",
      "Epoch 1, Train loss: 1410.6639, Valid loss: 728.298\n",
      "Epoch 2, Train loss: 1180.7547, Valid loss: 706.7503\n",
      "Epoch 3, Train loss: 1002.1984, Valid loss: 492.4453\n",
      "Epoch 4, Train loss: 939.9292, Valid loss: 449.6883\n",
      "Epoch 5, Train loss: 901.6889, Valid loss: 438.9269\n",
      "Epoch 6, Train loss: 877.297, Valid loss: 382.8253\n",
      "Epoch 7, Train loss: 842.2625, Valid loss: 428.6905\n",
      "Epoch 8, Train loss: 809.323, Valid loss: 417.0168\n",
      "Epoch 9, Train loss: 793.2199, Valid loss: 419.6608\n",
      "Epoch 10, Train loss: 808.08, Valid loss: 344.9222\n",
      "Epoch 11, Train loss: 836.8215, Valid loss: 394.7218\n",
      "Epoch 12, Train loss: 801.6247, Valid loss: 368.6279\n",
      "Epoch 13, Train loss: 814.7506, Valid loss: 360.1258\n",
      "Epoch 14, Train loss: 784.4339, Valid loss: 303.8913\n",
      "Epoch 15, Train loss: 814.253, Valid loss: 378.9214\n",
      "Epoch 16, Train loss: 782.0339, Valid loss: 352.5088\n",
      "Epoch 17, Train loss: 793.9777, Valid loss: 344.2657\n",
      "Epoch 18, Train loss: 784.279, Valid loss: 333.4822\n",
      "Epoch 19, Train loss: 776.5072, Valid loss: 333.0614\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 14, training loss = 784.4339, minimum valid loss = 303.8913\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 5}\n",
      "Epoch 0, Train loss: 2083.1519, Valid loss: 1805.0723\n",
      "Epoch 1, Train loss: 1753.7952, Valid loss: 1136.8964\n",
      "Epoch 2, Train loss: 1530.5796, Valid loss: 1076.5813\n",
      "Epoch 3, Train loss: 1322.0278, Valid loss: 786.462\n",
      "Epoch 4, Train loss: 1173.9067, Valid loss: 667.6166\n",
      "Epoch 5, Train loss: 1099.5932, Valid loss: 596.1389\n",
      "Epoch 6, Train loss: 1088.4484, Valid loss: 577.1928\n",
      "Epoch 7, Train loss: 1004.6571, Valid loss: 485.2508\n",
      "Epoch 8, Train loss: 1002.668, Valid loss: 472.2564\n",
      "Epoch 9, Train loss: 905.4084, Valid loss: 340.7604\n",
      "Epoch 10, Train loss: 839.5846, Valid loss: 396.3792\n",
      "Epoch 11, Train loss: 799.8535, Valid loss: 353.7047\n",
      "Epoch 12, Train loss: 819.1855, Valid loss: 319.3349\n",
      "Epoch 13, Train loss: 814.4036, Valid loss: 330.2884\n",
      "Epoch 14, Train loss: 801.6368, Valid loss: 305.1564\n",
      "Epoch 15, Train loss: 776.2588, Valid loss: 391.3205\n",
      "Epoch 16, Train loss: 760.0329, Valid loss: 332.8653\n",
      "Epoch 17, Train loss: 752.0222, Valid loss: 341.3817\n",
      "Epoch 18, Train loss: 748.245, Valid loss: 353.9114\n",
      "Epoch 19, Train loss: 778.448, Valid loss: 366.8894\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 14, training loss = 801.6368, minimum valid loss = 305.1564\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 5}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1889.5005, Valid loss: 1332.8936\n",
      "Epoch 1, Train loss: 1351.6389, Valid loss: 739.845\n",
      "Epoch 2, Train loss: 1102.5443, Valid loss: 658.2862\n",
      "Epoch 3, Train loss: 1029.0771, Valid loss: 537.3201\n",
      "Epoch 4, Train loss: 940.2823, Valid loss: 412.6565\n",
      "Epoch 5, Train loss: 864.2081, Valid loss: 379.8898\n",
      "Epoch 6, Train loss: 793.303, Valid loss: 358.9403\n",
      "Epoch 7, Train loss: 754.41, Valid loss: 345.7339\n",
      "Epoch 8, Train loss: 716.3474, Valid loss: 309.0104\n",
      "Epoch 9, Train loss: 720.7945, Valid loss: 328.3569\n",
      "Epoch 10, Train loss: 720.6478, Valid loss: 353.0329\n",
      "Epoch 11, Train loss: 733.5692, Valid loss: 287.1176\n",
      "Epoch 12, Train loss: 706.108, Valid loss: 345.8008\n",
      "Epoch 13, Train loss: 704.2282, Valid loss: 350.0956\n",
      "Epoch 14, Train loss: 714.7827, Valid loss: 304.2109\n",
      "Epoch 15, Train loss: 707.1548, Valid loss: 332.5176\n",
      "Epoch 16, Train loss: 661.6832, Valid loss: 319.7682\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 11, training loss = 733.5692, minimum valid loss = 287.1176\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 5}\n",
      "Epoch 0, Train loss: 1981.5805, Valid loss: 1517.6839\n",
      "Epoch 1, Train loss: 1555.5997, Valid loss: 958.635\n",
      "Epoch 2, Train loss: 1417.6412, Valid loss: 941.8019\n",
      "Epoch 3, Train loss: 1307.6788, Valid loss: 758.9607\n",
      "Epoch 4, Train loss: 1191.0445, Valid loss: 706.6256\n",
      "Epoch 5, Train loss: 1120.4499, Valid loss: 559.9468\n",
      "Epoch 6, Train loss: 992.0451, Valid loss: 525.1814\n",
      "Epoch 7, Train loss: 987.397, Valid loss: 526.1309\n",
      "Epoch 8, Train loss: 949.136, Valid loss: 533.3688\n",
      "Epoch 9, Train loss: 933.7385, Valid loss: 474.0221\n",
      "Epoch 10, Train loss: 963.1875, Valid loss: 551.9543\n",
      "Epoch 11, Train loss: 926.8627, Valid loss: 448.0542\n",
      "Epoch 12, Train loss: 946.3839, Valid loss: 476.6328\n",
      "Epoch 13, Train loss: 922.4607, Valid loss: 463.4269\n",
      "Epoch 14, Train loss: 930.4979, Valid loss: 440.5323\n",
      "Epoch 15, Train loss: 930.341, Valid loss: 432.9081\n",
      "Epoch 16, Train loss: 934.2135, Valid loss: 439.8851\n",
      "Epoch 17, Train loss: 932.9713, Valid loss: 433.2266\n",
      "Epoch 18, Train loss: 956.085, Valid loss: 486.6959\n",
      "Epoch 19, Train loss: 919.995, Valid loss: 513.8209\n",
      "Epoch 20, Train loss: 947.0884, Valid loss: 459.807\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 15, training loss = 930.341, minimum valid loss = 432.9081\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 338442.8894, Valid loss: 2261.5144\n",
      "Epoch 1, Train loss: 2301.9877, Valid loss: 2239.7526\n",
      "Epoch 2, Train loss: 2259.4259, Valid loss: 2169.7958\n",
      "Epoch 3, Train loss: 2172.9316, Valid loss: 2079.8236\n",
      "Epoch 4, Train loss: 2077.907, Valid loss: 1981.7044\n",
      "Epoch 5, Train loss: 1978.1951, Valid loss: 1880.1568\n",
      "Epoch 6, Train loss: 1875.6081, Valid loss: 1778.5448\n",
      "Epoch 7, Train loss: 1774.3293, Valid loss: 1678.9957\n",
      "Epoch 8, Train loss: 1675.4652, Valid loss: 1583.2212\n",
      "Epoch 9, Train loss: 1581.2759, Valid loss: 1491.4502\n",
      "Epoch 10, Train loss: 1491.2526, Valid loss: 1405.486\n",
      "Epoch 11, Train loss: 1407.1674, Valid loss: 1325.1162\n",
      "Epoch 12, Train loss: 1329.2412, Valid loss: 1250.514\n",
      "Epoch 13, Train loss: 1256.7748, Valid loss: 1182.6113\n",
      "Epoch 14, Train loss: 1191.2734, Valid loss: 1120.3842\n",
      "Epoch 15, Train loss: 1131.381, Valid loss: 1064.3553\n",
      "Epoch 16, Train loss: 1077.2437, Valid loss: 1014.3968\n",
      "Epoch 17, Train loss: 1029.4776, Valid loss: 969.3683\n",
      "Epoch 18, Train loss: 986.485, Valid loss: 929.6784\n",
      "Epoch 19, Train loss: 948.5442, Valid loss: 894.759\n",
      "Epoch 20, Train loss: 915.3188, Valid loss: 864.1501\n",
      "Epoch 21, Train loss: 886.077, Valid loss: 837.6957\n",
      "Epoch 22, Train loss: 853.3694, Valid loss: 814.6501\n",
      "Epoch 23, Train loss: 835.182, Valid loss: 796.8581\n",
      "Epoch 24, Train loss: 823.0713, Valid loss: 780.2759\n",
      "Epoch 25, Train loss: 841.4744, Valid loss: 765.286\n",
      "Epoch 26, Train loss: 792.159, Valid loss: 752.459\n",
      "Epoch 27, Train loss: 780.0895, Valid loss: 741.6762\n",
      "Epoch 28, Train loss: 769.7742, Valid loss: 732.7877\n",
      "Epoch 29, Train loss: 761.4371, Valid loss: 725.0316\n",
      "Epoch 30, Train loss: 754.1336, Valid loss: 718.6968\n",
      "Epoch 31, Train loss: 748.1129, Valid loss: 713.4507\n",
      "Epoch 32, Train loss: 743.1624, Valid loss: 709.0163\n",
      "Epoch 33, Train loss: 738.907, Valid loss: 705.4896\n",
      "Epoch 34, Train loss: 735.5132, Valid loss: 702.484\n",
      "Epoch 35, Train loss: 732.6678, Valid loss: 699.989\n",
      "Epoch 36, Train loss: 730.251, Valid loss: 698.0431\n",
      "Epoch 37, Train loss: 728.3998, Valid loss: 696.3075\n",
      "Epoch 38, Train loss: 726.7009, Valid loss: 695.0277\n",
      "Epoch 39, Train loss: 725.4901, Valid loss: 693.8\n",
      "Epoch 40, Train loss: 724.2882, Valid loss: 692.972\n",
      "Epoch 41, Train loss: 723.4387, Valid loss: 692.2399\n",
      "Epoch 42, Train loss: 722.725, Valid loss: 691.5919\n",
      "Epoch 43, Train loss: 722.1119, Valid loss: 691.0521\n",
      "Epoch 44, Train loss: 721.5782, Valid loss: 690.6847\n",
      "Epoch 45, Train loss: 721.1877, Valid loss: 690.4063\n",
      "Epoch 46, Train loss: 720.8707, Valid loss: 690.119\n",
      "Epoch 47, Train loss: 720.5736, Valid loss: 689.8667\n",
      "Epoch 48, Train loss: 720.3276, Valid loss: 689.6788\n",
      "Epoch 49, Train loss: 720.1417, Valid loss: 689.5627\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 720.1417, minimum valid loss = 689.5627\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 92223.3388, Valid loss: 2231.9901\n",
      "Epoch 1, Train loss: 2246.798, Valid loss: 2149.8675\n",
      "Epoch 2, Train loss: 2132.1729, Valid loss: 2008.3915\n",
      "Epoch 3, Train loss: 1979.9008, Valid loss: 1851.6989\n",
      "Epoch 4, Train loss: 1820.9425, Valid loss: 1695.303\n",
      "Epoch 5, Train loss: 1667.3857, Valid loss: 1545.806\n",
      "Epoch 6, Train loss: 1522.6553, Valid loss: 1409.8174\n",
      "Epoch 7, Train loss: 1392.7146, Valid loss: 1288.0301\n",
      "Epoch 8, Train loss: 1276.8875, Valid loss: 1181.6398\n",
      "Epoch 9, Train loss: 1175.8039, Valid loss: 1090.9383\n",
      "Epoch 10, Train loss: 1090.7864, Valid loss: 1012.9935\n",
      "Epoch 11, Train loss: 1019.504, Valid loss: 948.1325\n",
      "Epoch 12, Train loss: 958.6661, Valid loss: 894.1847\n",
      "Epoch 13, Train loss: 908.4179, Valid loss: 850.625\n",
      "Epoch 14, Train loss: 868.4924, Valid loss: 815.438\n",
      "Epoch 15, Train loss: 835.7125, Valid loss: 788.0771\n",
      "Epoch 16, Train loss: 810.7262, Valid loss: 765.6347\n",
      "Epoch 17, Train loss: 799.3729, Valid loss: 748.6063\n",
      "Epoch 18, Train loss: 774.961, Valid loss: 734.9682\n",
      "Epoch 19, Train loss: 762.53, Valid loss: 724.4997\n",
      "Epoch 20, Train loss: 752.9188, Valid loss: 716.6208\n",
      "Epoch 21, Train loss: 745.6439, Valid loss: 710.5578\n",
      "Epoch 22, Train loss: 740.0349, Valid loss: 705.9451\n",
      "Epoch 23, Train loss: 737.1344, Valid loss: 702.2731\n",
      "Epoch 24, Train loss: 732.2069, Valid loss: 699.3472\n",
      "Epoch 25, Train loss: 729.5997, Valid loss: 697.3107\n",
      "Epoch 26, Train loss: 727.7258, Valid loss: 695.8316\n",
      "Epoch 27, Train loss: 726.3258, Valid loss: 694.6766\n",
      "Epoch 28, Train loss: 725.2479, Valid loss: 693.8646\n",
      "Epoch 29, Train loss: 724.4715, Valid loss: 693.2526\n",
      "Epoch 30, Train loss: 723.8692, Valid loss: 692.7272\n",
      "Epoch 31, Train loss: 723.3268, Valid loss: 692.273\n",
      "Epoch 32, Train loss: 722.9307, Valid loss: 692.0076\n",
      "Epoch 33, Train loss: 722.6761, Valid loss: 691.764\n",
      "Epoch 34, Train loss: 722.415, Valid loss: 691.5418\n",
      "Epoch 35, Train loss: 719.6407, Valid loss: 653.2585\n",
      "Epoch 36, Train loss: 718.6909, Valid loss: 690.6621\n",
      "Epoch 37, Train loss: 721.2028, Valid loss: 690.5298\n",
      "Epoch 38, Train loss: 721.1306, Valid loss: 690.5144\n",
      "Epoch 39, Train loss: 721.1772, Valid loss: 690.5974\n",
      "Epoch 40, Train loss: 721.2959, Valid loss: 690.7102\n",
      "Epoch 41, Train loss: 721.3549, Valid loss: 690.713\n",
      "Epoch 42, Train loss: 721.4503, Valid loss: 690.7885\n",
      "Epoch 43, Train loss: 721.4575, Valid loss: 690.8352\n",
      "Epoch 44, Train loss: 721.5748, Valid loss: 690.9581\n",
      "Epoch 45, Train loss: 721.5947, Valid loss: 690.9067\n",
      "Epoch 46, Train loss: 721.5274, Valid loss: 690.8363\n",
      "Epoch 47, Train loss: 721.6025, Valid loss: 691.0037\n",
      "Epoch 48, Train loss: 721.6457, Valid loss: 690.9465\n",
      "Epoch 49, Train loss: 721.6701, Valid loss: 691.0012\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 35, training loss = 719.6407, minimum valid loss = 653.2585\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 8377.6464, Valid loss: 1973.8737\n",
      "Epoch 1, Train loss: 2166.3285, Valid loss: 1619.3188\n",
      "Epoch 2, Train loss: 1603.7264, Valid loss: 1331.0473\n",
      "Epoch 3, Train loss: 1271.0864, Valid loss: 1131.4546\n",
      "Epoch 4, Train loss: 1104.2996, Valid loss: 1002.4624\n",
      "Epoch 5, Train loss: 988.8132, Valid loss: 925.0675\n",
      "Epoch 6, Train loss: 900.081, Valid loss: 819.4546\n",
      "Epoch 7, Train loss: 735.5666, Valid loss: 576.066\n",
      "Epoch 8, Train loss: 725.4802, Valid loss: 596.9841\n",
      "Epoch 9, Train loss: 871.3438, Valid loss: 638.9238\n",
      "Epoch 10, Train loss: 715.8528, Valid loss: 595.8554\n",
      "Epoch 11, Train loss: 772.4585, Valid loss: 578.2954\n",
      "Epoch 12, Train loss: 731.1342, Valid loss: 508.2777\n",
      "Epoch 13, Train loss: 733.964, Valid loss: 617.0969\n",
      "Epoch 14, Train loss: 735.1988, Valid loss: 623.3907\n",
      "Epoch 15, Train loss: 747.8568, Valid loss: 659.5075\n",
      "Epoch 16, Train loss: 796.2279, Valid loss: 699.964\n",
      "Epoch 17, Train loss: 820.5279, Valid loss: 877.7294\n",
      "Epoch 18, Train loss: 750.1368, Valid loss: 566.6703\n",
      "Epoch 19, Train loss: 725.939, Valid loss: 562.7098\n",
      "Epoch 20, Train loss: 721.1987, Valid loss: 610.7053\n",
      "Epoch 21, Train loss: 663.2671, Valid loss: 525.8042\n",
      "Epoch 22, Train loss: 650.2041, Valid loss: 440.1722\n",
      "Epoch 23, Train loss: 695.8132, Valid loss: 542.1466\n",
      "Epoch 24, Train loss: 1915.1685, Valid loss: 1115.8373\n",
      "Epoch 25, Train loss: 1120.7823, Valid loss: 991.8935\n",
      "Epoch 26, Train loss: 316253.4295, Valid loss: 847.2357\n",
      "Epoch 27, Train loss: 24752.8821, Valid loss: 741.6964\n",
      "Epoch 28, Train loss: 346099.7638, Valid loss: 751.8019\n",
      "Epoch 29, Train loss: 804.3414, Valid loss: 790.1177\n",
      "Epoch 30, Train loss: 915.3403, Valid loss: 814.8775\n",
      "Epoch 31, Train loss: 954.1378, Valid loss: 829.1086\n",
      "Epoch 32, Train loss: 867.8019, Valid loss: 837.805\n",
      "Epoch 33, Train loss: 874.7877, Valid loss: 842.8968\n",
      "Epoch 34, Train loss: 878.6751, Valid loss: 845.9538\n",
      "Epoch 35, Train loss: 879.4911, Valid loss: 847.6831\n",
      "Epoch 36, Train loss: 880.8384, Valid loss: 849.1695\n",
      "Epoch 37, Train loss: 883.3975, Valid loss: 850.3526\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 22, training loss = 650.2041, minimum valid loss = 440.1722\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 329044.8747, Valid loss: 2260.1935\n",
      "Epoch 1, Train loss: 3006.6305, Valid loss: 2224.106\n",
      "Epoch 2, Train loss: 2246.2222, Valid loss: 2058.0167\n",
      "Epoch 3, Train loss: 1741.7557, Valid loss: 890.1665\n",
      "Epoch 4, Train loss: 4373.6855, Valid loss: 1801.4766\n",
      "Epoch 5, Train loss: 2237.9597, Valid loss: 2010.0698\n",
      "Epoch 6, Train loss: 1943.9864, Valid loss: 1830.1908\n",
      "Epoch 7, Train loss: 1835.3541, Valid loss: 1751.2505\n",
      "Epoch 8, Train loss: 1761.6121, Valid loss: 1684.0078\n",
      "Epoch 9, Train loss: 1699.2859, Valid loss: 1628.5112\n",
      "Epoch 10, Train loss: 1647.579, Valid loss: 1583.5749\n",
      "Epoch 11, Train loss: 1606.9709, Valid loss: 1547.4052\n",
      "Epoch 12, Train loss: 1582.9382, Valid loss: 1519.0034\n",
      "Epoch 13, Train loss: 1548.5798, Valid loss: 1497.3828\n",
      "Epoch 14, Train loss: 1529.6679, Valid loss: 1481.4795\n",
      "Epoch 15, Train loss: 1515.5551, Valid loss: 1469.6456\n",
      "Epoch 16, Train loss: 1505.6529, Valid loss: 1461.78\n",
      "Epoch 17, Train loss: 1498.0973, Valid loss: 1455.0138\n",
      "Epoch 18, Train loss: 1492.3332, Valid loss: 1450.1314\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 3, training loss = 1741.7557, minimum valid loss = 890.1665\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 103199.6163, Valid loss: 2210.6632\n",
      "Epoch 1, Train loss: 2228.4657, Valid loss: 2101.1744\n",
      "Epoch 2, Train loss: 2061.2918, Valid loss: 1961.6394\n",
      "Epoch 3, Train loss: 1863.5646, Valid loss: 1847.5749\n",
      "Epoch 4, Train loss: 1807.5783, Valid loss: 1774.0846\n",
      "Epoch 5, Train loss: 1741.1162, Valid loss: 1733.6168\n",
      "Epoch 6, Train loss: 1667.346, Valid loss: 1723.5243\n",
      "Epoch 7, Train loss: 1708.85, Valid loss: 1723.1149\n",
      "Epoch 8, Train loss: 1676.7979, Valid loss: 1720.3959\n",
      "Epoch 9, Train loss: 1603.6233, Valid loss: 1721.1568\n",
      "Epoch 10, Train loss: 1566.932, Valid loss: 1698.6784\n",
      "Epoch 11, Train loss: 1470.3864, Valid loss: 1193.1827\n",
      "Epoch 12, Train loss: 1276.0569, Valid loss: 957.2609\n",
      "Epoch 13, Train loss: 1325.3589, Valid loss: 1119.1749\n",
      "Epoch 14, Train loss: 1341.7264, Valid loss: 1493.6648\n",
      "Epoch 15, Train loss: 1304.547, Valid loss: 1064.2251\n",
      "Epoch 16, Train loss: 1275.1195, Valid loss: 938.0103\n",
      "Epoch 17, Train loss: 1217.8273, Valid loss: 974.9356\n",
      "Epoch 18, Train loss: 1255.6792, Valid loss: 950.3155\n",
      "Epoch 19, Train loss: 1233.5852, Valid loss: 994.6\n",
      "Epoch 20, Train loss: 1219.3803, Valid loss: 886.3037\n",
      "Epoch 21, Train loss: 1266.0055, Valid loss: 1039.0018\n",
      "Epoch 22, Train loss: 1269.4183, Valid loss: 858.7559\n",
      "Epoch 23, Train loss: 1205.0408, Valid loss: 830.8718\n",
      "Epoch 24, Train loss: 1267.519, Valid loss: 760.7678\n",
      "Epoch 25, Train loss: 1247.2821, Valid loss: 853.8031\n",
      "Epoch 26, Train loss: 1237.7212, Valid loss: 958.0825\n",
      "Epoch 27, Train loss: 1165.4326, Valid loss: 841.7478\n",
      "Epoch 28, Train loss: 1180.8125, Valid loss: 854.0864\n",
      "Epoch 29, Train loss: 1178.4052, Valid loss: 751.0741\n",
      "Epoch 30, Train loss: 1150.7781, Valid loss: 821.129\n",
      "Epoch 31, Train loss: 1110.4503, Valid loss: 696.7276\n",
      "Epoch 32, Train loss: 1156.2887, Valid loss: 646.0231\n",
      "Epoch 33, Train loss: 1214.9416, Valid loss: 1145.3545\n",
      "Epoch 34, Train loss: 1217.0365, Valid loss: 960.3332\n",
      "Epoch 35, Train loss: 1153.8195, Valid loss: 671.1841\n",
      "Epoch 36, Train loss: 1225.3139, Valid loss: 1085.1148\n",
      "Epoch 37, Train loss: 1224.4756, Valid loss: 770.9583\n",
      "Epoch 38, Train loss: 1165.5761, Valid loss: 1102.5737\n",
      "Epoch 39, Train loss: 1180.4212, Valid loss: 625.0162\n",
      "Epoch 40, Train loss: 1155.4167, Valid loss: 484.3742\n",
      "Epoch 41, Train loss: 1196.2233, Valid loss: 986.6992\n",
      "Epoch 42, Train loss: 1293.3692, Valid loss: 1350.5884\n",
      "Epoch 43, Train loss: 1302.6405, Valid loss: 637.2129\n",
      "Epoch 44, Train loss: 1207.3939, Valid loss: 799.9584\n",
      "Epoch 45, Train loss: 1195.6204, Valid loss: 732.4679\n",
      "Epoch 46, Train loss: 1171.8415, Valid loss: 939.1363\n",
      "Epoch 47, Train loss: 1173.2019, Valid loss: 861.4015\n",
      "Epoch 48, Train loss: 1168.321, Valid loss: 748.4077\n",
      "Epoch 49, Train loss: 1155.6128, Valid loss: 600.7978\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 40, training loss = 1155.4167, minimum valid loss = 484.3742\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1735.2228, Valid loss: 1059.6294\n",
      "Epoch 1, Train loss: 1364.7585, Valid loss: 663.4147\n",
      "Epoch 2, Train loss: 1215.3836, Valid loss: 904.9702\n",
      "Epoch 3, Train loss: 1047.6573, Valid loss: 733.0784\n",
      "Epoch 4, Train loss: 998.6834, Valid loss: 527.1274\n",
      "Epoch 5, Train loss: 901.0464, Valid loss: 366.2495\n",
      "Epoch 6, Train loss: 837.9407, Valid loss: 470.1396\n",
      "Epoch 7, Train loss: 776.8604, Valid loss: 435.9985\n",
      "Epoch 8, Train loss: 767.0934, Valid loss: 378.6389\n",
      "Epoch 9, Train loss: 711.469, Valid loss: 408.8875\n",
      "Epoch 10, Train loss: 728.397, Valid loss: 336.5906\n",
      "Epoch 11, Train loss: 695.7991, Valid loss: 435.1815\n",
      "Epoch 12, Train loss: 685.5941, Valid loss: 298.5472\n",
      "Epoch 13, Train loss: 661.3752, Valid loss: 421.5706\n",
      "Epoch 14, Train loss: 639.8949, Valid loss: 394.1771\n",
      "Epoch 15, Train loss: 626.0492, Valid loss: 373.034\n",
      "Epoch 16, Train loss: 614.2233, Valid loss: 281.3452\n",
      "Epoch 17, Train loss: 608.9038, Valid loss: 272.8044\n",
      "Epoch 18, Train loss: 593.3782, Valid loss: 369.9624\n",
      "Epoch 19, Train loss: 555.2831, Valid loss: 289.0373\n",
      "Epoch 20, Train loss: 582.3948, Valid loss: 293.2797\n",
      "Epoch 21, Train loss: 568.2034, Valid loss: 287.6775\n",
      "Epoch 22, Train loss: 566.9711, Valid loss: 313.4482\n",
      "Epoch 23, Train loss: 538.7976, Valid loss: 270.8449\n",
      "Epoch 24, Train loss: 536.7914, Valid loss: 328.2818\n",
      "Epoch 25, Train loss: 556.0496, Valid loss: 356.3859\n",
      "Epoch 26, Train loss: 530.4094, Valid loss: 275.7356\n",
      "Epoch 27, Train loss: 513.9927, Valid loss: 307.2208\n",
      "Epoch 28, Train loss: 507.4714, Valid loss: 276.5997\n",
      "Epoch 29, Train loss: 502.7596, Valid loss: 256.0733\n",
      "Epoch 30, Train loss: 508.2347, Valid loss: 312.6337\n",
      "Epoch 31, Train loss: 504.2262, Valid loss: 229.5339\n",
      "Epoch 32, Train loss: 516.8238, Valid loss: 447.6565\n",
      "Epoch 33, Train loss: 513.4561, Valid loss: 282.4564\n",
      "Epoch 34, Train loss: 482.0322, Valid loss: 242.858\n",
      "Epoch 35, Train loss: 476.9051, Valid loss: 411.343\n",
      "Epoch 36, Train loss: 480.1313, Valid loss: 287.874\n",
      "Epoch 37, Train loss: 460.1372, Valid loss: 294.3722\n",
      "Epoch 38, Train loss: 461.2182, Valid loss: 290.8015\n",
      "Epoch 39, Train loss: 466.4132, Valid loss: 278.1156\n",
      "Epoch 40, Train loss: 461.506, Valid loss: 285.4349\n",
      "Epoch 41, Train loss: 462.0775, Valid loss: 244.3429\n",
      "Epoch 42, Train loss: 467.1506, Valid loss: 331.7746\n",
      "Epoch 43, Train loss: 454.4749, Valid loss: 278.9939\n",
      "Epoch 44, Train loss: 453.9833, Valid loss: 278.1847\n",
      "Epoch 45, Train loss: 461.9027, Valid loss: 349.9595\n",
      "Epoch 46, Train loss: 478.0933, Valid loss: 291.913\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 31, training loss = 504.2262, minimum valid loss = 229.5339\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1676.1918, Valid loss: 914.7301\n",
      "Epoch 1, Train loss: 1325.4685, Valid loss: 941.3538\n",
      "Epoch 2, Train loss: 1138.4431, Valid loss: 452.9023\n",
      "Epoch 3, Train loss: 920.8016, Valid loss: 407.1812\n",
      "Epoch 4, Train loss: 844.678, Valid loss: 308.2408\n",
      "Epoch 5, Train loss: 709.9627, Valid loss: 242.3184\n",
      "Epoch 6, Train loss: 698.5547, Valid loss: 264.603\n",
      "Epoch 7, Train loss: 644.4832, Valid loss: 397.5822\n",
      "Epoch 8, Train loss: 639.0787, Valid loss: 249.5029\n",
      "Epoch 9, Train loss: 625.7433, Valid loss: 280.5646\n",
      "Epoch 10, Train loss: 599.5239, Valid loss: 289.6271\n",
      "Epoch 11, Train loss: 575.6748, Valid loss: 308.9261\n",
      "Epoch 12, Train loss: 555.8269, Valid loss: 281.8219\n",
      "Epoch 13, Train loss: 525.5424, Valid loss: 263.2992\n",
      "Epoch 14, Train loss: 525.1019, Valid loss: 301.585\n",
      "Epoch 15, Train loss: 528.6912, Valid loss: 234.8792\n",
      "Epoch 16, Train loss: 514.6081, Valid loss: 401.1108\n",
      "Epoch 17, Train loss: 522.6694, Valid loss: 228.0652\n",
      "Epoch 18, Train loss: 504.7724, Valid loss: 240.8366\n",
      "Epoch 19, Train loss: 482.6813, Valid loss: 283.6095\n",
      "Epoch 20, Train loss: 507.2241, Valid loss: 292.9721\n",
      "Epoch 21, Train loss: 499.2248, Valid loss: 318.461\n",
      "Epoch 22, Train loss: 472.5034, Valid loss: 260.4543\n",
      "Epoch 23, Train loss: 456.5293, Valid loss: 210.2111\n",
      "Epoch 24, Train loss: 455.6779, Valid loss: 201.3138\n",
      "Epoch 25, Train loss: 479.8819, Valid loss: 240.4533\n",
      "Epoch 26, Train loss: 453.0108, Valid loss: 184.0097\n",
      "Epoch 27, Train loss: 484.4669, Valid loss: 364.8448\n",
      "Epoch 28, Train loss: 441.946, Valid loss: 232.8496\n",
      "Epoch 29, Train loss: 460.7184, Valid loss: 190.7043\n",
      "Epoch 30, Train loss: 448.2562, Valid loss: 271.3577\n",
      "Epoch 31, Train loss: 432.6253, Valid loss: 206.9977\n",
      "Epoch 32, Train loss: 420.9969, Valid loss: 212.8168\n",
      "Epoch 33, Train loss: 416.5758, Valid loss: 197.2096\n",
      "Epoch 34, Train loss: 455.6102, Valid loss: 225.7025\n",
      "Epoch 35, Train loss: 435.1425, Valid loss: 262.3003\n",
      "Epoch 36, Train loss: 435.4148, Valid loss: 304.9629\n",
      "Epoch 37, Train loss: 409.3996, Valid loss: 210.3317\n",
      "Epoch 38, Train loss: 388.5042, Valid loss: 290.8234\n",
      "Epoch 39, Train loss: 426.8414, Valid loss: 254.5571\n",
      "Epoch 40, Train loss: 404.9308, Valid loss: 268.117\n",
      "Epoch 41, Train loss: 413.5224, Valid loss: 264.5742\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 26, training loss = 453.0108, minimum valid loss = 184.0097\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1443.0226, Valid loss: 617.1593\n",
      "Epoch 1, Train loss: 1116.861, Valid loss: 855.3083\n",
      "Epoch 2, Train loss: 983.6432, Valid loss: 369.4833\n",
      "Epoch 3, Train loss: 735.92, Valid loss: 309.9903\n",
      "Epoch 4, Train loss: 624.9011, Valid loss: 243.8149\n",
      "Epoch 5, Train loss: 621.5535, Valid loss: 324.114\n",
      "Epoch 6, Train loss: 585.4481, Valid loss: 169.8426\n",
      "Epoch 7, Train loss: 637.5483, Valid loss: 263.959\n",
      "Epoch 8, Train loss: 530.8196, Valid loss: 264.6389\n",
      "Epoch 9, Train loss: 575.0645, Valid loss: 284.9113\n",
      "Epoch 10, Train loss: 548.0484, Valid loss: 341.3652\n",
      "Epoch 11, Train loss: 532.3946, Valid loss: 188.8202\n",
      "Epoch 12, Train loss: 524.491, Valid loss: 289.4789\n",
      "Epoch 13, Train loss: 507.0874, Valid loss: 226.7253\n",
      "Epoch 14, Train loss: 518.7021, Valid loss: 223.9402\n",
      "Epoch 15, Train loss: 511.6353, Valid loss: 225.2123\n",
      "Epoch 16, Train loss: 535.0392, Valid loss: 302.3142\n",
      "Epoch 17, Train loss: 511.7354, Valid loss: 305.8048\n",
      "Epoch 18, Train loss: 481.1174, Valid loss: 204.3741\n",
      "Epoch 19, Train loss: 521.3973, Valid loss: 239.2853\n",
      "Epoch 20, Train loss: 507.4727, Valid loss: 276.737\n",
      "Epoch 21, Train loss: 489.8662, Valid loss: 296.9635\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 585.4481, minimum valid loss = 169.8426\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1641.3922, Valid loss: 469.6125\n",
      "Epoch 1, Train loss: 1152.8876, Valid loss: 332.3102\n",
      "Epoch 2, Train loss: 862.7489, Valid loss: 400.4097\n",
      "Epoch 3, Train loss: 802.0941, Valid loss: 407.9215\n",
      "Epoch 4, Train loss: 762.172, Valid loss: 233.4955\n",
      "Epoch 5, Train loss: 739.0443, Valid loss: 476.1131\n",
      "Epoch 6, Train loss: 722.9773, Valid loss: 279.8514\n",
      "Epoch 7, Train loss: 691.3006, Valid loss: 247.4754\n",
      "Epoch 8, Train loss: 646.131, Valid loss: 258.183\n",
      "Epoch 9, Train loss: 650.5184, Valid loss: 255.6704\n",
      "Epoch 10, Train loss: 650.1513, Valid loss: 377.0719\n",
      "Epoch 11, Train loss: 590.5382, Valid loss: 363.779\n",
      "Epoch 12, Train loss: 636.8064, Valid loss: 246.282\n",
      "Epoch 13, Train loss: 639.6363, Valid loss: 266.0091\n",
      "Epoch 14, Train loss: 632.9049, Valid loss: 398.6719\n",
      "Epoch 15, Train loss: 620.3203, Valid loss: 282.605\n",
      "Epoch 16, Train loss: 620.8105, Valid loss: 240.73\n",
      "Epoch 17, Train loss: 680.6594, Valid loss: 440.0985\n",
      "Epoch 18, Train loss: 667.3858, Valid loss: 476.2278\n",
      "Epoch 19, Train loss: 620.1262, Valid loss: 314.7508\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 4, training loss = 762.172, minimum valid loss = 233.4955\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1611.2087, Valid loss: 813.5746\n",
      "Epoch 1, Train loss: 1143.0934, Valid loss: 376.2682\n",
      "Epoch 2, Train loss: 815.5012, Valid loss: 498.4544\n",
      "Epoch 3, Train loss: 684.9344, Valid loss: 371.7675\n",
      "Epoch 4, Train loss: 721.4056, Valid loss: 326.1165\n",
      "Epoch 5, Train loss: 665.6799, Valid loss: 331.3569\n",
      "Epoch 6, Train loss: 666.6999, Valid loss: 289.9715\n",
      "Epoch 7, Train loss: 682.4245, Valid loss: 356.4997\n",
      "Epoch 8, Train loss: 694.007, Valid loss: 361.5057\n",
      "Epoch 9, Train loss: 670.3071, Valid loss: 362.0584\n",
      "Epoch 10, Train loss: 657.7937, Valid loss: 260.4771\n",
      "Epoch 11, Train loss: 661.7752, Valid loss: 444.5869\n",
      "Epoch 12, Train loss: 664.0786, Valid loss: 311.2967\n",
      "Epoch 13, Train loss: 674.7702, Valid loss: 489.1787\n",
      "Epoch 14, Train loss: 647.7094, Valid loss: 305.983\n",
      "Epoch 15, Train loss: 669.6172, Valid loss: 368.051\n",
      "Epoch 16, Train loss: 634.0067, Valid loss: 450.6978\n",
      "Epoch 17, Train loss: 663.7453, Valid loss: 263.6586\n",
      "Epoch 18, Train loss: 667.0779, Valid loss: 598.3769\n",
      "Epoch 19, Train loss: 705.7505, Valid loss: 259.9538\n",
      "Epoch 20, Train loss: 670.1838, Valid loss: 266.088\n",
      "Epoch 21, Train loss: 671.1241, Valid loss: 407.3368\n",
      "Epoch 22, Train loss: 682.274, Valid loss: 238.1317\n",
      "Epoch 23, Train loss: 734.494, Valid loss: 511.0258\n",
      "Epoch 24, Train loss: 704.593, Valid loss: 269.9134\n",
      "Epoch 25, Train loss: 709.4626, Valid loss: 412.7207\n",
      "Epoch 26, Train loss: 666.447, Valid loss: 207.9412\n",
      "Epoch 27, Train loss: 651.1638, Valid loss: 323.9824\n",
      "Epoch 28, Train loss: 684.6989, Valid loss: 379.0282\n",
      "Epoch 29, Train loss: 648.3333, Valid loss: 355.1922\n",
      "Epoch 30, Train loss: 645.8156, Valid loss: 276.7228\n",
      "Epoch 31, Train loss: 658.6385, Valid loss: 340.6468\n",
      "Epoch 32, Train loss: 687.0477, Valid loss: 419.3894\n",
      "Epoch 33, Train loss: 833.8707, Valid loss: 597.8212\n",
      "Epoch 34, Train loss: 709.3568, Valid loss: 262.1539\n",
      "Epoch 35, Train loss: 650.2866, Valid loss: 299.9648\n",
      "Epoch 36, Train loss: 665.1613, Valid loss: 298.7614\n",
      "Epoch 37, Train loss: 692.3606, Valid loss: 327.2097\n",
      "Epoch 38, Train loss: 658.6262, Valid loss: 364.769\n",
      "Epoch 39, Train loss: 639.3361, Valid loss: 279.5687\n",
      "Epoch 40, Train loss: 684.2527, Valid loss: 241.8114\n",
      "Epoch 41, Train loss: 657.6263, Valid loss: 345.0136\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 26, training loss = 666.447, minimum valid loss = 207.9412\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1711.774, Valid loss: 742.7759\n",
      "Epoch 1, Train loss: 1176.4624, Valid loss: 741.2055\n",
      "Epoch 2, Train loss: 1087.0716, Valid loss: 715.1856\n",
      "Epoch 3, Train loss: 984.8753, Valid loss: 436.6476\n",
      "Epoch 4, Train loss: 966.6455, Valid loss: 621.2559\n",
      "Epoch 5, Train loss: 934.697, Valid loss: 435.6812\n",
      "Epoch 6, Train loss: 918.5503, Valid loss: 576.5019\n",
      "Epoch 7, Train loss: 863.3288, Valid loss: 449.4647\n",
      "Epoch 8, Train loss: 849.9483, Valid loss: 407.0465\n",
      "Epoch 9, Train loss: 791.6142, Valid loss: 388.2546\n",
      "Epoch 10, Train loss: 797.4941, Valid loss: 502.6537\n",
      "Epoch 11, Train loss: 791.4807, Valid loss: 440.8646\n",
      "Epoch 12, Train loss: 780.1597, Valid loss: 387.2676\n",
      "Epoch 13, Train loss: 755.7588, Valid loss: 352.8842\n",
      "Epoch 14, Train loss: 733.6627, Valid loss: 356.3004\n",
      "Epoch 15, Train loss: 712.9342, Valid loss: 384.08\n",
      "Epoch 16, Train loss: 726.4465, Valid loss: 369.2169\n",
      "Epoch 17, Train loss: 695.9049, Valid loss: 437.8852\n",
      "Epoch 18, Train loss: 696.8922, Valid loss: 277.7009\n",
      "Epoch 19, Train loss: 682.0345, Valid loss: 391.0675\n",
      "Epoch 20, Train loss: 663.2106, Valid loss: 328.2143\n",
      "Epoch 21, Train loss: 674.3151, Valid loss: 281.1752\n",
      "Epoch 22, Train loss: 672.0713, Valid loss: 379.824\n",
      "Epoch 23, Train loss: 668.931, Valid loss: 301.8749\n",
      "Epoch 24, Train loss: 634.8017, Valid loss: 345.1257\n",
      "Epoch 25, Train loss: 644.6305, Valid loss: 290.1642\n",
      "Epoch 26, Train loss: 615.7448, Valid loss: 396.1478\n",
      "Epoch 27, Train loss: 608.2147, Valid loss: 302.8223\n",
      "Epoch 28, Train loss: 607.0823, Valid loss: 306.9493\n",
      "Epoch 29, Train loss: 612.8152, Valid loss: 311.9516\n",
      "Epoch 30, Train loss: 606.7296, Valid loss: 401.0173\n",
      "Epoch 31, Train loss: 585.177, Valid loss: 288.0081\n",
      "Epoch 32, Train loss: 585.9314, Valid loss: 355.8609\n",
      "Epoch 33, Train loss: 571.0304, Valid loss: 367.5613\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 18, training loss = 696.8922, minimum valid loss = 277.7009\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1182.545, Valid loss: 495.5459\n",
      "Epoch 1, Train loss: 810.5316, Valid loss: 470.3565\n",
      "Epoch 2, Train loss: 762.1585, Valid loss: 286.1428\n",
      "Epoch 3, Train loss: 711.056, Valid loss: 376.4201\n",
      "Epoch 4, Train loss: 668.0289, Valid loss: 259.1686\n",
      "Epoch 5, Train loss: 685.1912, Valid loss: 262.5316\n",
      "Epoch 6, Train loss: 612.6075, Valid loss: 257.9683\n",
      "Epoch 7, Train loss: 622.9533, Valid loss: 254.3813\n",
      "Epoch 8, Train loss: 622.1964, Valid loss: 370.8609\n",
      "Epoch 9, Train loss: 595.8291, Valid loss: 176.4093\n",
      "Epoch 10, Train loss: 593.6438, Valid loss: 441.4496\n",
      "Epoch 11, Train loss: 598.6518, Valid loss: 191.0111\n",
      "Epoch 12, Train loss: 559.6847, Valid loss: 198.1527\n",
      "Epoch 13, Train loss: 530.7131, Valid loss: 398.7854\n",
      "Epoch 14, Train loss: 509.8071, Valid loss: 182.0211\n",
      "Epoch 15, Train loss: 509.6929, Valid loss: 224.5066\n",
      "Epoch 16, Train loss: 505.1558, Valid loss: 220.0239\n",
      "Epoch 17, Train loss: 548.7022, Valid loss: 265.4241\n",
      "Epoch 18, Train loss: 508.5158, Valid loss: 291.3143\n",
      "Epoch 19, Train loss: 499.6065, Valid loss: 200.8011\n",
      "Epoch 20, Train loss: 487.9174, Valid loss: 190.1176\n",
      "Epoch 21, Train loss: 479.0354, Valid loss: 265.9712\n",
      "Epoch 22, Train loss: 483.123, Valid loss: 167.908\n",
      "Epoch 23, Train loss: 472.6, Valid loss: 286.5044\n",
      "Epoch 24, Train loss: 486.3242, Valid loss: 174.1829\n",
      "Epoch 25, Train loss: 484.6826, Valid loss: 296.6176\n",
      "Epoch 26, Train loss: 476.2232, Valid loss: 216.6873\n",
      "Epoch 27, Train loss: 423.1502, Valid loss: 148.4122\n",
      "Epoch 28, Train loss: 446.0807, Valid loss: 254.8991\n",
      "Epoch 29, Train loss: 433.8044, Valid loss: 191.3245\n",
      "Epoch 30, Train loss: 445.4639, Valid loss: 205.5715\n",
      "Epoch 31, Train loss: 415.8349, Valid loss: 157.5246\n",
      "Epoch 32, Train loss: 413.1753, Valid loss: 238.692\n",
      "Epoch 33, Train loss: 431.4235, Valid loss: 259.463\n",
      "Epoch 34, Train loss: 428.4929, Valid loss: 164.0258\n",
      "Epoch 35, Train loss: 423.3828, Valid loss: 243.6781\n",
      "Epoch 36, Train loss: 412.9079, Valid loss: 304.7782\n",
      "Epoch 37, Train loss: 436.5376, Valid loss: 168.5633\n",
      "Epoch 38, Train loss: 396.1307, Valid loss: 183.8706\n",
      "Epoch 39, Train loss: 410.4174, Valid loss: 203.144\n",
      "Epoch 40, Train loss: 394.4021, Valid loss: 155.9808\n",
      "Epoch 41, Train loss: 421.3822, Valid loss: 170.7366\n",
      "Epoch 42, Train loss: 389.6857, Valid loss: 209.2863\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 27, training loss = 423.1502, minimum valid loss = 148.4122\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1860.3009, Valid loss: 1070.846\n",
      "Epoch 1, Train loss: 1328.5246, Valid loss: 767.2239\n",
      "Epoch 2, Train loss: 1020.3549, Valid loss: 450.0973\n",
      "Epoch 3, Train loss: 765.6912, Valid loss: 235.2531\n",
      "Epoch 4, Train loss: 717.0264, Valid loss: 354.1186\n",
      "Epoch 5, Train loss: 650.5149, Valid loss: 345.2095\n",
      "Epoch 6, Train loss: 652.4198, Valid loss: 317.875\n",
      "Epoch 7, Train loss: 654.6096, Valid loss: 368.153\n",
      "Epoch 8, Train loss: 625.2114, Valid loss: 237.8176\n",
      "Epoch 9, Train loss: 607.1344, Valid loss: 278.114\n",
      "Epoch 10, Train loss: 593.9573, Valid loss: 229.3603\n",
      "Epoch 11, Train loss: 588.74, Valid loss: 331.2884\n",
      "Epoch 12, Train loss: 571.0871, Valid loss: 295.7969\n",
      "Epoch 13, Train loss: 563.5244, Valid loss: 225.0708\n",
      "Epoch 14, Train loss: 586.5266, Valid loss: 288.2324\n",
      "Epoch 15, Train loss: 564.083, Valid loss: 227.3799\n",
      "Epoch 16, Train loss: 576.2787, Valid loss: 379.8748\n",
      "Epoch 17, Train loss: 564.194, Valid loss: 380.6033\n",
      "Epoch 18, Train loss: 540.2523, Valid loss: 194.3203\n",
      "Epoch 19, Train loss: 541.7837, Valid loss: 207.0213\n",
      "Epoch 20, Train loss: 528.6354, Valid loss: 337.4155\n",
      "Epoch 21, Train loss: 537.7293, Valid loss: 323.3491\n",
      "Epoch 22, Train loss: 514.4993, Valid loss: 256.7231\n",
      "Epoch 23, Train loss: 545.701, Valid loss: 301.6211\n",
      "Epoch 24, Train loss: 519.8709, Valid loss: 229.5133\n",
      "Epoch 25, Train loss: 549.0402, Valid loss: 252.876\n",
      "Epoch 26, Train loss: 500.9813, Valid loss: 187.1876\n",
      "Epoch 27, Train loss: 484.1717, Valid loss: 201.14\n",
      "Epoch 28, Train loss: 491.7139, Valid loss: 220.1875\n",
      "Epoch 29, Train loss: 486.3609, Valid loss: 223.064\n",
      "Epoch 30, Train loss: 481.3754, Valid loss: 270.5818\n",
      "Epoch 31, Train loss: 481.0796, Valid loss: 223.466\n",
      "Epoch 32, Train loss: 488.3405, Valid loss: 203.3291\n",
      "Epoch 33, Train loss: 492.1877, Valid loss: 225.0565\n",
      "Epoch 34, Train loss: 512.8952, Valid loss: 192.0621\n",
      "Epoch 35, Train loss: 494.8665, Valid loss: 240.7509\n",
      "Epoch 36, Train loss: 471.6866, Valid loss: 238.3572\n",
      "Epoch 37, Train loss: 461.237, Valid loss: 216.9187\n",
      "Epoch 38, Train loss: 494.148, Valid loss: 223.7679\n",
      "Epoch 39, Train loss: 465.2641, Valid loss: 189.2193\n",
      "Epoch 40, Train loss: 493.4909, Valid loss: 275.4468\n",
      "Epoch 41, Train loss: 472.5591, Valid loss: 296.4883\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 26, training loss = 500.9813, minimum valid loss = 187.1876\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1298.4769, Valid loss: 542.4471\n",
      "Epoch 1, Train loss: 885.7704, Valid loss: 420.0712\n",
      "Epoch 2, Train loss: 785.9497, Valid loss: 249.4807\n",
      "Epoch 3, Train loss: 712.2441, Valid loss: 245.9021\n",
      "Epoch 4, Train loss: 663.5211, Valid loss: 210.0241\n",
      "Epoch 5, Train loss: 653.791, Valid loss: 304.6944\n",
      "Epoch 6, Train loss: 617.0094, Valid loss: 236.2163\n",
      "Epoch 7, Train loss: 638.6784, Valid loss: 354.4677\n",
      "Epoch 8, Train loss: 639.0162, Valid loss: 372.9322\n",
      "Epoch 9, Train loss: 610.9465, Valid loss: 256.191\n",
      "Epoch 10, Train loss: 570.1275, Valid loss: 292.7906\n",
      "Epoch 11, Train loss: 619.0735, Valid loss: 364.1135\n",
      "Epoch 12, Train loss: 625.0385, Valid loss: 307.465\n",
      "Epoch 13, Train loss: 611.1477, Valid loss: 276.1895\n",
      "Epoch 14, Train loss: 612.0131, Valid loss: 318.9069\n",
      "Epoch 15, Train loss: 610.1286, Valid loss: 325.2133\n",
      "Epoch 16, Train loss: 618.8752, Valid loss: 303.6466\n",
      "Epoch 17, Train loss: 608.824, Valid loss: 261.9553\n",
      "Epoch 18, Train loss: 627.575, Valid loss: 314.3473\n",
      "Epoch 19, Train loss: 606.202, Valid loss: 360.2433\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 4, training loss = 663.5211, minimum valid loss = 210.0241\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1292.8127, Valid loss: 728.2337\n",
      "Epoch 1, Train loss: 889.2393, Valid loss: 623.4248\n",
      "Epoch 2, Train loss: 823.6029, Valid loss: 338.4134\n",
      "Epoch 3, Train loss: 708.0447, Valid loss: 278.0749\n",
      "Epoch 4, Train loss: 674.1398, Valid loss: 380.0074\n",
      "Epoch 5, Train loss: 657.834, Valid loss: 349.1556\n",
      "Epoch 6, Train loss: 663.7708, Valid loss: 351.0497\n",
      "Epoch 7, Train loss: 642.731, Valid loss: 248.6414\n",
      "Epoch 8, Train loss: 640.6744, Valid loss: 237.8614\n",
      "Epoch 9, Train loss: 708.5217, Valid loss: 504.3575\n",
      "Epoch 10, Train loss: 699.7856, Valid loss: 492.2746\n",
      "Epoch 11, Train loss: 664.816, Valid loss: 276.7994\n",
      "Epoch 12, Train loss: 657.2352, Valid loss: 351.0895\n",
      "Epoch 13, Train loss: 680.9687, Valid loss: 234.1982\n",
      "Epoch 14, Train loss: 639.1925, Valid loss: 317.7506\n",
      "Epoch 15, Train loss: 641.6846, Valid loss: 249.4637\n",
      "Epoch 16, Train loss: 650.7319, Valid loss: 245.418\n",
      "Epoch 17, Train loss: 679.2944, Valid loss: 485.3696\n",
      "Epoch 18, Train loss: 686.1407, Valid loss: 280.4502\n",
      "Epoch 19, Train loss: 661.184, Valid loss: 246.2776\n",
      "Epoch 20, Train loss: 649.7966, Valid loss: 283.5026\n",
      "Epoch 21, Train loss: 665.2971, Valid loss: 359.6131\n",
      "Epoch 22, Train loss: 654.4606, Valid loss: 261.0123\n",
      "Epoch 23, Train loss: 650.0068, Valid loss: 327.6326\n",
      "Epoch 24, Train loss: 680.8674, Valid loss: 328.6013\n",
      "Epoch 25, Train loss: 639.1668, Valid loss: 290.8266\n",
      "Epoch 26, Train loss: 630.9452, Valid loss: 234.035\n",
      "Epoch 27, Train loss: 663.6549, Valid loss: 288.9138\n",
      "Epoch 28, Train loss: 666.1277, Valid loss: 521.3109\n",
      "Epoch 29, Train loss: 664.5238, Valid loss: 251.3529\n",
      "Epoch 30, Train loss: 598.3879, Valid loss: 401.5661\n",
      "Epoch 31, Train loss: 615.615, Valid loss: 297.6402\n",
      "Epoch 32, Train loss: 645.5202, Valid loss: 368.8876\n",
      "Epoch 33, Train loss: 629.1896, Valid loss: 245.0228\n",
      "Epoch 34, Train loss: 641.242, Valid loss: 243.1436\n",
      "Epoch 35, Train loss: 666.1376, Valid loss: 264.5252\n",
      "Epoch 36, Train loss: 618.8408, Valid loss: 227.006\n",
      "Epoch 37, Train loss: 632.6148, Valid loss: 315.6522\n",
      "Epoch 38, Train loss: 639.5043, Valid loss: 292.4483\n",
      "Epoch 39, Train loss: 660.8151, Valid loss: 321.5853\n",
      "Epoch 40, Train loss: 617.0559, Valid loss: 255.5194\n",
      "Epoch 41, Train loss: 669.7947, Valid loss: 260.0965\n",
      "Epoch 42, Train loss: 639.6971, Valid loss: 337.2311\n",
      "Epoch 43, Train loss: 665.5187, Valid loss: 252.662\n",
      "Epoch 44, Train loss: 608.3348, Valid loss: 200.195\n",
      "Epoch 45, Train loss: 642.6796, Valid loss: 257.0799\n",
      "Epoch 46, Train loss: 675.9567, Valid loss: 308.0866\n",
      "Epoch 47, Train loss: 604.0268, Valid loss: 314.4797\n",
      "Epoch 48, Train loss: 636.4802, Valid loss: 275.4856\n",
      "Epoch 49, Train loss: 663.8983, Valid loss: 327.6042\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 44, training loss = 608.3348, minimum valid loss = 200.195\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1793.1975, Valid loss: 751.6243\n",
      "Epoch 1, Train loss: 1274.652, Valid loss: 685.9395\n",
      "Epoch 2, Train loss: 1062.0642, Valid loss: 635.7501\n",
      "Epoch 3, Train loss: 983.8222, Valid loss: 446.453\n",
      "Epoch 4, Train loss: 905.0387, Valid loss: 391.5385\n",
      "Epoch 5, Train loss: 812.0879, Valid loss: 366.3978\n",
      "Epoch 6, Train loss: 807.9048, Valid loss: 336.2024\n",
      "Epoch 7, Train loss: 789.1201, Valid loss: 317.9218\n",
      "Epoch 8, Train loss: 793.9264, Valid loss: 336.4042\n",
      "Epoch 9, Train loss: 796.1847, Valid loss: 420.6225\n",
      "Epoch 10, Train loss: 764.0452, Valid loss: 394.1249\n",
      "Epoch 11, Train loss: 758.7115, Valid loss: 315.2717\n",
      "Epoch 12, Train loss: 790.2273, Valid loss: 287.7299\n",
      "Epoch 13, Train loss: 771.8221, Valid loss: 310.6477\n",
      "Epoch 14, Train loss: 774.7071, Valid loss: 342.5157\n",
      "Epoch 15, Train loss: 780.5331, Valid loss: 343.7947\n",
      "Epoch 16, Train loss: 746.8411, Valid loss: 283.0141\n",
      "Epoch 17, Train loss: 755.784, Valid loss: 302.0571\n",
      "Epoch 18, Train loss: 740.7245, Valid loss: 286.0515\n",
      "Epoch 19, Train loss: 724.5042, Valid loss: 338.1553\n",
      "Epoch 20, Train loss: 718.8285, Valid loss: 313.9284\n",
      "Epoch 21, Train loss: 741.4744, Valid loss: 332.7636\n",
      "Epoch 22, Train loss: 711.6295, Valid loss: 272.2897\n",
      "Epoch 23, Train loss: 741.1736, Valid loss: 301.5522\n",
      "Epoch 24, Train loss: 715.5724, Valid loss: 300.7575\n",
      "Epoch 25, Train loss: 702.695, Valid loss: 289.0167\n",
      "Epoch 26, Train loss: 708.4164, Valid loss: 248.0156\n",
      "Epoch 27, Train loss: 679.6611, Valid loss: 273.8828\n",
      "Epoch 28, Train loss: 676.4046, Valid loss: 300.7605\n",
      "Epoch 29, Train loss: 683.1339, Valid loss: 297.2137\n",
      "Epoch 30, Train loss: 679.6154, Valid loss: 299.992\n",
      "Epoch 31, Train loss: 674.7845, Valid loss: 286.099\n",
      "Epoch 32, Train loss: 695.1921, Valid loss: 293.0084\n",
      "Epoch 33, Train loss: 696.6398, Valid loss: 337.2729\n",
      "Epoch 34, Train loss: 686.5475, Valid loss: 367.6912\n",
      "Epoch 35, Train loss: 670.0651, Valid loss: 340.3508\n",
      "Epoch 36, Train loss: 665.8172, Valid loss: 275.8298\n",
      "Epoch 37, Train loss: 669.9695, Valid loss: 243.5578\n",
      "Epoch 38, Train loss: 647.5409, Valid loss: 313.5068\n",
      "Epoch 39, Train loss: 683.1646, Valid loss: 265.2921\n",
      "Epoch 40, Train loss: 660.3221, Valid loss: 302.7524\n",
      "Epoch 41, Train loss: 640.1115, Valid loss: 263.4644\n",
      "Epoch 42, Train loss: 663.6202, Valid loss: 260.7208\n",
      "Epoch 43, Train loss: 652.7651, Valid loss: 243.764\n",
      "Epoch 44, Train loss: 640.6286, Valid loss: 259.4514\n",
      "Epoch 45, Train loss: 627.68, Valid loss: 316.6267\n",
      "Epoch 46, Train loss: 620.2779, Valid loss: 273.1325\n",
      "Epoch 47, Train loss: 609.1808, Valid loss: 253.8666\n",
      "Epoch 48, Train loss: 618.5572, Valid loss: 275.9356\n",
      "Epoch 49, Train loss: 632.4016, Valid loss: 294.3775\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 37, training loss = 669.9695, minimum valid loss = 243.5578\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2042.5937, Valid loss: 1485.4919\n",
      "Epoch 1, Train loss: 1551.4319, Valid loss: 956.6982\n",
      "Epoch 2, Train loss: 1266.1623, Valid loss: 657.7007\n",
      "Epoch 3, Train loss: 1194.2327, Valid loss: 709.914\n",
      "Epoch 4, Train loss: 1105.4096, Valid loss: 601.4618\n",
      "Epoch 5, Train loss: 1034.8987, Valid loss: 499.5037\n",
      "Epoch 6, Train loss: 983.7921, Valid loss: 424.2323\n",
      "Epoch 7, Train loss: 913.0665, Valid loss: 447.9391\n",
      "Epoch 8, Train loss: 872.7814, Valid loss: 393.5979\n",
      "Epoch 9, Train loss: 871.1838, Valid loss: 327.9914\n",
      "Epoch 10, Train loss: 848.2919, Valid loss: 336.2968\n",
      "Epoch 11, Train loss: 830.5657, Valid loss: 410.9806\n",
      "Epoch 12, Train loss: 805.1596, Valid loss: 410.7143\n",
      "Epoch 13, Train loss: 799.9197, Valid loss: 390.8406\n",
      "Epoch 14, Train loss: 808.9412, Valid loss: 413.6161\n",
      "Epoch 15, Train loss: 802.0354, Valid loss: 388.8228\n",
      "Epoch 16, Train loss: 819.668, Valid loss: 355.5525\n",
      "Epoch 17, Train loss: 833.331, Valid loss: 344.5743\n",
      "Epoch 18, Train loss: 792.4943, Valid loss: 365.5173\n",
      "Epoch 19, Train loss: 773.8993, Valid loss: 423.0851\n",
      "Epoch 20, Train loss: 767.4147, Valid loss: 342.9336\n",
      "Epoch 21, Train loss: 777.0542, Valid loss: 327.1085\n",
      "Epoch 22, Train loss: 781.8781, Valid loss: 368.8216\n",
      "Epoch 23, Train loss: 735.7066, Valid loss: 452.7965\n",
      "Epoch 24, Train loss: 770.5251, Valid loss: 399.6441\n",
      "Epoch 25, Train loss: 778.6979, Valid loss: 319.6739\n",
      "Epoch 26, Train loss: 726.9029, Valid loss: 383.0967\n",
      "Epoch 27, Train loss: 712.6252, Valid loss: 260.5865\n",
      "Epoch 28, Train loss: 716.3213, Valid loss: 311.9359\n",
      "Epoch 29, Train loss: 698.1373, Valid loss: 353.0635\n",
      "Epoch 30, Train loss: 654.1691, Valid loss: 343.3145\n",
      "Epoch 31, Train loss: 682.8998, Valid loss: 347.7591\n",
      "Epoch 32, Train loss: 684.0798, Valid loss: 268.3082\n",
      "Epoch 33, Train loss: 662.937, Valid loss: 301.172\n",
      "Epoch 34, Train loss: 654.8647, Valid loss: 272.539\n",
      "Epoch 35, Train loss: 656.9726, Valid loss: 233.6247\n",
      "Epoch 36, Train loss: 614.9139, Valid loss: 260.4438\n",
      "Epoch 37, Train loss: 593.8211, Valid loss: 201.2019\n",
      "Epoch 38, Train loss: 615.7719, Valid loss: 244.6842\n",
      "Epoch 39, Train loss: 586.1267, Valid loss: 312.4618\n",
      "Epoch 40, Train loss: 549.6144, Valid loss: 262.4049\n",
      "Epoch 41, Train loss: 576.9325, Valid loss: 209.5663\n",
      "Epoch 42, Train loss: 585.5807, Valid loss: 215.0245\n",
      "Epoch 43, Train loss: 560.1934, Valid loss: 219.0102\n",
      "Epoch 44, Train loss: 564.249, Valid loss: 228.8361\n",
      "Epoch 45, Train loss: 561.659, Valid loss: 199.1738\n",
      "Epoch 46, Train loss: 567.6503, Valid loss: 198.7757\n",
      "Epoch 47, Train loss: 556.642, Valid loss: 294.8276\n",
      "Epoch 48, Train loss: 572.3751, Valid loss: 247.7319\n",
      "Epoch 49, Train loss: 540.1573, Valid loss: 283.8129\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 46, training loss = 567.6503, minimum valid loss = 198.7757\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2027.5241, Valid loss: 1401.3547\n",
      "Epoch 1, Train loss: 1427.8684, Valid loss: 865.8396\n",
      "Epoch 2, Train loss: 1232.3695, Valid loss: 567.4309\n",
      "Epoch 3, Train loss: 1088.6281, Valid loss: 613.1948\n",
      "Epoch 4, Train loss: 1042.4056, Valid loss: 566.8619\n",
      "Epoch 5, Train loss: 1004.4371, Valid loss: 594.7764\n",
      "Epoch 6, Train loss: 982.0656, Valid loss: 481.0218\n",
      "Epoch 7, Train loss: 961.9406, Valid loss: 455.5885\n",
      "Epoch 8, Train loss: 973.0628, Valid loss: 503.8463\n",
      "Epoch 9, Train loss: 917.4744, Valid loss: 479.9159\n",
      "Epoch 10, Train loss: 948.0802, Valid loss: 482.9845\n",
      "Epoch 11, Train loss: 920.3992, Valid loss: 508.3022\n",
      "Epoch 12, Train loss: 902.4032, Valid loss: 447.3733\n",
      "Epoch 13, Train loss: 900.1565, Valid loss: 403.4832\n",
      "Epoch 14, Train loss: 871.1278, Valid loss: 402.8179\n",
      "Epoch 15, Train loss: 879.8883, Valid loss: 460.9559\n",
      "Epoch 16, Train loss: 864.9564, Valid loss: 469.8768\n",
      "Epoch 17, Train loss: 886.0803, Valid loss: 502.7429\n",
      "Epoch 18, Train loss: 892.6793, Valid loss: 446.5621\n",
      "Epoch 19, Train loss: 879.7135, Valid loss: 418.7158\n",
      "Epoch 20, Train loss: 874.389, Valid loss: 385.7945\n",
      "Epoch 21, Train loss: 855.2108, Valid loss: 315.2293\n",
      "Epoch 22, Train loss: 881.7615, Valid loss: 366.8956\n",
      "Epoch 23, Train loss: 854.1243, Valid loss: 351.3488\n",
      "Epoch 24, Train loss: 849.6259, Valid loss: 394.3109\n",
      "Epoch 25, Train loss: 826.4491, Valid loss: 484.2119\n",
      "Epoch 26, Train loss: 835.5207, Valid loss: 389.3245\n",
      "Epoch 27, Train loss: 838.4508, Valid loss: 440.6027\n",
      "Epoch 28, Train loss: 840.4428, Valid loss: 473.6256\n",
      "Epoch 29, Train loss: 826.7207, Valid loss: 408.4413\n",
      "Epoch 30, Train loss: 829.142, Valid loss: 441.3318\n",
      "Epoch 31, Train loss: 844.8449, Valid loss: 440.7005\n",
      "Epoch 32, Train loss: 841.0839, Valid loss: 362.4144\n",
      "Epoch 33, Train loss: 863.5897, Valid loss: 397.3562\n",
      "Epoch 34, Train loss: 807.2327, Valid loss: 469.7433\n",
      "Epoch 35, Train loss: 812.8317, Valid loss: 383.8747\n",
      "Epoch 36, Train loss: 805.9021, Valid loss: 331.5525\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 21, training loss = 855.2108, minimum valid loss = 315.2293\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1829.4734, Valid loss: 863.7949\n",
      "Epoch 1, Train loss: 1333.9589, Valid loss: 801.9211\n",
      "Epoch 2, Train loss: 1092.3556, Valid loss: 618.2723\n",
      "Epoch 3, Train loss: 1035.2185, Valid loss: 529.1734\n",
      "Epoch 4, Train loss: 1018.0828, Valid loss: 576.6964\n",
      "Epoch 5, Train loss: 977.4547, Valid loss: 510.5641\n",
      "Epoch 6, Train loss: 961.9895, Valid loss: 540.2432\n",
      "Epoch 7, Train loss: 942.8906, Valid loss: 449.6469\n",
      "Epoch 8, Train loss: 947.2405, Valid loss: 486.1249\n",
      "Epoch 9, Train loss: 935.0345, Valid loss: 417.2752\n",
      "Epoch 10, Train loss: 918.1418, Valid loss: 493.2443\n",
      "Epoch 11, Train loss: 920.5837, Valid loss: 453.8934\n",
      "Epoch 12, Train loss: 942.0419, Valid loss: 414.6004\n",
      "Epoch 13, Train loss: 921.0574, Valid loss: 496.9243\n",
      "Epoch 14, Train loss: 883.0618, Valid loss: 447.9666\n",
      "Epoch 15, Train loss: 900.8297, Valid loss: 483.6811\n",
      "Epoch 16, Train loss: 909.5223, Valid loss: 463.8522\n",
      "Epoch 17, Train loss: 882.217, Valid loss: 430.4074\n",
      "Epoch 18, Train loss: 857.416, Valid loss: 452.8491\n",
      "Epoch 19, Train loss: 877.3171, Valid loss: 477.7426\n",
      "Epoch 20, Train loss: 871.2356, Valid loss: 406.557\n",
      "Epoch 21, Train loss: 831.2278, Valid loss: 428.081\n",
      "Epoch 22, Train loss: 890.9338, Valid loss: 475.075\n",
      "Epoch 23, Train loss: 857.8876, Valid loss: 395.2827\n",
      "Epoch 24, Train loss: 851.8568, Valid loss: 448.1471\n",
      "Epoch 25, Train loss: 846.4457, Valid loss: 407.0629\n",
      "Epoch 26, Train loss: 840.5552, Valid loss: 432.8174\n",
      "Epoch 27, Train loss: 866.1258, Valid loss: 394.2144\n",
      "Epoch 28, Train loss: 871.6476, Valid loss: 451.4331\n",
      "Epoch 29, Train loss: 830.7657, Valid loss: 528.2112\n",
      "Epoch 30, Train loss: 859.4059, Valid loss: 396.4047\n",
      "Epoch 31, Train loss: 857.7471, Valid loss: 468.9033\n",
      "Epoch 32, Train loss: 832.8222, Valid loss: 447.2563\n",
      "Epoch 33, Train loss: 849.0692, Valid loss: 430.7432\n",
      "Epoch 34, Train loss: 864.4205, Valid loss: 406.9275\n",
      "Epoch 35, Train loss: 845.6193, Valid loss: 409.5376\n",
      "Epoch 36, Train loss: 884.7773, Valid loss: 467.9911\n",
      "Epoch 37, Train loss: 793.1809, Valid loss: 392.414\n",
      "Epoch 38, Train loss: 854.2588, Valid loss: 444.5872\n",
      "Epoch 39, Train loss: 823.2787, Valid loss: 395.2582\n",
      "Epoch 40, Train loss: 828.9153, Valid loss: 407.2884\n",
      "Epoch 41, Train loss: 840.0238, Valid loss: 409.6881\n",
      "Epoch 42, Train loss: 868.4073, Valid loss: 393.7183\n",
      "Epoch 43, Train loss: 800.4204, Valid loss: 431.2434\n",
      "Epoch 44, Train loss: 796.0277, Valid loss: 456.3435\n",
      "Epoch 45, Train loss: 816.4914, Valid loss: 440.1398\n",
      "Epoch 46, Train loss: 819.5099, Valid loss: 342.1622\n",
      "Epoch 47, Train loss: 815.7807, Valid loss: 462.6151\n",
      "Epoch 48, Train loss: 841.3425, Valid loss: 440.1105\n",
      "Epoch 49, Train loss: 790.1028, Valid loss: 427.72\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 46, training loss = 819.5099, minimum valid loss = 342.1622\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1867.9988, Valid loss: 1003.6614\n",
      "Epoch 1, Train loss: 1424.554, Valid loss: 908.5863\n",
      "Epoch 2, Train loss: 1200.5222, Valid loss: 653.6658\n",
      "Epoch 3, Train loss: 1001.9889, Valid loss: 471.6286\n",
      "Epoch 4, Train loss: 865.2518, Valid loss: 427.5882\n",
      "Epoch 5, Train loss: 740.3682, Valid loss: 316.0225\n",
      "Epoch 6, Train loss: 729.1179, Valid loss: 350.3085\n",
      "Epoch 7, Train loss: 692.9745, Valid loss: 329.4331\n",
      "Epoch 8, Train loss: 668.1091, Valid loss: 296.6251\n",
      "Epoch 9, Train loss: 698.7811, Valid loss: 333.9017\n",
      "Epoch 10, Train loss: 690.6688, Valid loss: 288.2817\n",
      "Epoch 11, Train loss: 654.0962, Valid loss: 290.5148\n",
      "Epoch 12, Train loss: 655.6013, Valid loss: 299.8205\n",
      "Epoch 13, Train loss: 680.6361, Valid loss: 419.8469\n",
      "Epoch 14, Train loss: 671.4674, Valid loss: 354.4666\n",
      "Epoch 15, Train loss: 660.8683, Valid loss: 263.9011\n",
      "Epoch 16, Train loss: 680.931, Valid loss: 285.9297\n",
      "Epoch 17, Train loss: 666.3791, Valid loss: 289.5869\n",
      "Epoch 18, Train loss: 658.0731, Valid loss: 266.3764\n",
      "Epoch 19, Train loss: 651.1188, Valid loss: 272.7971\n",
      "Epoch 20, Train loss: 645.7101, Valid loss: 337.6913\n",
      "Epoch 21, Train loss: 633.4732, Valid loss: 260.6352\n",
      "Epoch 22, Train loss: 645.8244, Valid loss: 325.0069\n",
      "Epoch 23, Train loss: 663.5086, Valid loss: 317.6823\n",
      "Epoch 24, Train loss: 651.197, Valid loss: 263.8859\n",
      "Epoch 25, Train loss: 667.3751, Valid loss: 307.2724\n",
      "Epoch 26, Train loss: 631.0847, Valid loss: 259.3083\n",
      "Epoch 27, Train loss: 648.492, Valid loss: 227.2514\n",
      "Epoch 28, Train loss: 628.063, Valid loss: 311.6773\n",
      "Epoch 29, Train loss: 631.1922, Valid loss: 288.7739\n",
      "Epoch 30, Train loss: 627.7259, Valid loss: 293.5978\n",
      "Epoch 31, Train loss: 602.951, Valid loss: 262.628\n",
      "Epoch 32, Train loss: 607.5802, Valid loss: 261.5956\n",
      "Epoch 33, Train loss: 614.0908, Valid loss: 286.6835\n",
      "Epoch 34, Train loss: 580.2499, Valid loss: 258.7894\n",
      "Epoch 35, Train loss: 640.0905, Valid loss: 284.3916\n",
      "Epoch 36, Train loss: 605.2094, Valid loss: 310.6573\n",
      "Epoch 37, Train loss: 645.6149, Valid loss: 315.2814\n",
      "Epoch 38, Train loss: 643.2158, Valid loss: 272.973\n",
      "Epoch 39, Train loss: 645.9009, Valid loss: 335.0745\n",
      "Epoch 40, Train loss: 636.5387, Valid loss: 335.3388\n",
      "Epoch 41, Train loss: 625.1039, Valid loss: 243.3525\n",
      "Epoch 42, Train loss: 637.183, Valid loss: 284.0279\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 27, training loss = 648.492, minimum valid loss = 227.2514\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 15}\n",
      "Epoch 0, Train loss: 1960.4791, Valid loss: 1486.1267\n",
      "Epoch 1, Train loss: 1398.5232, Valid loss: 695.2398\n",
      "Epoch 2, Train loss: 1196.2872, Valid loss: 628.9687\n",
      "Epoch 3, Train loss: 974.7394, Valid loss: 489.3896\n",
      "Epoch 4, Train loss: 945.5527, Valid loss: 469.0777\n",
      "Epoch 5, Train loss: 914.955, Valid loss: 467.5692\n",
      "Epoch 6, Train loss: 883.807, Valid loss: 341.9458\n",
      "Epoch 7, Train loss: 837.6454, Valid loss: 450.2525\n",
      "Epoch 8, Train loss: 860.0141, Valid loss: 333.3322\n",
      "Epoch 9, Train loss: 818.4213, Valid loss: 404.4923\n",
      "Epoch 10, Train loss: 805.6285, Valid loss: 335.2977\n",
      "Epoch 11, Train loss: 835.5076, Valid loss: 375.4851\n",
      "Epoch 12, Train loss: 814.2608, Valid loss: 382.8481\n",
      "Epoch 13, Train loss: 819.4429, Valid loss: 350.4424\n",
      "Epoch 14, Train loss: 784.6373, Valid loss: 333.4081\n",
      "Epoch 15, Train loss: 783.3077, Valid loss: 327.1164\n",
      "Epoch 16, Train loss: 782.463, Valid loss: 344.8398\n",
      "Epoch 17, Train loss: 722.4248, Valid loss: 313.8182\n",
      "Epoch 18, Train loss: 773.2849, Valid loss: 322.4685\n",
      "Epoch 19, Train loss: 770.0754, Valid loss: 309.6903\n",
      "Epoch 20, Train loss: 781.2861, Valid loss: 383.2915\n",
      "Epoch 21, Train loss: 758.978, Valid loss: 363.4708\n",
      "Epoch 22, Train loss: 780.2662, Valid loss: 331.6523\n",
      "Epoch 23, Train loss: 789.9532, Valid loss: 338.4444\n",
      "Epoch 24, Train loss: 759.8037, Valid loss: 281.1606\n",
      "Epoch 25, Train loss: 736.7299, Valid loss: 336.7483\n",
      "Epoch 26, Train loss: 711.6165, Valid loss: 311.2558\n",
      "Epoch 27, Train loss: 720.0562, Valid loss: 335.8061\n",
      "Epoch 28, Train loss: 729.3795, Valid loss: 270.8066\n",
      "Epoch 29, Train loss: 747.5926, Valid loss: 379.9642\n",
      "Epoch 30, Train loss: 754.2194, Valid loss: 358.2244\n",
      "Epoch 31, Train loss: 754.8783, Valid loss: 285.2904\n",
      "Epoch 32, Train loss: 732.7538, Valid loss: 322.645\n",
      "Epoch 33, Train loss: 743.519, Valid loss: 288.6856\n",
      "Epoch 34, Train loss: 733.0081, Valid loss: 327.1971\n",
      "Epoch 35, Train loss: 702.2318, Valid loss: 335.5902\n",
      "Epoch 36, Train loss: 731.1418, Valid loss: 329.4239\n",
      "Epoch 37, Train loss: 716.1404, Valid loss: 330.1308\n",
      "Epoch 38, Train loss: 717.6438, Valid loss: 297.4246\n",
      "Epoch 39, Train loss: 714.6998, Valid loss: 363.6651\n",
      "Epoch 40, Train loss: 719.9445, Valid loss: 341.6004\n",
      "Epoch 41, Train loss: 725.7137, Valid loss: 297.3201\n",
      "Epoch 42, Train loss: 713.692, Valid loss: 306.6576\n",
      "Epoch 43, Train loss: 720.0546, Valid loss: 285.0633\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 28, training loss = 729.3795, minimum valid loss = 270.8066\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2045.1631, Valid loss: 1708.6256\n",
      "Epoch 1, Train loss: 1527.1257, Valid loss: 774.735\n",
      "Epoch 2, Train loss: 1261.225, Valid loss: 684.7714\n",
      "Epoch 3, Train loss: 1027.5433, Valid loss: 489.65\n",
      "Epoch 4, Train loss: 904.7133, Valid loss: 476.4973\n",
      "Epoch 5, Train loss: 890.2046, Valid loss: 402.9259\n",
      "Epoch 6, Train loss: 853.3375, Valid loss: 420.7103\n",
      "Epoch 7, Train loss: 864.2933, Valid loss: 388.666\n",
      "Epoch 8, Train loss: 841.4253, Valid loss: 380.9613\n",
      "Epoch 9, Train loss: 817.31, Valid loss: 351.42\n",
      "Epoch 10, Train loss: 804.7863, Valid loss: 345.3393\n",
      "Epoch 11, Train loss: 825.0677, Valid loss: 407.4538\n",
      "Epoch 12, Train loss: 786.2316, Valid loss: 291.1411\n",
      "Epoch 13, Train loss: 787.3806, Valid loss: 342.9624\n",
      "Epoch 14, Train loss: 770.4161, Valid loss: 380.462\n",
      "Epoch 15, Train loss: 765.5806, Valid loss: 335.2514\n",
      "Epoch 16, Train loss: 764.3158, Valid loss: 345.2308\n",
      "Epoch 17, Train loss: 756.8358, Valid loss: 340.3466\n",
      "Epoch 18, Train loss: 772.7524, Valid loss: 314.7016\n",
      "Epoch 19, Train loss: 770.7305, Valid loss: 377.5364\n",
      "Epoch 20, Train loss: 757.9447, Valid loss: 311.2067\n",
      "Epoch 21, Train loss: 770.1954, Valid loss: 282.9947\n",
      "Epoch 22, Train loss: 772.753, Valid loss: 366.4479\n",
      "Epoch 23, Train loss: 737.8485, Valid loss: 325.1995\n",
      "Epoch 24, Train loss: 734.99, Valid loss: 306.8075\n",
      "Epoch 25, Train loss: 731.5943, Valid loss: 247.6948\n",
      "Epoch 26, Train loss: 752.1945, Valid loss: 357.2132\n",
      "Epoch 27, Train loss: 720.8629, Valid loss: 286.9782\n",
      "Epoch 28, Train loss: 709.4376, Valid loss: 331.1555\n",
      "Epoch 29, Train loss: 708.4542, Valid loss: 322.8877\n",
      "Epoch 30, Train loss: 722.7151, Valid loss: 259.0188\n",
      "Epoch 31, Train loss: 733.7402, Valid loss: 296.6444\n",
      "Epoch 32, Train loss: 748.2043, Valid loss: 327.4712\n",
      "Epoch 33, Train loss: 721.079, Valid loss: 308.224\n",
      "Epoch 34, Train loss: 681.3384, Valid loss: 264.8394\n",
      "Epoch 35, Train loss: 729.8634, Valid loss: 315.695\n",
      "Epoch 36, Train loss: 723.2666, Valid loss: 329.6765\n",
      "Epoch 37, Train loss: 727.9536, Valid loss: 299.8291\n",
      "Epoch 38, Train loss: 698.6753, Valid loss: 290.5707\n",
      "Epoch 39, Train loss: 686.3778, Valid loss: 292.9034\n",
      "Epoch 40, Train loss: 688.0092, Valid loss: 261.4828\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 25, training loss = 731.5943, minimum valid loss = 247.6948\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2209.9417, Valid loss: 2094.956\n",
      "Epoch 1, Train loss: 2032.2453, Valid loss: 1683.6052\n",
      "Epoch 2, Train loss: 1656.2718, Valid loss: 953.0573\n",
      "Epoch 3, Train loss: 1462.5332, Valid loss: 1034.8442\n",
      "Epoch 4, Train loss: 1364.0958, Valid loss: 861.9067\n",
      "Epoch 5, Train loss: 1337.0426, Valid loss: 808.1948\n",
      "Epoch 6, Train loss: 1240.0429, Valid loss: 729.3015\n",
      "Epoch 7, Train loss: 1109.8457, Valid loss: 541.4931\n",
      "Epoch 8, Train loss: 1027.059, Valid loss: 620.856\n",
      "Epoch 9, Train loss: 992.6349, Valid loss: 494.1047\n",
      "Epoch 10, Train loss: 971.9674, Valid loss: 503.8998\n",
      "Epoch 11, Train loss: 1026.4052, Valid loss: 416.1841\n",
      "Epoch 12, Train loss: 946.9537, Valid loss: 478.2899\n",
      "Epoch 13, Train loss: 936.6216, Valid loss: 513.5803\n",
      "Epoch 14, Train loss: 914.5492, Valid loss: 435.4584\n",
      "Epoch 15, Train loss: 922.7505, Valid loss: 447.7765\n",
      "Epoch 16, Train loss: 925.6545, Valid loss: 484.2772\n",
      "Epoch 17, Train loss: 921.4109, Valid loss: 428.7305\n",
      "Epoch 18, Train loss: 929.3165, Valid loss: 469.6887\n",
      "Epoch 19, Train loss: 907.7112, Valid loss: 429.053\n",
      "Epoch 20, Train loss: 934.5314, Valid loss: 419.239\n",
      "Epoch 21, Train loss: 893.161, Valid loss: 440.2438\n",
      "Epoch 22, Train loss: 941.1969, Valid loss: 458.4131\n",
      "Epoch 23, Train loss: 920.9839, Valid loss: 419.1563\n",
      "Epoch 24, Train loss: 893.7347, Valid loss: 411.3662\n",
      "Epoch 25, Train loss: 888.7446, Valid loss: 440.8829\n",
      "Epoch 26, Train loss: 859.6109, Valid loss: 445.0592\n",
      "Epoch 27, Train loss: 874.2696, Valid loss: 443.7544\n",
      "Epoch 28, Train loss: 862.2003, Valid loss: 400.287\n",
      "Epoch 29, Train loss: 889.0006, Valid loss: 411.3447\n",
      "Epoch 30, Train loss: 880.4504, Valid loss: 422.8977\n",
      "Epoch 31, Train loss: 855.3533, Valid loss: 432.4203\n",
      "Epoch 32, Train loss: 871.7194, Valid loss: 458.2206\n",
      "Epoch 33, Train loss: 863.841, Valid loss: 476.558\n",
      "Epoch 34, Train loss: 849.1961, Valid loss: 400.275\n",
      "Epoch 35, Train loss: 890.315, Valid loss: 425.0277\n",
      "Epoch 36, Train loss: 863.6633, Valid loss: 446.2934\n",
      "Epoch 37, Train loss: 867.8693, Valid loss: 421.6037\n",
      "Epoch 38, Train loss: 874.9258, Valid loss: 356.8678\n",
      "Epoch 39, Train loss: 842.8561, Valid loss: 412.5233\n",
      "Epoch 40, Train loss: 835.3903, Valid loss: 389.405\n",
      "Epoch 41, Train loss: 856.1272, Valid loss: 475.5174\n",
      "Epoch 42, Train loss: 857.9824, Valid loss: 402.8784\n",
      "Epoch 43, Train loss: 851.4134, Valid loss: 393.2795\n",
      "Epoch 44, Train loss: 852.4712, Valid loss: 420.4858\n",
      "Epoch 45, Train loss: 825.1955, Valid loss: 413.3877\n",
      "Epoch 46, Train loss: 861.5087, Valid loss: 383.7793\n",
      "Epoch 47, Train loss: 841.5137, Valid loss: 447.8612\n",
      "Epoch 48, Train loss: 846.3128, Valid loss: 408.2086\n",
      "Epoch 49, Train loss: 844.3778, Valid loss: 382.2167\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 38, training loss = 874.9258, minimum valid loss = 356.8678\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 5}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2035.4453, Valid loss: 1667.0498\n",
      "Epoch 1, Train loss: 1515.5809, Valid loss: 785.7795\n",
      "Epoch 2, Train loss: 1297.2203, Valid loss: 863.5303\n",
      "Epoch 3, Train loss: 1168.8882, Valid loss: 663.5767\n",
      "Epoch 4, Train loss: 1126.3373, Valid loss: 646.3866\n",
      "Epoch 5, Train loss: 1071.7562, Valid loss: 562.128\n",
      "Epoch 6, Train loss: 1021.5097, Valid loss: 532.7386\n",
      "Epoch 7, Train loss: 1022.325, Valid loss: 592.2185\n",
      "Epoch 8, Train loss: 1047.0107, Valid loss: 457.3852\n",
      "Epoch 9, Train loss: 967.5912, Valid loss: 522.7195\n",
      "Epoch 10, Train loss: 945.083, Valid loss: 481.3606\n",
      "Epoch 11, Train loss: 989.1681, Valid loss: 540.4579\n",
      "Epoch 12, Train loss: 973.4946, Valid loss: 497.476\n",
      "Epoch 13, Train loss: 967.7317, Valid loss: 535.4607\n",
      "Epoch 14, Train loss: 959.7355, Valid loss: 509.0501\n",
      "Epoch 15, Train loss: 900.1218, Valid loss: 505.667\n",
      "Epoch 16, Train loss: 940.2945, Valid loss: 470.0976\n",
      "Epoch 17, Train loss: 928.408, Valid loss: 536.0213\n",
      "Epoch 18, Train loss: 909.8062, Valid loss: 456.1663\n",
      "Epoch 19, Train loss: 936.486, Valid loss: 452.2785\n",
      "Epoch 20, Train loss: 960.0332, Valid loss: 435.5368\n",
      "Epoch 21, Train loss: 908.2556, Valid loss: 434.8047\n",
      "Epoch 22, Train loss: 926.647, Valid loss: 430.1155\n",
      "Epoch 23, Train loss: 949.8001, Valid loss: 477.6548\n",
      "Epoch 24, Train loss: 910.551, Valid loss: 512.6466\n",
      "Epoch 25, Train loss: 927.0576, Valid loss: 436.3315\n",
      "Epoch 26, Train loss: 917.3572, Valid loss: 498.4667\n",
      "Epoch 27, Train loss: 882.0349, Valid loss: 429.0546\n",
      "Epoch 28, Train loss: 907.2299, Valid loss: 485.0183\n",
      "Epoch 29, Train loss: 933.1772, Valid loss: 446.3175\n",
      "Epoch 30, Train loss: 911.2783, Valid loss: 454.4127\n",
      "Epoch 31, Train loss: 913.0116, Valid loss: 451.9901\n",
      "Epoch 32, Train loss: 916.6061, Valid loss: 426.1119\n",
      "Epoch 33, Train loss: 907.1208, Valid loss: 377.2596\n",
      "Epoch 34, Train loss: 929.099, Valid loss: 442.8836\n",
      "Epoch 35, Train loss: 900.4472, Valid loss: 388.7118\n",
      "Epoch 36, Train loss: 893.6262, Valid loss: 497.372\n",
      "Epoch 37, Train loss: 896.7444, Valid loss: 441.1399\n",
      "Epoch 38, Train loss: 917.6469, Valid loss: 465.2112\n",
      "Epoch 39, Train loss: 875.469, Valid loss: 407.6163\n",
      "Epoch 40, Train loss: 857.8988, Valid loss: 444.6308\n",
      "Epoch 41, Train loss: 858.2404, Valid loss: 430.6857\n",
      "Epoch 42, Train loss: 871.5626, Valid loss: 442.944\n",
      "Epoch 43, Train loss: 845.6179, Valid loss: 450.7431\n",
      "Epoch 44, Train loss: 894.1742, Valid loss: 439.321\n",
      "Epoch 45, Train loss: 868.8333, Valid loss: 463.8499\n",
      "Epoch 46, Train loss: 890.7964, Valid loss: 481.3443\n",
      "Epoch 47, Train loss: 842.8102, Valid loss: 391.8357\n",
      "Epoch 48, Train loss: 864.027, Valid loss: 413.6072\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 33, training loss = 907.1208, minimum valid loss = 377.2596\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 15}\n",
      "Epoch 0, Train loss: 2060.9936, Valid loss: 1723.3771\n",
      "Epoch 1, Train loss: 1572.9552, Valid loss: 917.953\n",
      "Epoch 2, Train loss: 1342.9147, Valid loss: 865.2226\n",
      "Epoch 3, Train loss: 1225.3483, Valid loss: 775.6069\n",
      "Epoch 4, Train loss: 1155.4889, Valid loss: 656.0745\n",
      "Epoch 5, Train loss: 1130.6025, Valid loss: 597.6273\n",
      "Epoch 6, Train loss: 1073.2091, Valid loss: 540.0628\n",
      "Epoch 7, Train loss: 1015.6505, Valid loss: 556.0444\n",
      "Epoch 8, Train loss: 1021.0265, Valid loss: 523.9571\n",
      "Epoch 9, Train loss: 986.3613, Valid loss: 530.2556\n",
      "Epoch 10, Train loss: 973.1304, Valid loss: 547.1696\n",
      "Epoch 11, Train loss: 976.7809, Valid loss: 505.9889\n",
      "Epoch 12, Train loss: 965.2152, Valid loss: 569.4581\n",
      "Epoch 13, Train loss: 956.2017, Valid loss: 465.0646\n",
      "Epoch 14, Train loss: 967.2297, Valid loss: 556.3455\n",
      "Epoch 15, Train loss: 978.4468, Valid loss: 546.9867\n",
      "Epoch 16, Train loss: 908.9397, Valid loss: 516.6499\n",
      "Epoch 17, Train loss: 924.9876, Valid loss: 438.8196\n",
      "Epoch 18, Train loss: 927.5678, Valid loss: 471.7854\n",
      "Epoch 19, Train loss: 929.0274, Valid loss: 501.9119\n",
      "Epoch 20, Train loss: 941.0721, Valid loss: 523.7896\n",
      "Epoch 21, Train loss: 950.2869, Valid loss: 453.0776\n",
      "Epoch 22, Train loss: 912.1413, Valid loss: 456.3272\n",
      "Epoch 23, Train loss: 939.0003, Valid loss: 487.1864\n",
      "Epoch 24, Train loss: 893.659, Valid loss: 441.8002\n",
      "Epoch 25, Train loss: 925.1568, Valid loss: 406.4511\n",
      "Epoch 26, Train loss: 919.3589, Valid loss: 472.2258\n",
      "Epoch 27, Train loss: 937.2189, Valid loss: 475.8864\n",
      "Epoch 28, Train loss: 916.3243, Valid loss: 471.3881\n",
      "Epoch 29, Train loss: 863.3465, Valid loss: 449.7694\n",
      "Epoch 30, Train loss: 878.072, Valid loss: 425.2329\n",
      "Epoch 31, Train loss: 873.907, Valid loss: 467.1641\n",
      "Epoch 32, Train loss: 934.8153, Valid loss: 452.9946\n",
      "Epoch 33, Train loss: 893.673, Valid loss: 484.8594\n",
      "Epoch 34, Train loss: 873.6229, Valid loss: 470.5051\n",
      "Epoch 35, Train loss: 891.1102, Valid loss: 500.3076\n",
      "Epoch 36, Train loss: 908.2744, Valid loss: 442.1092\n",
      "Epoch 37, Train loss: 873.494, Valid loss: 396.1956\n",
      "Epoch 38, Train loss: 888.904, Valid loss: 454.7413\n",
      "Epoch 39, Train loss: 890.8699, Valid loss: 451.3357\n",
      "Epoch 40, Train loss: 858.8886, Valid loss: 441.5018\n",
      "Epoch 41, Train loss: 890.3309, Valid loss: 436.9825\n",
      "Epoch 42, Train loss: 874.8496, Valid loss: 368.1846\n",
      "Epoch 43, Train loss: 906.2201, Valid loss: 479.4635\n",
      "Epoch 44, Train loss: 867.2674, Valid loss: 458.8173\n",
      "Epoch 45, Train loss: 864.7516, Valid loss: 469.1385\n",
      "Epoch 46, Train loss: 863.0251, Valid loss: 510.2208\n",
      "Epoch 47, Train loss: 861.6124, Valid loss: 462.4882\n",
      "Epoch 48, Train loss: 861.5651, Valid loss: 442.4306\n",
      "Epoch 49, Train loss: 838.4034, Valid loss: 370.5232\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 42, training loss = 874.8496, minimum valid loss = 368.1846\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = 10 ** 7\n",
    "best_train_loss = 0\n",
    "best_loss_record = None\n",
    "best_model = None\n",
    "best_params = None\n",
    "count = 0\n",
    "\n",
    "for param in param_grid:\n",
    "        print(\"Parameter settings:\")\n",
    "        print(param)\n",
    "        model = NeuralNet(train_set.dataset.dim).to(device)  # Construct model and move to device\n",
    "        loss_record, local_best_model = train(model, train_set, valid_set, param, device, verbose = True)\n",
    "        if min(loss_record[\"valid\"]) < best_valid_loss:\n",
    "            best_valid_loss = min(loss_record[\"valid\"])\n",
    "            best_train_loss = min(loss_record[\"train\"])\n",
    "            best_loss_record = loss_record\n",
    "            best_model = local_best_model\n",
    "            best_params = param\n",
    "        print(\"=\"*50)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Tuning Result ==========\n",
      "Best params are: {'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 15}\n",
      "Training MSE: 389.68571267960266, Valid MSE: 148.4122362580208\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"models/HW1.model_pth\"\n",
    "print(\"=\"*10, \"Tuning Result\", \"=\"*10)\n",
    "print(f\"Best params are: {best_params}\")\n",
    "print(f\"Training MSE: {best_train_loss}, Valid MSE: {best_valid_loss}\")\n",
    "torch.save(best_model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve(best_loss_record[\"train\"], best_loss_record[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test(best_model, test_set, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['INDEX', 'PM2.5'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i+1, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to ./submission/submission_1031_9.csv\n"
     ]
    }
   ],
   "source": [
    "submit_path = \"./submission/submission_1101_1.csv\"\n",
    "save_pred(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-088d69fb3456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PM2.5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PM2.5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    plt.plot(data_train[col])\n",
    "    plt.plot(data_train[\"PM2.5\"])\n",
    "    plt.legend([col, \"PM2.5\"])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    plt.boxplot(data_train[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMB_TEMP</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CO</th>\n",
       "      <th>NMHC</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>PM10</th>\n",
       "      <th>RH</th>\n",
       "      <th>SO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMB_TEMP  CH4    CO  NMHC    NO   NO2   NOx   PM10    RH  SO2\n",
       "0         19.0  2.1  0.40  0.19   1.8  20.0  22.0   57.0  91.0  2.1\n",
       "1         18.0  2.2  0.46  0.18   3.9  19.0  23.0   65.0  92.0  2.6\n",
       "2         18.0  2.3  0.46  0.17   5.3  19.0  25.0   61.0  92.0  2.6\n",
       "3         18.0  2.3  0.46  0.19  14.0  22.0  37.0   58.0  93.0  2.5\n",
       "4         18.0  2.2  0.37  0.16   4.1  17.0  21.0   41.0  93.0  2.0\n",
       "...        ...  ...   ...   ...   ...   ...   ...    ...   ...  ...\n",
       "8755      17.0  2.2  0.85  0.17   0.9  16.0  17.0  177.0  69.0  2.7\n",
       "8756      17.0  2.2  0.82  0.17   0.9  18.0  18.0  143.0  70.0  2.3\n",
       "8757      17.0  2.1  0.78  0.16   1.1  15.0  17.0  110.0  72.0  2.3\n",
       "8758      16.0  2.1  0.73  0.14   1.2  17.0  18.0   96.0  73.0  2.4\n",
       "8759      16.0  2.1  0.65  0.13   1.3  17.0  18.0   87.0  73.0  2.3\n",
       "\n",
       "[8760 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NeuralNet(train_set.dataset.dim).to(device)\n",
    "model_check_point = torch.load(model_save_path, map_location='cpu')\n",
    "model.load_state_dict(model_check_point)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
