{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#For data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For reproducibility\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, preprocess_params):\n",
    "        self.train_valid_ratio = preprocess_params[\"train_valid_ratio\"]\n",
    "        self.p_value_threshold = preprocess_params[\"p_value_threshold\"]\n",
    "        self.scaler = None\n",
    "\n",
    "    def preproces_train_data(self, data_path):\n",
    "        \"\"\"\n",
    "        get_lag_variable -> correlation coefficient -> split_train_valid -> normalize\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(data_path)\n",
    "        X = data.loc[:, data.columns != \"PM2.5\"]\n",
    "        y = data.loc[:, data.columns == \"PM2.5\"]\n",
    "        #X = self.get_lag_variable(X, self.num_lag, isTesting = False)\n",
    "        #X = X.dropna()\n",
    "        X = self.do_correlation_coefficient_test(X, y, self.p_value_threshold)\n",
    "        X_train, X_valid, y_train, y_valid = self.split_train_valid(X, y, self.train_valid_ratio)\n",
    "        X_train = self.normalize(X_train, isTraining=True)\n",
    "        X_valid = self.normalize(X_valid, isTraining=False)\n",
    "        return X_train, X_valid, np.array(y_train), np.array(y_valid)\n",
    "\n",
    "    def preprocess_test_data(self, data_path):\n",
    "        \"\"\"\n",
    "        get_lag_variable -> correlation coefficient -> normalize\n",
    "        \"\"\"\n",
    "        X_test = pd.read_csv(data_path)\n",
    "        #X_test = self.get_lag_variable(X_test, self.num_lag, isTesting = True)\n",
    "        X_test = X_test[self.kept_columns]\n",
    "        X_test = self.normalize(X_test, isTraining=False)\n",
    "        return X_test\n",
    "\n",
    "    def normalize(self, data, isTraining = False):\n",
    "        if isTraining:\n",
    "            self.scaler = StandardScaler().fit(data)\n",
    "        data_scaled = self.scaler.transform(data)\n",
    "        return data_scaled\n",
    "\n",
    "    def do_correlation_coefficient_test(self, X, y, p_value_threshold):\n",
    "        corr_features = abs(X.corrwith(y['PM2.5'])).sort_values(ascending=False)\n",
    "        print(corr_features)\n",
    "        self.kept_columns = (corr_features[corr_features > p_value_threshold]).index\n",
    "        X = X[self.kept_columns]\n",
    "        return X\n",
    "\n",
    "    def split_train_valid(self, X, y, train_valid_ratio = 0.75):\n",
    "        data_len = len(X)\n",
    "        train_len = round(train_valid_ratio * data_len)\n",
    "        train_index = random.sample(range(data_len), train_len)\n",
    "        valid_index = [i for i in range(data_len) if i not in train_index]\n",
    "        X_train = X.iloc[train_index, :]\n",
    "        y_train = y.iloc[train_index, :]\n",
    "        X_valid = X.iloc[valid_index, :]\n",
    "        y_valid = y.iloc[valid_index, :]\n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    # def get_lag_variable(self, data: pd.DataFrame, num_lag: int, isTesting = False):\n",
    "    #     #start from 0\n",
    "    #     data_ = data.copy()\n",
    "    #     for idx in range(num_lag):\n",
    "    #         #start from 1\n",
    "    #         time = idx + 1\n",
    "    #         lag_data = data.shift(time)\n",
    "    #         lag_data.columns = [col + \"_\" + str(-time) for col in data.columns]\n",
    "    #         data_ = pd.concat([data_, lag_data], axis = 1)\n",
    "    #     #fill testing set nan with \"mean\" \n",
    "    #     if isTesting:\n",
    "    #         data_.fillna(data_.mean(), inplace = True)\n",
    "    #     return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM10        0.854973\n",
      "CO          0.593970\n",
      "CH4         0.526962\n",
      "NO2         0.460302\n",
      "NOx         0.391366\n",
      "SO2         0.325951\n",
      "AMB_TEMP    0.299272\n",
      "NMHC        0.129990\n",
      "NO          0.108846\n",
      "RH          0.073292\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/\"\n",
    "train_path = os.path.join(data_path, \"training_data.csv\")\n",
    "test_path = os.path.join(data_path, \"testing_data.csv\")\n",
    "preprocess_params = {\n",
    "    \"train_valid_ratio\": 0.75,\n",
    "    \"p_value_threshold\": 0.3\n",
    "}\n",
    "\n",
    "DP = DataPreprocessor(preprocess_params)\n",
    "X_train, X_valid, y_train, y_valid = DP.preproces_train_data(train_path)\n",
    "X_test = DP.preprocess_test_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6588, 6)\n",
      "(2196, 6)\n",
      "(8760, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPADataSet(Dataset):\n",
    "    \"\"\"Dataset for loading and preprocessing the EPA Weather data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y = None, mode = \"train\"):\n",
    "        self.mode = mode\n",
    "        if mode in [\"train\", \"valid\"]:\n",
    "            self.data = torch.from_numpy(X).float()\n",
    "            self.target = torch.from_numpy(y.reshape(-1, 1)).float()\n",
    "            self.dim = X.shape[1]\n",
    "            # print(f\"Check nan...\")\n",
    "            # if not np.any(np.isnan(X)) and not np.any(np.isnan(y)):\n",
    "            #     print(\"No nan in dataset.\")\n",
    "        else:\n",
    "            self.data = torch.from_numpy(X).float()\n",
    "            self.dim = X.shape[1]\n",
    "        #     print(f\"Check nan...\")\n",
    "        #     if not np.any(np.isnan(X)):\n",
    "        #         print(\"No nan in dataset.\")\n",
    "\n",
    "        print(f\"Finish reading {self.mode} dataset ({len(self.data)} samples, {self.dim} features)\")\n",
    "        # if mode in [\"train\", \"valid\"]:\n",
    "        #     print(f\"Check target size... The target size is {len(self.target)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in [\"train\", \"valid\"]:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataloader(mode, batch_size, X, y = None):\n",
    "    \"\"\"Generates a dataset, then is put into a dataloader. \n",
    "    \"\"\"\n",
    "    dataset = EPADataSet(X, y, mode)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle = (mode == \"train\"), drop_last=False, pin_memory = True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(train_loss, valid_loss):\n",
    "    num_epoch = len(train_loss)\n",
    "    x = [i+1 for i in range(num_epoch)]\n",
    "    plt.plot(x, train_loss)\n",
    "    plt.plot(x, valid_loss)\n",
    "    plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #input layer\n",
    "            nn.Linear(input_dim, 3), \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #hidden layer\n",
    "            nn.Linear(3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #output layer\n",
    "            nn.Linear(3, 1)\n",
    "        )\n",
    "\n",
    "        self.net.apply(self.init_weights)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, valid_set, params_set, device, verbose = False):\n",
    "    #params settings\n",
    "    num_epoch = params_set[\"num_epoch\"]\n",
    "    optimizer = getattr(torch.optim, params_set['optimizer'])(\n",
    "    model.parameters(), **params_set['optim_hparams'])\n",
    "    patience = params_set[\"patience\"]\n",
    "\n",
    "    #recording\n",
    "    best_train_MSE = 0\n",
    "    best_valid_MSE = 99999\n",
    "    best_epoch = 0\n",
    "    loss_record = {\"train\": [], \"valid\": []}\n",
    "\n",
    "    #start training\n",
    "    for idx_epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_SSE = 0\n",
    "        for idx, train_data in enumerate(train_set):\n",
    "            X, y = train_data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(X)\n",
    "            train_loss = model.cal_loss(train_pred.flatten(), y.flatten())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_SSE += (train_loss.detach().cpu().item()) * len(X)\n",
    "        train_MSE = train_SSE / len(train_set.dataset)\n",
    "        loss_record[\"train\"].append(train_MSE)\n",
    "        #validation\n",
    "        valid_MSE = valid(model, valid_set, device)\n",
    "        loss_record[\"valid\"].append(valid_MSE)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {idx_epoch}, Train loss: {round(train_MSE, 4)}, Valid loss: {round(valid_MSE, 4)}\")\n",
    "\n",
    "        #save best result\n",
    "        if valid_MSE < best_valid_MSE:\n",
    "            best_valid_MSE = valid_MSE\n",
    "            best_train_MSE = train_MSE\n",
    "            best_epoch = idx_epoch\n",
    "        \n",
    "        #early stopping\n",
    "        if valid_MSE > best_valid_MSE and idx_epoch >= best_epoch + patience:\n",
    "            print(\"=\"*50)\n",
    "            print(\"Early Stopping!\")\n",
    "            print(f\"Best epoch is {best_epoch}, training loss = {round(best_train_MSE, 4)}, minimum valid loss = {round(best_valid_MSE, 4)}\")\n",
    "            return loss_record, model\n",
    "\n",
    "    #Training until the final epoch\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Model result:\")\n",
    "    print(f\"Best epoch is {best_epoch}, training loss = {round(best_train_MSE, 4)}, minimum valid loss = {round(best_valid_MSE, 4)}\")\n",
    "    return loss_record, model\n",
    "\n",
    "def valid(model, valid_set, device):\n",
    "    valid_SSE = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()     \n",
    "        for idx, valid_data in enumerate(valid_set):\n",
    "            X, y = valid_data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            valid_pred = model(X)\n",
    "            valid_loss = model.cal_loss(valid_pred, y)\n",
    "            valid_SSE += (valid_loss.detach().cpu().item()) * len(X)\n",
    "    valid_MSE = valid_SSE / len(valid_set.dataset)\n",
    "    return valid_MSE\n",
    "\n",
    "def test(model, test_set, device):\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X in test_set:\n",
    "            X = X.to(device)\n",
    "            test_pred = model(X)\n",
    "            preds.append(test_pred.flatten().detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "train_path = os.path.join(data_path, \"training_data.csv\")\n",
    "test_path = os.path.join(data_path, \"testing_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_params = {\n",
    "    \"train_valid_ratio\": 0.75,\n",
    "    \"p_value_threshold\": 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set = {\n",
    "    \"num_batch\": [256, 512, 1024], \n",
    "    \"num_epoch\": [20, 50],\n",
    "    #optimizer\n",
    "    \"optimizer\": [\"Adam\"],\n",
    "    \"optim_hparams\": [{\n",
    "        \"lr\": [0.01, 0.05, 0.1, 0.5],\n",
    "        \"weight_decay\": [0.01, 0.05, 0.1, 1, 10]\n",
    "    }],\n",
    "    #for early stopping\n",
    "    \"patience\": [10]\n",
    "}\n",
    "\n",
    "param_grid = []\n",
    "count = 0\n",
    "for values in itertools.product(*params_set.values()):\n",
    "    param = dict(zip(params_set.keys(), values))\n",
    "    for optim_values in itertools.product(*param[\"optim_hparams\"].values()):\n",
    "        param_copy = param.copy()\n",
    "        param_copy[\"optim_hparams\"] = dict(zip(param[\"optim_hparams\"].keys(), optim_values))\n",
    "        param_grid.append(param_copy)\n",
    "\n",
    "device = get_device() \n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 711.6547, minimum valid loss = 708.0999\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 152.1248, minimum valid loss = 135.9713\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 150.5359, minimum valid loss = 134.8672\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 169.6412, minimum valid loss = 149.6318\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 14, training loss = 159.1983, minimum valid loss = 140.2773\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 1, training loss = 194.3316, minimum valid loss = 167.2659\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 743.1494, minimum valid loss = 728.8027\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 167.2679, minimum valid loss = 158.8187\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 147.9291, minimum valid loss = 139.221\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 144.7474, minimum valid loss = 137.0426\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 168.8137, minimum valid loss = 159.3603\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 7, training loss = 173.7537, minimum valid loss = 160.2674\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 169.3438, minimum valid loss = 159.4898\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 139.8297, minimum valid loss = 136.1231\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 146.3056, minimum valid loss = 139.9722\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 12, training loss = 169.9868, minimum valid loss = 163.5614\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 167.5688, minimum valid loss = 160.2035\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 169.5089, minimum valid loss = 163.2143\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 168.0666, minimum valid loss = 164.2142\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1859.3359, minimum valid loss = 1799.0398\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 168.7101, minimum valid loss = 163.5142\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 165.0801, minimum valid loss = 160.5356\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 156.9326, minimum valid loss = 151.4048\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 166.2933, minimum valid loss = 160.7662\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 169.4278, minimum valid loss = 165.0019\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 169.3157, minimum valid loss = 164.3315\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 168.7925, minimum valid loss = 162.6376\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 167.6343, minimum valid loss = 161.9032\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 167.5682, minimum valid loss = 163.3141\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 169.1427, minimum valid loss = 162.4434\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 20, training loss = 161.742, minimum valid loss = 143.5728\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 192.7059, minimum valid loss = 163.2514\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 19, training loss = 152.0719, minimum valid loss = 135.6935\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 25, training loss = 157.9973, minimum valid loss = 143.1131\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 35, training loss = 164.3031, minimum valid loss = 150.2926\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 195.6793, minimum valid loss = 163.5097\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 31, training loss = 711.7171, minimum valid loss = 708.0465\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 8, training loss = 140.9981, minimum valid loss = 134.6744\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 26, training loss = 165.3435, minimum valid loss = 158.5017\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 14, training loss = 138.1417, minimum valid loss = 134.9747\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 31, training loss = 150.9172, minimum valid loss = 142.1329\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 172.8222, minimum valid loss = 162.3093\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 27, training loss = 165.4932, minimum valid loss = 158.2721\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 139.6675, minimum valid loss = 133.0954\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 19, training loss = 139.8578, minimum valid loss = 134.1257\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 27, training loss = 711.1782, minimum valid loss = 708.0469\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 20, training loss = 141.3336, minimum valid loss = 133.7181\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 21, training loss = 170.334, minimum valid loss = 160.5454\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 163.5422, minimum valid loss = 158.9865\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 34, training loss = 164.4839, minimum valid loss = 159.5029\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 42, training loss = 138.1645, minimum valid loss = 133.8054\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 22, training loss = 711.772, minimum valid loss = 708.0465\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 48, training loss = 146.3264, minimum valid loss = 142.7771\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 45, training loss = 167.1422, minimum valid loss = 161.4977\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 138.4871, minimum valid loss = 136.9278\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 39, training loss = 167.1656, minimum valid loss = 163.0136\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 166.4372, minimum valid loss = 161.9965\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 48, training loss = 145.7893, minimum valid loss = 142.9538\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 164.9719, minimum valid loss = 160.3842\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 256, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 21, training loss = 169.2084, minimum valid loss = 163.0347\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 171.387, minimum valid loss = 162.9804\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 155.6631, minimum valid loss = 145.4647\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 10, training loss = 168.9584, minimum valid loss = 161.5049\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 173.6959, minimum valid loss = 160.4366\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 5, training loss = 169.5628, minimum valid loss = 160.9853\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 4, training loss = 193.1816, minimum valid loss = 164.0251\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1016.5662, minimum valid loss = 973.4086\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 16, training loss = 164.206, minimum valid loss = 160.3904\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 145.3214, minimum valid loss = 141.5418\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 151.3922, minimum valid loss = 143.6804\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 147.8618, minimum valid loss = 138.6385\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 7, training loss = 170.9035, minimum valid loss = 163.132\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 155.4429, minimum valid loss = 152.905\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 164.6912, minimum valid loss = 162.701\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 164.7486, minimum valid loss = 161.033\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 138.1597, minimum valid loss = 132.5458\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 170.8733, minimum valid loss = 164.2779\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 11, training loss = 169.3842, minimum valid loss = 163.2655\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 160.1652, minimum valid loss = 154.3098\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 170.3277, minimum valid loss = 163.5058\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 166.4978, minimum valid loss = 165.7419\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 169.6337, minimum valid loss = 165.1153\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 162.4373, minimum valid loss = 159.2393\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1874.2379, minimum valid loss = 1731.0615\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2120.4574, minimum valid loss = 2060.7736\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 207.2404, minimum valid loss = 194.9648\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 381.3252, minimum valid loss = 315.2262\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 182.0076, minimum valid loss = 171.5354\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2120.7845, minimum valid loss = 2061.1338\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2123.8839, minimum valid loss = 2064.3564\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 17, training loss = 144.316, minimum valid loss = 136.0594\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 45, training loss = 146.6784, minimum valid loss = 139.4047\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 21, training loss = 315.9287, minimum valid loss = 301.6799\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 12, training loss = 179.1914, minimum valid loss = 159.0542\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 40, training loss = 159.5662, minimum valid loss = 145.9153\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 29, training loss = 177.7228, minimum valid loss = 166.1325\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 44, training loss = 166.7441, minimum valid loss = 158.4156\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 138.3737, minimum valid loss = 134.6169\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 40, training loss = 139.0348, minimum valid loss = 134.091\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 23, training loss = 134.8005, minimum valid loss = 132.5154\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 6, training loss = 168.9924, minimum valid loss = 162.4005\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 30, training loss = 170.3808, minimum valid loss = 160.793\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 139.6101, minimum valid loss = 134.4071\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 21, training loss = 164.0444, minimum valid loss = 159.6754\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 897.203, minimum valid loss = 870.089\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 164.4313, minimum valid loss = 159.5375\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 16, training loss = 140.7898, minimum valid loss = 134.9501\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 11, training loss = 169.0076, minimum valid loss = 162.3973\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 43, training loss = 166.9548, minimum valid loss = 162.2226\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 160.6537, minimum valid loss = 157.5212\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 43, training loss = 167.4743, minimum valid loss = 162.5077\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 1775.4532, minimum valid loss = 1721.3494\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 163.6004, minimum valid loss = 159.4117\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 14, training loss = 166.7045, minimum valid loss = 162.4862\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 168.6341, minimum valid loss = 161.7769\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 154.6106, minimum valid loss = 152.1017\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 156.3802, minimum valid loss = 151.8401\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 163.5267, minimum valid loss = 161.785\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 167.4341, minimum valid loss = 163.4624\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 174.2802, minimum valid loss = 167.6777\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 152.0016, minimum valid loss = 145.0761\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 214.2537, minimum valid loss = 207.9619\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 168.567, minimum valid loss = 163.0736\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 169.534, minimum valid loss = 163.9947\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 151.4264, minimum valid loss = 146.8044\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 15, training loss = 172.1314, minimum valid loss = 165.4301\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 164.8571, minimum valid loss = 160.6862\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 166.9222, minimum valid loss = 163.3037\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 169.7136, minimum valid loss = 161.8429\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 160.9772, minimum valid loss = 156.3004\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 168.4018, minimum valid loss = 162.4473\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 166.4947, minimum valid loss = 160.8409\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 163.7134, minimum valid loss = 157.0831\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1750.772, minimum valid loss = 1689.0582\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 18, training loss = 168.5999, minimum valid loss = 164.3511\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 166.9757, minimum valid loss = 162.3766\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 165.8895, minimum valid loss = 162.6195\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 17, training loss = 167.0209, minimum valid loss = 160.4991\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 202.7902, minimum valid loss = 191.6848\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 193.9241, minimum valid loss = 186.3124\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 191.4368, minimum valid loss = 180.9414\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 245.5676, minimum valid loss = 218.241\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1834.6566, minimum valid loss = 1763.5284\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 1871.1456, minimum valid loss = 1769.4604\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 397.3954, minimum valid loss = 350.0672\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 518.7024, minimum valid loss = 450.7901\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 2164.2876, minimum valid loss = 2104.7616\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 873.262, minimum valid loss = 751.5697\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 807.5785, minimum valid loss = 715.1946\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 20, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 19, training loss = 731.3977, minimum valid loss = 650.0677\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 18, training loss = 320.1374, minimum valid loss = 311.6312\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 45, training loss = 143.3695, minimum valid loss = 134.4858\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 169.7677, minimum valid loss = 156.4285\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 44, training loss = 142.3259, minimum valid loss = 133.2636\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 36, training loss = 162.5415, minimum valid loss = 157.1757\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.5, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 29, training loss = 179.6769, minimum valid loss = 162.4589\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 46, training loss = 165.4275, minimum valid loss = 160.2815\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 857.996, minimum valid loss = 833.0088\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 43, training loss = 145.6291, minimum valid loss = 140.7157\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 42, training loss = 137.4956, minimum valid loss = 134.0339\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 29, training loss = 164.8048, minimum valid loss = 159.2416\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 24, training loss = 166.9367, minimum valid loss = 160.1278\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 44, training loss = 167.4165, minimum valid loss = 163.573\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 31, training loss = 163.1998, minimum valid loss = 159.6894\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 143.7877, minimum valid loss = 138.8857\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 165.5159, minimum valid loss = 159.7331\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 46, training loss = 165.3336, minimum valid loss = 160.1042\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.05, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Early Stopping!\n",
      "Best epoch is 25, training loss = 168.1512, minimum valid loss = 162.4036\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 47, training loss = 168.6248, minimum valid loss = 164.2882\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 168.8878, minimum valid loss = 164.088\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 166.7318, minimum valid loss = 162.3338\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 152.9369, minimum valid loss = 149.7465\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 159.5941, minimum valid loss = 155.5416\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.01, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 42, training loss = 168.4426, minimum valid loss = 163.3861\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.001}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 172.9228, minimum valid loss = 168.5107\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.01}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 173.295, minimum valid loss = 167.389\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.05}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 173.9648, minimum valid loss = 169.9225\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 170.2492, minimum valid loss = 165.2029\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 1}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 2087.1103, minimum valid loss = 2029.1477\n",
      "==================================================\n",
      "Parameter settings:\n",
      "{'num_batch': 1024, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.005, 'weight_decay': 10}, 'patience': 10}\n",
      "Finish reading train dataset (6588 samples, 6 features)\n",
      "Finish reading valid dataset (2196 samples, 6 features)\n",
      "Finish reading test dataset (8760 samples, 6 features)\n",
      "==================================================\n",
      "Model result:\n",
      "Best epoch is 49, training loss = 169.0424, minimum valid loss = 163.0546\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = 10 ** 7\n",
    "best_train_loss = 0\n",
    "best_loss_record = None\n",
    "best_model = None\n",
    "best_params = None\n",
    "count = 0\n",
    "all_loss_record = list()\n",
    "\n",
    "#change when you need to tune this model\n",
    "model_save_path = \"models/HW1.model_1101_1204.pth\"\n",
    "\n",
    "for param in param_grid:\n",
    "        print(\"Parameter settings:\")\n",
    "        print(param)\n",
    "\n",
    "        train_set = prep_dataloader(\"train\", param[\"num_batch\"], X_train, y_train)\n",
    "        valid_set = prep_dataloader(\"valid\", param[\"num_batch\"], X_valid, y_valid)\n",
    "        test_set = prep_dataloader(\"test\", param[\"num_batch\"], X_test)\n",
    "\n",
    "        model = NeuralNet(train_set.dataset.dim).to(device)  # Construct model and move to device\n",
    "        loss_record, local_best_model = train(model, train_set, valid_set, param, device, verbose = False)\n",
    "        all_loss_record.append(min(loss_record[\"valid\"]))\n",
    "        if min(loss_record[\"valid\"]) < best_valid_loss:\n",
    "            best_valid_loss = min(loss_record[\"valid\"])\n",
    "            best_train_loss = min(loss_record[\"train\"])\n",
    "            best_loss_record = loss_record\n",
    "            best_model = local_best_model\n",
    "            best_params = param\n",
    "            torch.save(best_model.state_dict(), model_save_path)\n",
    "        print(\"=\"*50)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Tuning Result ==========\n",
      "Best params are: {'num_batch': 512, 'num_epoch': 50, 'optimizer': 'Adam', 'optim_hparams': {'lr': 0.1, 'weight_decay': 0.1}, 'patience': 10}\n",
      "Training MSE: 134.43952184050605, Valid MSE: 132.5154302767283\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*10, \"Tuning Result\", \"=\"*10)\n",
    "print(f\"Best params are: {best_params}\")\n",
    "print(f\"Training MSE: {best_train_loss}, Valid MSE: {best_valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve(best_loss_record[\"train\"], best_loss_record[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test(best_model, test_set, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['INDEX', 'PM2.5'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i+1, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to ./submission/submission_1101_6.csv\n"
     ]
    }
   ],
   "source": [
    "submit_path = \"./submission/submission_1101_6.csv\"\n",
    "save_pred(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-088d69fb3456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PM2.5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PM2.5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    plt.plot(data_train[col])\n",
    "    plt.plot(data_train[\"PM2.5\"])\n",
    "    plt.legend([col, \"PM2.5\"])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUaUlEQVR4nO3df4xd5X3n8ffXv5h6HMBjDxaM4xgJSIyshbQjRJ20WmC7Cd0smG4ggVXWakYZpIQh2ayycbG0dKW1hKN4UxaqSlbtxZWKty5tbLRaZ4uM2yhigzrORunE7i4UbOpfePAPbMyOPba/+8c940wm1/HMPddzPYf3SxqdH/ece775Ix8eP+e5zxOZiSSpWqa1ugBJUvMZ7pJUQYa7JFWQ4S5JFWS4S1IFGe6SVEGGuyRVkOGuD7yI2BMR/y8i3ouIQxHxXETMKT57LiL+05jrF0dERsSM1lQsXZrhLtX8y8ycA9wOfBz4vdaWI5VjuEujZOYh4H9SC3lpyjLcpVEiYiFwL/B6q2uRygjnltEHXUTsAeYDCcwBXgb+VWYej4jngM8DQ6NumQZ8CJiZmWcnt1ppfGy5SzXLM/NDwD8FPkYt7Ed8OzOvHfkD/kkL6pMmxHCXRsnMvwGeA77d4lKkUhzKJf2iPwD2RMRtrS5EapQtd2mMzBwE/gT4D62uRWqUL1QlqYJsuUtSBRnuklRBhrskVZDhLkkVdEUMhZw/f34uXry41WVI0pSyc+fOdzKzs95nV0S4L168mP7+/laXIUlTSkTsvdhnl+yWiYgNEXE4IgZGneuIiJci4rViO7c4HxHxXyLi9Yj4SUT8anP+J0iSJmI8fe7PAZ8ec24lsD0zbwa2F8dQm03v5uKvF/ij5pQpSZqIS4Z7Zn4fODrm9P3AxmJ/I7B81Pk/yZofAtdGxPVNqlWSNE6NjpZZkJkHi/1DwIJivwv4x1HX7SvO/YKI6I2I/ojoHxwcbLAMSVI9pYdCZm3+ggnPYZCZ6zKzOzO7OzvrvuyVJDWo0XB/e6S7pdgeLs7vBz486rqFxTlJ0iRqNNxfBFYU+yuAraPO/5ti1MydwLujum8kSZPkkuPcI2ITtdVp5kfEPuBJ4Clgc0T0AHuBh4rL/wfw29TWn3wf+N3LULMk6RIuGe6Z+fBFPrqnzrUJfKVsUdLlEhGT8hyn0larXRG/UJUmy0RDNyIMak1JThwmSRVkuEtSBRnuklRBhrskVZDhLkkVZLhLUgUZ7pJUQYa7JFWQ4S5JFWS4S1IFGe6SVEGGuyRVkOEuSRVkuEtSBRnuklRBhrskVVCpcI+Ir0bEQET8NCK+VpzriIiXIuK1Yju3KZVKksat4XCPiKXAl4A7gNuAz0TETcBKYHtm3gxsL44lSZOoTMt9CfBqZr6fmWeBvwF+B7gf2FhcsxFYXqpCSdKElQn3AeA3ImJeRMwGfhv4MLAgMw8W1xwCFtS7OSJ6I6I/IvoHBwdLlCFJGqvhcM/M3cAa4K+A7wE/Bs6NuSaBuqsLZ+a6zOzOzO7Ozs5Gy5Ak1VHqhWpmrs/MX8vM3wSOAf8XeDsirgcotofLlylJmoiyo2WuK7aLqPW3Pw+8CKwoLlkBbC3zDEnSxM0oef9fRMQ8YBj4SmYej4ingM0R0QPsBR4qW6QkaWJKhXtm/kadc0eAe8p8rySpHH+hKkkVZLhLUgUZ7pJUQYa7JFWQ4S5JFWS4S1IFGe6SVEGGuyRVkOEuSRVUdvoBqWU6Ojo4duzYZX9ORFz2Z8ydO5ejR49e9ufog8Nw15R17NgxarNKT32T8R8QfbDYLSNJFWS4S1IFGe6SVEGGuyRVkOEuSRVUdpm9fxsRP42IgYjYFBFtEXFjRLwaEa9HxJ9FxKxmFStJGp+Gwz0iuoDHge7MXApMBz4PrAG+k5k3UVs0u6cZhUqSxq9st8wM4FciYgYwGzgI3A28UHy+EVhe8hmSpAlqONwzcz/wbeAtaqH+LrATOJ6ZZ4vL9gFd9e6PiN6I6I+I/sHBwUbLkCTVUaZbZi5wP3AjcAPQDnx6vPdn5rrM7M7M7s7OzkbLkCTVUaZb5p8Bb2bmYGYOA38JfAK4tuimAVgI7C9ZoyRpgsqE+1vAnRExO2oTY9wD7AJ2AJ8trlkBbC1XoiRposr0ub9K7cXpj4C/K75rHfBN4OsR8TowD1jfhDolSRNQalbIzHwSeHLM6TeAO8p8rySpHKf81ZSVT14Nv39Nq8toinzy6laXoIox3DVlxX88Uan53PP3W12FqsS5ZSSpggx3Saogw12SKshwl6QKMtwlqYIMd0mqIMNdkirIcJekCjLcJamCDHdJqiDDXZIqyHCXpAoy3CWpggx3Saogw12SKqjhcI+Ij0bEj0f9nYiIr0VER0S8FBGvFdu5zSxYknRpZdZQ/T+ZeXtm3g78GvA+8F1gJbA9M28GthfHkqRJ1KxumXuAf8jMvcD9wMbi/EZgeZOeIUkap2aF++eBTcX+gsw8WOwfAhbUuyEieiOiPyL6BwcHm1SGJAmaEO4RMQu4D/jzsZ9lbYHLuotcZua6zOzOzO7Ozs6yZUiSRmlGy/1e4EeZ+XZx/HZEXA9QbA834RmSpAmY0YTveJifdckAvAisAJ4qtlub8AyprohodQlNMXeug8rUXKXCPSLagd8CHh11+ilgc0T0AHuBh8o8Q7qYWq/f5RURk/IcqdlKhXtmngLmjTl3hNroGUlSi/gLVUmqIMNdkirIcJekCjLcJamCDHdJqiDDXZIqyHCXpAoy3CWpggx3Saogw12SKshwl6QKMtwlqYIMd0mqIMNdkirIcJekCjLcJamCDHdJqqBS4R4R10bECxHx9xGxOyJ+PSI6IuKliHit2Lo4pCRNsrIt96eB72Xmx4DbgN3ASmB7Zt4MbC+OJUmTqOFwj4hrgN8E1gNk5pnMPA7cD2wsLtsILC9XoiRposq03G8EBoH/GhH/OyL+OCLagQWZebC45hCwoN7NEdEbEf0R0T84OFiiDEnSWGXCfQbwq8AfZebHgVOM6YLJzASy3s2ZuS4zuzOzu7Ozs0QZkqSxyoT7PmBfZr5aHL9ALezfjojrAYrt4XIlSpImquFwz8xDwD9GxEeLU/cAu4AXgRXFuRXA1lIVSpImbEbJ+/uAP42IWcAbwO9S+w/G5ojoAfYCD5V8hiRpgkqFe2b+GOiu89E9Zb5XklSOv1CVpAoy3CWpggx3Saogw12SKshwl6QKMtwlqYIMd0mqIMNdkirIcJekCjLcJamCDHdJqiDDXZIqyHCXpAoy3CWpggx3Saogw12SKqjUYh0RsQc4CZwDzmZmd0R0AH8GLAb2AA9l5rFyZUqSJqIZLfe7MvP2zBxZkWklsD0zbwa2F8eSpEl0Obpl7gc2FvsbgeWX4RmSpF+ibLgn8FcRsTMieotzCzLzYLF/CFhQ78aI6I2I/ojoHxwcLFmGJGm0Un3uwCczc39EXAe8FBF/P/rDzMyIyHo3ZuY6YB1Ad3d33WskSY0p1XLPzP3F9jDwXeAO4O2IuB6g2B4uW6QkaWIaDveIaI+ID43sA/8cGABeBFYUl60AtpYtUpI0MWW6ZRYA342Ike95PjO/FxF/C2yOiB5gL/BQ+TIlSRPRcLhn5hvAbXXOHwHuKVOUJKkcf6EqSRVkuEtSBZUdCilNKcU7ost+T6aje9Vattz1gZKZl/wbCfO2tjZ++MMf0tbWBtRCfjz3G+y6Ethyl8YYCeehoSHuvPPOXzgvTQW23KWLmDNnDjt37mTOnDmtLkWaMFvu0kWcPHnywraRfneplQx36SIMdE1ldstIUgUZ7pJUQYa7JFWQ4S7VsWzZsp8bt75s2bJWlyRNiC9UpTpeeeUVX6hqSrPlLkkVZLhLF7F27VpOnTrF2rVrW12KNGFxJfykuru7O/v7+1tdhgTUxrdfc801vPvuuxfOjRxfCf9/kUZExM7M7K73mS13qY53332X++67j8HBQe67776fC3ppKij9QjUipgP9wP7M/ExE3Aj8N2AesBP4QmaeKfscabJt27aNBQsWMH369FaXIk1YM1ruXwV2jzpeA3wnM28CjgE9TXiGNOmGh4c5f/48w8PDrS5FmrBS4R4RC4F/AfxxcRzA3cALxSUbgeVlniG1guPcNdWV7Zb5A+DfAx8qjucBxzPzbHG8D+iqd2NE9AK9AIsWLSpZhtRcr7zyCtOmTbuweIcvUjXVNNxyj4jPAIczc2cj92fmuszszszuzs7ORsuQmq6jowP42eIcI9uR89JUUKZb5hPAfRGxh9oL1LuBp4FrI2LkXwQLgf2lKpQm2YkTJ+jo6ODll1/mzJkzvPzyy3R0dHDixIlWlyaNW8Phnpm/l5kLM3Mx8Hng5cz818AO4LPFZSuAraWrlCbR2bNnWbt2LX19fbS1tdHX18fatWs5e/bspW+WrhCXY5z7N4GvR8Tr1Prg11+GZ0iXzVVXXcXRo0cZGBjg3LlzDAwMcPToUa666qpWlyaNW1MmDsvMvwb+uth/A7ijGd8rtcKXvvQlvvGNb/Ctb32Lw4cPc9111zE4OMiXv/zlVpcmjZu/UJXGWLZsGe3t7Rw9epTM5OjRo7S3tzscUlOK4S6NsXr1ah5//HFuueUWpk2bxi233MLjjz/O6tWrW12aNG7O5y6NsWvXLt5//33Wr1/PJz/5SX7wgx/Q09PDnj17Wl2aNG623KUxZs2axWOPPcZdd93FzJkzueuuu3jssceYNWtWq0uTxs0pf6Uxpk2bxrx585gzZw5vvfUWixYt4r333uPIkSOcP3++1eVJFzjlrzQBXV1dF8a0jzR+zp49S1dX3Zk0pCuS4S7V0dbWxoYNGzh9+jQbNmygra2t1SVJE2K4S2McOHCABx54gHvvvZdZs2Zx77338sADD3DgwIFWlyaNm+EujXHDDTewZcsWtm3bxpkzZ9i2bRtbtmzhhhtuaHVp0rg5FFKq4/jx43zqU59ieHiYmTNnMmPGDObNm9fqsqRxs+UujbFv3z5Onz59YYrfjo4OTp8+zb59+1pcmTR+hrs0RkTw6KOPcujQITKTQ4cO8eijj1JbaEyaGuyWkcbITDZv3sy2bdvYu3cvH/nIRzh58qSrMWlKseUujTFjxgyGhoYALrTWh4aGmDHDtpCmDsNdGuPqq69maGiIvr4+Tp48SV9fH0NDQ1x99dWtLk0aN8NdGuP48eP09vbyxBNP0N7ezhNPPEFvby/Hjx9vdWnSuBnu0hhLlizhwQcfZGhoiMxkaGiIBx98kCVLlrS6NGncGu5EjIg24PvAVcX3vJCZT0bEjdQWzJ4H7AS+kJlnmlGsNBlWrVrF5z73Odrb2y+8UD116hRPP/10q0uTxq1My/00cHdm3gbcDnw6Iu4E1gDfycybgGNAT+kqpRZx+KOmqobDPWveKw5nFn8J3A28UJzfCCwvU6A02VavXk1vby/t7e0AtLe309vb60pMmlJKje2KiOnUul5uAv4Q+AfgeGaeLS7ZB9SdJzUieoFegEWLFpUpQ2qqXbt2cerUKTZs2HBhJaYvfvGL7N27t9WlSeNW6oVqZp7LzNuBhcAdwMcmcO+6zOzOzO7Ozs4yZUhNNWvWLPr6+n5uJaa+vj5XYtKU0pTRMpl5HNgB/DpwbUSM/ItgIbC/Gc+QJsuZM2d49tln2bFjB8PDw+zYsYNnn32WM2ccF6Cpo+Fwj4jOiLi22P8V4LeA3dRC/rPFZSuArSVrlCbVrbfeyiOPPEJfXx9tbW309fXxyCOPcOutt7a6NGncyrTcrwd2RMRPgL8FXsrM/w58E/h6RLxObTjk+vJlSpNn1apVPP/88zzzzDMMDQ3xzDPP8Pzzz7Nq1apWlyaNW8MvVDPzJ8DH65x/g1r/uzQlPfzwwwD09fWxe/dulixZwurVqy+cl6aCuBJmuuvu7s7+/v5WlyFJU0pE7MzM7nqfOf2AJFWQ4S5JFWS4S1IFGe5SHZs2bWLp0qVMnz6dpUuXsmnTplaXJE2IS8tIY2zatIlVq1axfv36C9MP9PTU5r9zxIymCkfLSGMsXbqU5cuXs2XLlgtDIUeOBwYGWl2edMEvGy1jy10aw4nDVAX2uUtjOHGYqsBuGWmMadOmMX/+/F9Yiemdd97h/PnzrS5PusAfMUkT0NXVdWEGyJGVmM6cOUNXV92lCaQrkuEu1TES6iP/snW5PU01hrs0xv79+5k5cybws1CfOXMm+/e7NIGmDsNdGmPWrFmsXLmSN998k3PnzvHmm2+ycuVKX6hqSvGFqjTGtGnTmDdvHnPmzOGtt95i0aJFvPfeexw5csQXqrqi+EJVmoCuri6Gh4eBn/W5Dw8P+0JVU0qZZfY+HBE7ImJXRPw0Ir5anO+IiJci4rViO7d55UqTY/bs2WzYsIHTp0+zYcMGZs+e3eqSpAkp03I/C/y7zLwVuBP4SkTcCqwEtmfmzcD24liaMg4cOMCaNWt+bg3VNWvWcODAgVaXJo1bw+GemQcz80fF/klqi2N3AfcDG4vLNgLLS9YoTaolS5awcOFCBgYGOHfuHAMDAyxcuJAlS5a0ujRp3JrS5x4Ri6mtp/oqsCAzDxYfHQIWXOSe3ojoj4j+wcHBZpQhNcWqVavo6elhx44dDA8Ps2PHDnp6elwgW1NK6YnDImIO8BfA1zLzxOgfe2RmRkTd4TiZuQ5YB7XRMmXrkJrFBbJVBaXCPSJmUgv2P83MvyxOvx0R12fmwYi4Hjhctkhpsj388MOGuaa0MqNlAlgP7M7M/zzqoxeBFcX+CmBr4+VJkhpRpuX+CeALwN9FxI+Lc08ATwGbI6IH2As8VKpCSdKENRzumfkD4GKzKd3T6PdKksrzF6qSVEGGuyRV0BUxcVhEDFLrn5euNPOBd1pdhHQRH8nMznofXBHhLl2pIqL/YrPuSVcyu2UkqYIMd0mqIMNd+uXWtboAqRH2uUtSBdlyl6QKMtwlqYIMd6mOiNgQEYcjYqDVtUiNMNyl+p4DPt3qIqRGGe5SHZn5feBoq+uQGmW4S1IFGe6SVEGGuyRVkOEuSRVkuEt1RMQm4H8BH42IfcWykdKU4fQDklRBttwlqYIMd0mqIMNdkirIcJekCjLcJamCDHdJqiDDXZIq6P8DOqorKg8ZFIgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in data_train.columns:\n",
    "    plt.boxplot(data_train[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMB_TEMP</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CO</th>\n",
       "      <th>NMHC</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>PM10</th>\n",
       "      <th>RH</th>\n",
       "      <th>SO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMB_TEMP  CH4    CO  NMHC    NO   NO2   NOx   PM10    RH  SO2\n",
       "0         19.0  2.1  0.40  0.19   1.8  20.0  22.0   57.0  91.0  2.1\n",
       "1         18.0  2.2  0.46  0.18   3.9  19.0  23.0   65.0  92.0  2.6\n",
       "2         18.0  2.3  0.46  0.17   5.3  19.0  25.0   61.0  92.0  2.6\n",
       "3         18.0  2.3  0.46  0.19  14.0  22.0  37.0   58.0  93.0  2.5\n",
       "4         18.0  2.2  0.37  0.16   4.1  17.0  21.0   41.0  93.0  2.0\n",
       "...        ...  ...   ...   ...   ...   ...   ...    ...   ...  ...\n",
       "8755      17.0  2.2  0.85  0.17   0.9  16.0  17.0  177.0  69.0  2.7\n",
       "8756      17.0  2.2  0.82  0.17   0.9  18.0  18.0  143.0  70.0  2.3\n",
       "8757      17.0  2.1  0.78  0.16   1.1  15.0  17.0  110.0  72.0  2.3\n",
       "8758      16.0  2.1  0.73  0.14   1.2  17.0  18.0   96.0  73.0  2.4\n",
       "8759      16.0  2.1  0.65  0.13   1.3  17.0  18.0   87.0  73.0  2.3\n",
       "\n",
       "[8760 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NeuralNet(train_set.dataset.dim).to(device)\n",
    "model_check_point = torch.load(model_save_path, map_location='cpu')\n",
    "model.load_state_dict(model_check_point)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
